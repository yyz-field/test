<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>v1.0.1 on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/</link>
    <description>Recent content in v1.0.1 on Longhorn Manual Test Cases</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BestEffort Recurring Job Cleanup</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/besteffort-recurring-job/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/besteffort-recurring-job/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Set up a &lt;code&gt;BackupStore&lt;/code&gt; anywhere (since the cleanup fails at the &lt;code&gt;Engine&lt;/code&gt; level, any &lt;code&gt;BackupStore&lt;/code&gt; can be used.&lt;/li&gt;&#xA;&lt;li&gt;Add both of the &lt;code&gt;Engine Images&lt;/code&gt; listed here:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;quay.io/ttpcodes/longhorn-engine:no-cleanup&lt;/code&gt; - &lt;code&gt;Snapshot&lt;/code&gt; and &lt;code&gt;Backup&lt;/code&gt; deletion are both set to return an error. If the &lt;code&gt;Snapshot&lt;/code&gt; part of a &lt;code&gt;Backup&lt;/code&gt; fails, that will error out first and &lt;code&gt;Backup&lt;/code&gt; deletion will not be reached.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;quay.io/ttpcodes/longhorn-engine:no-cleanup-backup&lt;/code&gt; - Only &lt;code&gt;Backup&lt;/code&gt; deletion is set to return an error. The &lt;code&gt;Snapshot&lt;/code&gt; part of a &lt;code&gt;Backup&lt;/code&gt; should succeed, and the &lt;code&gt;Backup&lt;/code&gt; deletion will fail.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The next steps need to be repeated for each &lt;code&gt;Engine Image&lt;/code&gt; (this is to test the code for &lt;code&gt;Snapshots&lt;/code&gt; and &lt;code&gt;Backups&lt;/code&gt; separately).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Change imagePullPolicy to IfNotPresent Test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/change-imagepullpolicy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/change-imagepullpolicy/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn using Helm chart with the new &lt;code&gt;longhorn master&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that Engine Image daemonset, Manager daemonset, UI deployment, Driver Deployer deployment has the field &lt;code&gt;spec.template.spec.containers.imagePullPolicy&lt;/code&gt; set to &lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;run the bash script &lt;code&gt;dev/scripts/update-image-pull-policy.sh&lt;/code&gt; inside &lt;code&gt;longhorn&lt;/code&gt; repo&lt;/li&gt;&#xA;&lt;li&gt;Verify that Engine Image daemonset, Manager daemonset, UI deployment, Driver Deployer deployment has the field &lt;code&gt;spec.template.spec.containers.imagePullPolicy&lt;/code&gt; set back to &lt;code&gt;Always&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>DR volume related latest backup deletion test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/dr-volume-latest-backup-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/dr-volume-latest-backup-deletion/</guid>
      <description>&lt;p&gt;DR volume keeps getting the latest update from the related backups. Edge cases where the latest backup is deleted can be test as below.&lt;/p&gt;&#xA;&lt;h2 id=&#34;case-1&#34;&gt;Case 1:&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume and take multiple backups for the same.&lt;/li&gt;&#xA;&lt;li&gt;Delete the latest backup.&lt;/li&gt;&#xA;&lt;li&gt;Create another cluster and set the same backup store to access the backups created in step 1.&lt;/li&gt;&#xA;&lt;li&gt;Go to backup page and click on the backup. Verify the &lt;code&gt;Create Disaster Recovery&lt;/code&gt; option is enabled for it.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;case-2&#34;&gt;Case 2:&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume V1 and take multiple backups for the same.&lt;/li&gt;&#xA;&lt;li&gt;Create another cluster and set the same backup store to access the backups created in step 1.&lt;/li&gt;&#xA;&lt;li&gt;Go to backup page and Create a Disaster Recovery Volume for the backups created in step 1.&lt;/li&gt;&#xA;&lt;li&gt;Create more backup(s) for volume V1 from step 1.&lt;/li&gt;&#xA;&lt;li&gt;Delete the latest backup before the DR volume starts the incremental restore process.&lt;/li&gt;&#xA;&lt;li&gt;Verify the DR Volume still remains healthy.&lt;/li&gt;&#xA;&lt;li&gt;Activate the DR Volume to verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>NFSv4 Enforcement (No NFSv3 Fallback)</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/nfsv4-enforcement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/nfsv4-enforcement/</guid>
      <description>&lt;p&gt;Since the client falling back to &lt;code&gt;NFSv3&lt;/code&gt; usually results in a failure to mount the &lt;code&gt;NFS&lt;/code&gt; share, the way we can check for &lt;code&gt;NFSv3&lt;/code&gt; fallback is to check the error message returned and see if it mentions &lt;code&gt;rpc.statd&lt;/code&gt;, since dependencies on &lt;code&gt;rpc.statd&lt;/code&gt; and other services are no longer needed for &lt;code&gt;NFSv4&lt;/code&gt;, but are needed for &lt;code&gt;NFSv3&lt;/code&gt;. The &lt;code&gt;NFS&lt;/code&gt; mount &lt;strong&gt;should not&lt;/strong&gt; fall back to &lt;code&gt;NFSv3&lt;/code&gt; and instead only give the user a warning that the server may be &lt;code&gt;NFSv3&lt;/code&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Priority Class Default Setting</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/priorityclass-default-setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/priorityclass-default-setting/</guid>
      <description>&lt;p&gt;There are three different cases we need to test when the user inputs a default setting for &lt;code&gt;Priority Class&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install &lt;code&gt;Longhorn&lt;/code&gt; with no &lt;code&gt;priority-class&lt;/code&gt; set in the default settings. The &lt;code&gt;Priority Class&lt;/code&gt; setting should be empty after the installation completes according to the &lt;code&gt;longhorn-ui&lt;/code&gt;, and the default &lt;code&gt;Priority&lt;/code&gt; of all &lt;code&gt;Pods&lt;/code&gt; in the &lt;code&gt;longhorn-system&lt;/code&gt; namespace should be &lt;code&gt;0&lt;/code&gt;:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;~ kubectl -n longhorn-system describe pods | grep Priority&#xA;# should be repeated many times&#xA;Priority:     0&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;Install &lt;code&gt;Longhorn&lt;/code&gt; with a nonexistent &lt;code&gt;priority-class&lt;/code&gt; in the default settings. The system should fail to come online. The &lt;code&gt;Priority Class&lt;/code&gt; setting should be set and the status of the &lt;code&gt;Daemon Set&lt;/code&gt; for the &lt;code&gt;longhorn-manager&lt;/code&gt; should indicate that the reason it failed was due to an invalid &lt;code&gt;Priority Class&lt;/code&gt;:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;~ kubectl -n longhorn-system describe lhs priority-class&#xA;Name:         priority-class&#xA;...&#xA;Value:                 nonexistent-priority-class&#xA;...&#xA;~ kubectl -n longhorn-system describe daemonset.apps/longhorn-manager&#xA;Name:           longhorn-manager&#xA;...&#xA;  Priority Class Name:  nonexistent-priority-class&#xA;Events:&#xA;  Type     Reason            Age                From                  Message&#xA;  ----     ------            ----               ----                  -------&#xA;  Normal   SuccessfulCreate  23s                daemonset-controller  Created pod: longhorn-manager-gbskd&#xA;  Normal   SuccessfulCreate  23s                daemonset-controller  Created pod: longhorn-manager-9s7mg&#xA;  Normal   SuccessfulCreate  23s                daemonset-controller  Created pod: longhorn-manager-gtl2j&#xA;  Normal   SuccessfulDelete  17s                daemonset-controller  Deleted pod: longhorn-manager-9s7mg&#xA;  Normal   SuccessfulDelete  17s                daemonset-controller  Deleted pod: longhorn-manager-gbskd&#xA;  Normal   SuccessfulDelete  17s                daemonset-controller  Deleted pod: longhorn-manager-gtl2j&#xA;  Warning  FailedCreate      4s (x14 over 15s)  daemonset-controller  Error creating: pods &amp;#34;longhorn-manager-&amp;#34; is forbidden: no PriorityClass with name nonexistent-priority-class was found&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;Install &lt;code&gt;Longhorn&lt;/code&gt; with a valid &lt;code&gt;priority-class&lt;/code&gt; in the default settings. The &lt;code&gt;Priority Class&lt;/code&gt; setting should be set according to the &lt;code&gt;longhorn-ui&lt;/code&gt;, and all the &lt;code&gt;Pods&lt;/code&gt; in the &lt;code&gt;longhorn-system&lt;/code&gt; namespace should have the right &lt;code&gt;Priority&lt;/code&gt; set:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;~ kubectl -n longhorn-system describe pods | grep Priority&#xA;# should be repeated many times&#xA;Priority:             2000001000&#xA;Priority Class Name:  system-node-critical&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Return an error when fail to remount a volume</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/error-fail-remount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/error-fail-remount/</guid>
      <description>&lt;h3 id=&#34;case-1-volume-with-a-corrupted-filesystem-try-to-remount&#34;&gt;Case 1: Volume with a corrupted filesystem try to remount&lt;/h3&gt;&#xA;&lt;p&gt;Steps to reproduce bug:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume of size 1GB, say &lt;code&gt;terminate-immediatly&lt;/code&gt; volume.&lt;/li&gt;&#xA;&lt;li&gt;Create PV/PVC from the volume &lt;code&gt;terminate-immediatly&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a deployment of 1 pod with image &lt;code&gt;ubuntu:xenial&lt;/code&gt; and the PVC &lt;code&gt;terminate-immediatly&lt;/code&gt; in default namespace&lt;/li&gt;&#xA;&lt;li&gt;Find the node on which the pod is scheduled to. Let&amp;rsquo;s say the node is &lt;code&gt;Node-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;ssh into &lt;code&gt;Node-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;destroy the filesystem of &lt;code&gt;terminate-immediatly&lt;/code&gt; by running command  &lt;code&gt;dd if=/dev/zero of=/dev/longhorn/terminate-immediatly&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Find and kill the engine instance manager in &lt;code&gt;Node-X&lt;/code&gt;. Longhorn manager will notice that the instance manager is down and try to bring up a new instance manager e for &lt;code&gt;Node-X&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;After bringing up the instance manager e, Longhorn manager will try to remount the volume &lt;code&gt;terminate-immediatly&lt;/code&gt;. The remounting should fail bc we already destroyed the filesystem of the volume.&lt;/li&gt;&#xA;&lt;li&gt;We should see this log message&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[longhorn-manager-xv5th] time=&amp;#34;2020-06-23T18:13:15Z&amp;#34; level=info msg=&amp;#34;Event(v1.ObjectReference{Kind:\&amp;#34;Volume\&amp;#34;, Namespace:\&amp;#34;longhorn-system\&amp;#34;, Name:\&amp;#34;terminate-immediatly\&amp;#34;, UID:\&amp;#34;de6ae587-fc7c-40bd-b513-47175ddddf97\&amp;#34;, APIVersion:\&amp;#34;longhorn.io/v1beta1\&amp;#34;, ResourceVersion:\&amp;#34;4088981\&amp;#34;, FieldPath:\&amp;#34;\&amp;#34;}): type: &amp;#39;Warning&amp;#39; reason: &amp;#39;Remount&amp;#39; cannot proceed to remount terminate-immediatly on phan-cluster-v3-worker1: cannot get the filesystem type by using the command blkid /dev/longhorn/terminate-immediatly | sed &amp;#39;s/.*TYPE=//g&amp;#39;&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;case-2-volume-with-no-filesystem-try-to-remount&#34;&gt;Case 2: Volume with no filesystem try to remount&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume of size 1GB, say &lt;code&gt;terminate-immediatly&lt;/code&gt; volume.&lt;/li&gt;&#xA;&lt;li&gt;Attach volume &lt;code&gt;terminate-immediatly&lt;/code&gt; to a node, say &lt;code&gt;Node-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Find and kill the engine instance manager in &lt;code&gt;Node-1&lt;/code&gt;. Longhorn manager will notice that the instance manager is down and try to bring up a new instance manager e for &lt;code&gt;Node-1&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;After bringing up the instance manager e, Longhorn manager will try to remount the volume &lt;code&gt;terminate-immediatly&lt;/code&gt;. The remounting should fail bc the volume does not have a filesystem.&lt;/li&gt;&#xA;&lt;li&gt;We should see that Longhorn reattached but skip the remount the volume &lt;code&gt;terminate-immediatly&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume can be detached.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test access style for S3 compatible backupstore</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/test-s3-access-style/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/test-s3-access-style/</guid>
      <description>&lt;h3 id=&#34;case-1-using-alibaba-cloud-oss-bucket-as-backupstore&#34;&gt;Case 1: Using Alibaba Cloud OSS bucket as backupstore&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create an OSS bucket within Region China in Alibaba Cloud(Aliyun).&lt;/li&gt;&#xA;&lt;li&gt;Create a secret without &lt;code&gt;VIRTUAL_HOSTED_STYLE&lt;/code&gt; for the OSS bucket.&lt;/li&gt;&#xA;&lt;li&gt;Set backup target and the secret in Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Try to list backup. Then the error &lt;code&gt;error: AWS Error: SecondLevelDomainForbidden Please use virtual hosted style to access.&lt;/code&gt; is triggered.&lt;/li&gt;&#xA;&lt;li&gt;Add &lt;code&gt;VIRTUAL_HOSTED_STYLE: dHJ1ZQ== # true&lt;/code&gt; to the secret.&lt;/li&gt;&#xA;&lt;li&gt;Backup list/create/delete/restore work fine after the configuration.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-2-using-aws-s3-bucket-as-backupstore&#34;&gt;Case 2: Using AWS S3 bucket as backupstore&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a secret without &lt;code&gt;VIRTUAL_HOSTED_STYLE&lt;/code&gt; for the S3 bucket.&lt;/li&gt;&#xA;&lt;li&gt;Set backup target and the secret in Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify backup list/create/delete/restore work fine without the configuration.&lt;/li&gt;&#xA;&lt;li&gt;Add &lt;code&gt;VIRTUAL_HOSTED_STYLE: dHJ1ZQ== # true&lt;/code&gt; to the secret.&lt;/li&gt;&#xA;&lt;li&gt;Verify backup list/create/delete/restore still work fine after the configuration.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test S3 backupstore in a cluster sitting behind a HTTP proxy</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/test-s3-backupstore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/test-s3-backupstore/</guid>
      <description>&lt;p&gt;&lt;sup&gt;Related issue: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/3136&#34;&gt;3136&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&#xA;&lt;p&gt;Requirement:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Set up a stand alone Squid, HTTP web proxy&#xA;&lt;ul&gt;&#xA;&lt;li&gt;To configure Squid proxy: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1967#issuecomment-736959332&#34;&gt;a comment about squid config&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;If setting up instance on AWS: &lt;a href=&#34;https://user-images.githubusercontent.com/22139961/87112575-0ca3eb80-c221-11ea-86ef-91ed5f8384cc.png&#34;&gt;a EC2 security group setting&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;S3 with existing backups&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create credential for &lt;strong&gt;Backup Target&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; $ secret_name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;aws-secret-proxy&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; $ proxy_ip&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;123.123.123.123&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; $ no_proxy_params&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,192.168.0.0/16&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; $ kubectl create secret generic $secret_name &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;AWS_ACCESS_KEY_ID&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$AWS_ID &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;AWS_SECRET_ACCESS_KEY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$AWS_KEY &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;HTTP_PROXY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$proxy_ip:3128 &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;HTTPS_PROXY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$proxy_ip:3128 &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;NO_PROXY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$no_proxy_params &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt; -n longhorn-system&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;Open Longhorn UI&lt;/li&gt;&#xA;&lt;li&gt;Click on &lt;em&gt;Setting&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Scroll down to &lt;em&gt;Backup Target Credential Secret&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Fill in &lt;code&gt;$secret_name&lt;/code&gt; assigned in step 1. and save setting&lt;/li&gt;&#xA;&lt;li&gt;Go to &lt;em&gt;Backup&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Restore from existing backups and watch the volume become ready&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Volume Deletion UI Warnings</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/ui-volume-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/ui-volume-deletion/</guid>
      <description>&lt;p&gt;A number of cases need to be manually tested in &lt;code&gt;longhorn-ui&lt;/code&gt;. To test these cases, create the &lt;code&gt;Volume&lt;/code&gt; with the specified conditions in each case, and then try to delete it. What is observed should match what is described in the test case:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;A regular &lt;code&gt;Volume&lt;/code&gt;. Only the default deletion prompt should show up asking to confirm deletion.&lt;/li&gt;&#xA;&lt;li&gt;A &lt;code&gt;Volume&lt;/code&gt; with a &lt;code&gt;Persistent Volume&lt;/code&gt;. The deletion prompt should tell the user that there is a &lt;code&gt;Persistent Volume&lt;/code&gt; that will be deleted along with the &lt;code&gt;Volume&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;A &lt;code&gt;Volume&lt;/code&gt; with a &lt;code&gt;Persistent Volume&lt;/code&gt; and &lt;code&gt;Persistent Volume Claim&lt;/code&gt;. The deletion prompt should tell the user that there is a &lt;code&gt;Persistent Volume&lt;/code&gt; and &lt;code&gt;Persistent Volume Claim&lt;/code&gt; that will be deleted along with the &lt;code&gt;Volume&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;A &lt;code&gt;Volume&lt;/code&gt; that is &lt;code&gt;Attached&lt;/code&gt;. The deletion prompt should indicate what &lt;code&gt;Node&lt;/code&gt; the &lt;code&gt;Volume&lt;/code&gt; is attached to and warn the user about errors that may occur as a result of deleting an attached &lt;code&gt;Volume&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;A &lt;code&gt;Volume&lt;/code&gt; that is &lt;code&gt;Attached&lt;/code&gt; and has a &lt;code&gt;Persistent Volume&lt;/code&gt;. The deletion prompt should contain the information from both test cases 2 and 4.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Additionally, here are bulk deletion test cases that need testing:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
