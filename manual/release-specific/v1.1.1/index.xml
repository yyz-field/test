<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>v1.1.1 on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/</link>
    <description>Recent content in v1.1.1 on Longhorn Manual Test Cases</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CSI Sanity Check</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/csi-sanity-check/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/csi-sanity-check/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2076&#34;&gt;https://github.com/longhorn/longhorn/issues/2076&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;run-csi-sanity&#34;&gt;Run csi-sanity&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Prepare Longhorn cluster and setup backup target.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Make csi-sanity binary from &lt;a href=&#34;https://github.com/kubernetes-csi/csi-test&#34;&gt;csi-test&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;On one of the cluster node, run csi-sanity binary.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;csi-sanity -csi.endpoint /var/lib/kubelet/obsoleted-longhorn-plugins/driver.longhorn.io/csi.sock -ginkgo.skip&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;should create volume from an existing source snapshot|should return appropriate values|should succeed when creating snapshot with maximum-length name|should succeed when requesting to create a snapshot with already existing name and same source volume ID|should fail when requesting to create a snapshot with already existing name and different source volume ID&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;&#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Longhorn with engine is not deployed on all the nodes</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/partial-engine-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/partial-engine-deployment/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related Issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2081&#34;&gt;https://github.com/longhorn/longhorn/issues/2081&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;scenarios&#34;&gt;Scenarios:&lt;/h2&gt;&#xA;&lt;h3 id=&#34;case-1-test-volume-operations-when-some-of-the-engine-image-daemonset-pods-are-miss-scheduled&#34;&gt;Case 1: Test volume operations when some of the engine image DaemonSet pods are miss scheduled&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn in a 3-node cluster: &lt;code&gt;node-1&lt;/code&gt;, &lt;code&gt;node-2&lt;/code&gt;, &lt;code&gt;node-3&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a volume, &lt;code&gt;vol-1&lt;/code&gt;, of 3 replicas&lt;/li&gt;&#xA;&lt;li&gt;Create another volume, &lt;code&gt;vol-2&lt;/code&gt;, of 3 replicas&lt;/li&gt;&#xA;&lt;li&gt;Taint &lt;code&gt;node-1&lt;/code&gt; with the taint: &lt;code&gt;key=value:NoSchedule&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check that all functions (attach, detach, snapshot, backup, expand, restore, creating DR volume, &amp;hellip; ) are working ok for &lt;code&gt;vol-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-2-test-volume-operations-when-some-of-engine-image-daemonset-pods-are-not-fully-deployed&#34;&gt;Case 2: Test volume operations when some of engine image DaemonSet pods are not fully deployed&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Continue from case 1&lt;/li&gt;&#xA;&lt;li&gt;Attach &lt;code&gt;vol-1&lt;/code&gt; to &lt;code&gt;node-1&lt;/code&gt;. Change the number of replicas of &lt;code&gt;vol-1&lt;/code&gt; to 2. Delete the replica on &lt;code&gt;node-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Delete the pod on &lt;code&gt;node-1&lt;/code&gt; of the engine image DaemonSet. Or delete the engine image DaemonSet and wait for Longhorn to automatically recreates it.&lt;/li&gt;&#xA;&lt;li&gt;Notice that the engine image CR state become deploying&lt;/li&gt;&#xA;&lt;li&gt;Verify that functions (detach, snapshot, backup) are working ok for &lt;code&gt;vol-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Detach &lt;code&gt;vol-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn cannot attach &lt;code&gt;vol-1&lt;/code&gt; to &lt;code&gt;node-1&lt;/code&gt; since there is no engine image on &lt;code&gt;node-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check that all functions (attach to other nodes, detach, snapshot, backup, expand, restore, creating DR volume, &amp;hellip; ) are working ok for &lt;code&gt;vol-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;vol-2&lt;/code&gt; cannot be attached to any nodes because one of its replica is sitting on the &lt;code&gt;node-1&lt;/code&gt; which doesn&amp;rsquo;t have the engine image&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-3-test-engine-upgrade-when-some-of-the-engine-image-daemonset-pods-are-not-fully-deployed&#34;&gt;Case 3: Test engine upgrade when some of the engine image DaemonSet pods are not fully deployed&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Continue from case 2&lt;/li&gt;&#xA;&lt;li&gt;Deploy a new engine image, &lt;code&gt;newEI&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that you can upgrade &lt;code&gt;vol-1&lt;/code&gt; to &lt;code&gt;newEI&lt;/code&gt; (both live and offline upgrade)&lt;/li&gt;&#xA;&lt;li&gt;Verify that you can not upgrade &lt;code&gt;vol-2&lt;/code&gt; to &lt;code&gt;newEI&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-4-test-replicas-scheduling-when-some-of-the-engine-image-daemonset-pods-are-not-fully-deployed&#34;&gt;Case 4: Test replicas scheduling when some of the engine image DaemonSet pods are not fully deployed&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Continue from case 2&lt;/li&gt;&#xA;&lt;li&gt;Create a new volume, &lt;code&gt;vol-3&lt;/code&gt;, with 2 replicas&lt;/li&gt;&#xA;&lt;li&gt;Verify that replicas of &lt;code&gt;vol-3&lt;/code&gt; are on &lt;code&gt;node-2&lt;/code&gt; and &lt;code&gt;node-3&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check that all functions (attach, detach, snapshot, backup, expand, restore, creating DR volume,&amp;hellip; ) are working ok for &lt;code&gt;vol-3&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-5-test-longhorn-upgrade-when-some-of-the-engine-image-daemonset-pods-are-not-fully-deployed&#34;&gt;Case 5: Test Longhorn upgrade when some of the engine image DaemonSet pods are not fully deployed&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Continue from case 3&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to a new version&lt;/li&gt;&#xA;&lt;li&gt;Verify that the upgrade is not blocked. Longhorn successfully upgrades to the new version using the same default engine image even though the default engine image is not fully deployed.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-6-test-auto-upgrade-engine-feature-when-some-of-the-engine-image-daemonset-pods-are-not-fully-deployed&#34;&gt;Case 6: Test &lt;code&gt;auto upgrade engine&lt;/code&gt; feature when some of the engine image DaemonSet pods are not fully deployed&lt;/h3&gt;&#xA;&lt;p&gt;With the engine image is missing on &lt;code&gt;node-1&lt;/code&gt;, we need  to re-verify the manual test for feature &lt;code&gt;auto upgrade engine&lt;/code&gt; &lt;a href=&#34;https://github.com/longhorn/longhorn-tests/blob/master/docs/content/manual/pre-release/upgrade/auto-upgrade-engine.md&#34;&gt;https://github.com/longhorn/longhorn-tests/blob/master/docs/content/manual/pre-release/upgrade/auto-upgrade-engine.md&lt;/a&gt;&#xA;Make sure that Longhorn automatically upgrades engine image for volumes that are either:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Set Tolerations/PriorityClass For System Components</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/tolerations_priorityclass_setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/tolerations_priorityclass_setting/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2120&#34;&gt;https://github.com/longhorn/longhorn/issues/2120&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Manual Tests:&lt;/p&gt;&#xA;&lt;h3 id=&#34;case-1-existing-longhorn-installation&#34;&gt;Case 1: Existing Longhorn installation&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn master.&lt;/li&gt;&#xA;&lt;li&gt;Change toleration in UI setting&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn.io/last-applied-tolerations&lt;/code&gt; annotation and &lt;code&gt;toleration&lt;/code&gt; of manager, drive deployer, UI are not changed.&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn.io/last-applied-tolerations&lt;/code&gt; annotation and &lt;code&gt;toleration&lt;/code&gt; for managed components (CSI components, IM pods, share manager pod, EI daemonset, backing-image-manager, cronjob) are updated correctly&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-2-new-installation-by-helm&#34;&gt;Case 2: New installation by Helm&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn master, set tolerations like:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;defaultSettings&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;taintToleration&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;key=value:NoSchedule&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;longhornManager&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;priorityClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;~&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;key&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Equal&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;value&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NoSchedule&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;longhornDriver&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;priorityClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;~&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;key&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Equal&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;value&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NoSchedule&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;longhornUI&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;priorityClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;~&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;key&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Equal&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;value&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NoSchedule   &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;Verify that the toleration is added for: IM pods, Share Manager pods, CSI deployments, CSI daemonset, the backup jobs, manager, drive deployer, UI&lt;/li&gt;&#xA;&lt;li&gt;Uninstall the Helm release.&#xA;Verify that uninstalling job has the same toleration as Longhorn manager.&#xA;Verify that the uninstallation success.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-3-upgrading-from-helm&#34;&gt;Case 3: Upgrading from Helm&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn v1.0.2 using Helm, set tolerations using Longhorn UI&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to master version, verify that &lt;code&gt;longhorn.io/managed-by: longhorn-manager&lt;/code&gt; is not set for manager, driver deployer and UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn.io/managed-by: longhorn-manager&lt;/code&gt; label is added for:  IM CRs, EI CRs, Share Manager CRs, IM pods, Share Manager pods, CSI services, CSI deployments, CSI daemonset.&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn.io/last-applied-tolerations&lt;/code&gt; is set for: IM pods, Share Manager pods, CSI deployments, CSI daemonset&lt;/li&gt;&#xA;&lt;li&gt;Edit the tolerations using Longhorn UI and verify the tolerations get updated for components other than Longhorn manager, driver deployer and UI only. Longhorn manager, driver deployer and UI pods should not get restarted.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade the chart to specify toleration for manager, drive deployer, UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify that the toleration get applied&lt;/li&gt;&#xA;&lt;li&gt;Repeat this test case with Longhorn v1.1.0 in step 1&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-4-upgrading-from-kubectl&#34;&gt;Case 4: Upgrading from kubectl&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn v1.0.2 using kubectl, set tolerations using Longhorn UI&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to master version, verify that &lt;code&gt;longhorn.io/managed-by: longhorn-manager&lt;/code&gt; is not set for manager, driver deployer and UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn.io/managed-by: longhorn-manager&lt;/code&gt; label is added for:  IM CRs, EI CRs, Share Manager CRs, IM pods, Share Manager pods, CSI services, CSI deployments, CSI daemonset.&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn.io/last-applied-tolerations&lt;/code&gt; is set for: IM pods, Share Manager pods, CSI deployments, CSI daemonset&lt;/li&gt;&#xA;&lt;li&gt;Edit the tolerations using Longhorn UI and verify the tolerations get updated for components other than Longhorn manager, driver deployer and UI only. Longhorn manager, driver deployer and UI pods should not get restarted.&lt;/li&gt;&#xA;&lt;li&gt;Edit the Yaml to specify toleration for manager, drive deployer, UI and upgrade Longhorn using kubectl command.&lt;/li&gt;&#xA;&lt;li&gt;Verify that the toleration get applied&lt;/li&gt;&#xA;&lt;li&gt;Repeat this test case with Longhorn v1.1.0 in step 1&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-5-node-with-taints&#34;&gt;Case 5: Node with taints&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Add some taints to all node in the cluster, e.g., &lt;code&gt;key=value:NoSchedule&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Repeate case 2, 3, 4&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-6-priority-class-ui&#34;&gt;Case 6: Priority Class UI&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Change Priority Class setting in Longhorn UI&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn only updates the managed components&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-7-priority-class-helm&#34;&gt;Case 7: Priority Class Helm&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Change Priority Class in Helm for manager, driver, UI&lt;/li&gt;&#xA;&lt;li&gt;Verify that only priority class name of manager, driver, UI get updated&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test Disable IPv6</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/disable_ipv6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/disable_ipv6/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2136&#34;&gt;https://github.com/longhorn/longhorn/issues/2136&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2197&#34;&gt;https://github.com/longhorn/longhorn/issues/2197&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Longhorn v1.1.1 should work with IPv6 disabled.&lt;/p&gt;&#xA;&lt;h2 id=&#34;scenario&#34;&gt;Scenario&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Kubernetes&lt;/li&gt;&#xA;&lt;li&gt;Disable IPv6 on all the worker nodes using the following&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Go to the folder /etc/default&#xA;In the grub file, edit the value GRUB_CMDLINE_LINUX_DEFAULT=&amp;#34;ipv6.disable=1&amp;#34;&#xA;Once the file is saved update by the command update-grub&#xA;Reboot the node and once the node becomes active, &#xA;Use the command cat /proc/cmdline to verify &amp;#34;ipv6.disable=1&amp;#34; is reflected in the values &#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn and test basic use cases.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Accessing the UI for CRUD RWO/RWX volumes&lt;/li&gt;&#xA;&lt;li&gt;Use kubectl to create workloads using RWO/RWX volumes.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Re-enable ipv6 on all the nodes using:&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;By deleting the &amp;#34;ipv6.disable=1&amp;#34; from the GRUB_CMDLINE_LINUX_DEFAULT accessing grub file from /etc/default&#xA;Perform a reboot and once the node comes active&#xA;Use the command cat /proc/cmdline to verify &amp;#34;ipv6.disable=1&amp;#34; is not reflected in the values&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Make sure the basic use cases still works.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test File Sync Cancellation</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-file-sync-cancellation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-file-sync-cancellation/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2416&#34;&gt;https://github.com/longhorn/longhorn/issues/2416&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-step&#34;&gt;Test step&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;For test convenience, manually launch the backing image manager pods:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: apps/v1&#xA;kind: DaemonSet&#xA;metadata:&#xA;  labels:&#xA;    app: backing-image-manager&#xA;  name: backing-image-manager&#xA;  namespace: longhorn-system&#xA;spec:&#xA;  selector:&#xA;    matchLabels:&#xA;      app: backing-image-manager&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: backing-image-manager&#xA;    spec:&#xA;      containers:&#xA;      - name: backing-image-manager&#xA;        image: longhornio/backing-image-manager:master&#xA;        imagePullPolicy: Always&#xA;        securityContext:&#xA;          privileged: true&#xA;        command:&#xA;        - backing-image-manager&#xA;        - --debug&#xA;        - daemon&#xA;        - --listen&#xA;        - 0.0.0.0:8000&#xA;        readinessProbe:&#xA;          tcpSocket:&#xA;            port: 8000&#xA;        volumeMounts:&#xA;        - name: disk-path&#xA;          mountPath: /data&#xA;      volumes:&#xA;      - name: disk-path&#xA;        hostPath:&#xA;          path: /var/lib/longhorn/&#xA;      serviceAccountName: longhorn-service-account&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;Download a backing image in the first pod:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# alias bm=&amp;#34;backing-image-manager backing-image&amp;#34;&#xA;# bm pull --name bi-test --uuid uuid-bi-test --download-url https://cloud-images.ubuntu.com/minimal/releases/focal/release-20200729/ubuntu-20.04-minimal-cloudimg-amd64.img&#xA;# bm ls&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;In the 2nd pod, limit the bandwidth first so that the downloading won&amp;rsquo;t be done within several seconds&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;tc qdisc del dev eth0 root&#xA;tc qdisc add dev eth0 root tbf rate 500kbit latency 0.1ms burst 1000kbit&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;Then start to get the backing image file from the 1st BIM pod&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# alias bm=&amp;#34;backing-image-manager backing-image&amp;#34;&#xA;# bm sync --name bi-test --uuid uuid-bi-test --download-url https://cloud-images.ubuntu.com/minimal/releases/focal/release-20200729/ubuntu-20.04-minimal-cloudimg-amd64.img --size 208601088 --from-host &amp;lt;the IP of 1st BIM pod&amp;gt; --to-host &amp;lt;the IP of 2nd BIM pod&amp;gt;&#xA;# bm ls&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;5&#34;&gt;&#xA;&lt;li&gt;During the syncing, directly deleting the downloading backing image in the 2nd pod&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# bm delete bi-test&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;6&#34;&gt;&#xA;&lt;li&gt;Wait 1 minute and check the log of the 2nd pod. Make sure there is no download/sync (failure) related logs after the deletion. Then restarting sync without deletion, the backing image can be downloaded successfully.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test Frontend Traffic</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/ws-traffic-flood/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/ws-traffic-flood/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2372&#34;&gt;https://github.com/longhorn/longhorn/issues/2372&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-frontend-traffic&#34;&gt;Test Frontend Traffic&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; 100 pvc created.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; all pvcs deployed and detached.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; monitor traffic in frontend pod with nload.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apk add nload&#xA;nload&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; should not see a continuing large amount of traffic when there is no operation happening. The smaller spikes are mostly coming from event resources which possibly could be enhanced later (&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2433)&#34;&gt;https://github.com/longhorn/longhorn/issues/2433)&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Node Delete</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/delete-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/delete-node/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2186&#34;&gt;https://github.com/longhorn/longhorn/issues/2186&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2462&#34;&gt;https://github.com/longhorn/longhorn/issues/2462&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;delete-method&#34;&gt;Delete Method&lt;/h2&gt;&#xA;&lt;p&gt;Should verify with both of the delete methods.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Bulk Delete - This is the &lt;code&gt;Delete&lt;/code&gt; on the Node page.&lt;/li&gt;&#xA;&lt;li&gt;Node Delete - This is the &lt;code&gt;Remove Node&lt;/code&gt; for each node &lt;code&gt;Operation&lt;/code&gt; drop-down list.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;test-node-delete---should-grey-out-when-node-not-down&#34;&gt;Test Node Delete - should grey out when node not down&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; node not &lt;code&gt;Down&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Try to delete any node.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Should see button greyed out.&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-node-delete&#34;&gt;Test Node Delete&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; pod with pvc created.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Node Selector</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-node-selector/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-node-selector/</guid>
      <description>&lt;h3 id=&#34;prepare-the-cluster&#34;&gt;Prepare the cluster&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Using Rancher RKE to create a cluster of 2 Windows worker nodes and 3 Linux worker nodes.&lt;/li&gt;&#xA;&lt;li&gt;Rancher will add the taint &lt;code&gt;cattle.io/os=linux:NoSchedule&lt;/code&gt; to Linux nodes&lt;/li&gt;&#xA;&lt;li&gt;Kubernetes will add label &lt;code&gt;kubernetes.io/os:linux&lt;/code&gt; to Linux nodes&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;test-steps&#34;&gt;Test steps&lt;/h3&gt;&#xA;&lt;p&gt;Repeat the following steps for each type of Longhorn installation: Rancher, Helm, Kubectl:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Follow the Longhorn document at the PR &lt;a href=&#34;https://github.com/longhorn/website/pull/287&#34;&gt;https://github.com/longhorn/website/pull/287&lt;/a&gt; to install Longhorn with toleration &lt;code&gt;cattle.io/os=linux:NoSchedule&lt;/code&gt; and node selector &lt;code&gt;kubernetes.io/os:linux&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn get deployed successfully on the 3 Linux nodes&lt;/li&gt;&#xA;&lt;li&gt;Verify all volume basic functionalities is working ok&lt;/li&gt;&#xA;&lt;li&gt;Create a volume of 3 replica named &lt;code&gt;vol-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Add label &lt;code&gt;longhorn.io/rating:best&lt;/code&gt; to 2 Linux nodes&lt;/li&gt;&#xA;&lt;li&gt;Follow the Longhorn document at the PR &lt;a href=&#34;https://github.com/longhorn/website/pull/287&#34;&gt;https://github.com/longhorn/website/pull/287&lt;/a&gt; to set 2 node selectors &lt;code&gt;kubernetes.io/os:linux&lt;/code&gt; and &lt;code&gt;longhorn.io/rating:best&lt;/code&gt; for Longhorn&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn gets deployed successfully on the 2 Linux nodes&lt;/li&gt;&#xA;&lt;li&gt;Attach the &lt;code&gt;vol-1&lt;/code&gt; to a node, verify that the attachment succeeds. One replica of the volume fails because it is on the down node.&lt;/li&gt;&#xA;&lt;li&gt;Delete all failed replicas on the down node&lt;/li&gt;&#xA;&lt;li&gt;Delete the down node, verify the deletion succeeds&lt;/li&gt;&#xA;&lt;li&gt;Detach the &lt;code&gt;vol-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Follow the Longhorn document at the PR &lt;a href=&#34;https://github.com/longhorn/website/pull/287&#34;&gt;https://github.com/longhorn/website/pull/287&lt;/a&gt; to set 1 node selector &lt;code&gt;kubernetes.io/os:linux&lt;/code&gt; for Longhorn&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn get redeployed successfully on the 3 Linux nodes&lt;/li&gt;&#xA;&lt;li&gt;Attach the &lt;code&gt;vol-1&lt;/code&gt; to a node, verify that the attachment succeeds. Longhorn starts rebuild the 3rd replica on the new node&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test RWX share-mount ownership reset</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/rwx-mount-ownership-reset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/rwx-mount-ownership-reset/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2357&#34;&gt;https://github.com/longhorn/longhorn/issues/2357&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-rwx-share-mount-ownership&#34;&gt;Test RWX share-mount ownership&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Setup one of cluster node to use host FQDN.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;root@ip-172-30-0-139:/home/ubuntu# cat /etc/hosts&#xA;127.0.0.1 localhost&#xA;54.255.224.72 ip-172-30-0-139.lan ip-172-30-0-139&#xA;&#xA;root@ip-172-30-0-139:/home/ubuntu# hostname&#xA;ip-172-30-0-139&#xA;&#xA;root@ip-172-30-0-139:/home/ubuntu# hostname -f&#xA;ip-172-30-0-139.lan&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; &lt;code&gt;Domain = localdomain&lt;/code&gt; is commented out in &lt;code&gt;/etc/idmapd.conf&lt;/code&gt; on cluster hosts.&#xA;This is to ensure &lt;code&gt;localdomain&lt;/code&gt; is not enforce to sync between server and client.&#xA;Ref: &lt;a href=&#34;https://github.com/longhorn/website/pull/279&#34;&gt;https://github.com/longhorn/website/pull/279&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;root@ip-172-30-0-139:~# cat /etc/idmapd.conf &#xA;[General]&#xA;&#xA;Verbosity = 0&#xA;Pipefs-Directory = /run/rpc_pipefs&#xA;# set your own domain here, if it differs from FQDN minus hostname&#xA;# Domain = localdomain&#xA;&#xA;[Mapping]&#xA;&#xA;Nobody-User = nobody&#xA;Nobody-Group = nogroup&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; pod with rwx pvc deployed to the node with host FQDN.&#xA;Here need to update &lt;code&gt;nodeSelector.kubernetes.io/hostname&lt;/code&gt; below to match&#xA;the node name.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Service Account mount on host</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-service-account-mount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-service-account-mount/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;This test case should be tested on both yaml installation, chart installation (Helm and Rancher UI), as well as upgrade scenarios&lt;/li&gt;&#xA;&lt;li&gt;After install Longhorn using on of the above method, ssh into a worker node that has a longhorn-manager pod running&lt;/li&gt;&#xA;&lt;li&gt;check the mount point &lt;code&gt;/run/secrets/kubernetes.io/serviceaccount&lt;/code&gt; by running:&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;root@node-1:~# findmnt  /run/secrets/kubernetes.io/serviceaccount&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that there is no such mount point&lt;/li&gt;&#xA;&lt;li&gt;Kill the longhorn-manager pod on the above node and wait for it to be recreated and running&lt;/li&gt;&#xA;&lt;li&gt;check the mount point &lt;code&gt;/run/secrets/kubernetes.io/serviceaccount&lt;/code&gt; by running:&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;root@node-1:~# findmnt  /run/secrets/kubernetes.io/serviceaccount&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that there is no such mount point&lt;/li&gt;&#xA;&lt;li&gt;Repeat the step 5 to 7 a few times&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test Snapshot Purge Error Handling</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/snapshot-purge-error-handling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/snapshot-purge-error-handling/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1895&#34;&gt;https://github.com/longhorn/longhorn/issues/1895&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Longhorn v1.1.1 handles the error during snapshot purge better and reports to Longhorn-manager.&lt;/p&gt;&#xA;&lt;h2 id=&#34;scenario-1&#34;&gt;Scenario-1&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume with 3 replicas and attach to a pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data into the volume and take a snapshot.&lt;/li&gt;&#xA;&lt;li&gt;Delete a replica that will result in creating a system generated snapshot.&lt;/li&gt;&#xA;&lt;li&gt;Wait for replica to finish and take another snapshot.&lt;/li&gt;&#xA;&lt;li&gt;ssh into a node and resize the latest snapshot. (e.g &lt;code&gt;dd if=/dev/urandom count=50 bs=1M of=&amp;lt;SNAPSHOT-NAME&amp;gt;.img&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Trigger snapshot purge by delete the oldest snapshot.&lt;/li&gt;&#xA;&lt;li&gt;Verify the replica (on the node from step 5) shows error &lt;code&gt;file sizes are not equal and the parent file is larger than the child file&lt;/code&gt; and starts to rebuild.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;scenario-2&#34;&gt;Scenario-2&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume with 3 replicas and attach to a pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data into the volume and take two snapshots.&lt;/li&gt;&#xA;&lt;li&gt;Delete a replica that will result in creating a system generated snapshot.&lt;/li&gt;&#xA;&lt;li&gt;While the rebuilding is in progress, delete a snapshot to trigger SnapshotPurge.&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn manager reports error like &lt;code&gt;Failed to purge snapshots: REPLICA_ADDRESS: cannot purge snapshots because REPLICA_ADDRESS is rebuilding&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test system upgrade with the deprecated CPU setting</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/system-upgrade-with-deprecated-cpu-setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/system-upgrade-with-deprecated-cpu-setting/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2207&#34;&gt;https://github.com/longhorn/longhorn/issues/2207&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-step&#34;&gt;Test step&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy a cluster that each node has different CPUs.&lt;/li&gt;&#xA;&lt;li&gt;Launch Longhorn v1.1.0.&lt;/li&gt;&#xA;&lt;li&gt;Deploy some workloads using Longhorn volumes.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade to the latest Longhorn version. Validate:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;all workloads work fine and no instance manager pod crash during the upgrade.&lt;/li&gt;&#xA;&lt;li&gt;The fields &lt;code&gt;node.Spec.EngineManagerCPURequest&lt;/code&gt; and &lt;code&gt;node.Spec.ReplicaManagerCPURequest&lt;/code&gt; of each node are the same as the setting &lt;code&gt;Guaranteed Engine CPU&lt;/code&gt; value in the old version * 1000.&lt;/li&gt;&#xA;&lt;li&gt;The old setting &lt;code&gt;Guaranteed Engine CPU&lt;/code&gt; is deprecated with an empty value.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Modify new settings &lt;code&gt;Guaranteed Engine Manager CPU&lt;/code&gt; and &lt;code&gt;Guaranteed Replica Manager CPU&lt;/code&gt;. Validate all workloads work fine and no instance manager pod restart.&lt;/li&gt;&#xA;&lt;li&gt;Scale down all workloads and wait for the volume detachment.&lt;/li&gt;&#xA;&lt;li&gt;Set &lt;code&gt;node.Spec.EngineManagerCPURequest&lt;/code&gt; and &lt;code&gt;node.Spec.ReplicaManagerCPURequest&lt;/code&gt; to 0 for some node. Verify the new settings will be applied to those node and the related instance manager pods will be recreated with the CPU requests matching the new settings.&lt;/li&gt;&#xA;&lt;li&gt;Scale up all workloads and verify the data as well as the volume r/w.&lt;/li&gt;&#xA;&lt;li&gt;Do cleanup.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
