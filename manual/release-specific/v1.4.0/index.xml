<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>v1.4.0 on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/</link>
    <description>Recent content in v1.4.0 on Longhorn Manual Test Cases</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Test CSI plugin liveness probe</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-csi-plugin-liveness-probe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-csi-plugin-liveness-probe/</guid>
      <description>Related discussion https://github.com/longhorn/longhorn/issues/3907&#xA;Test CSI plugin liveness probe should recover CSI socket file Given healthy Longhorn cluster&#xA;When delete the Longhorn CSI socket file on one of the node(node-1). rm /var/lib/kubelet/plugins/driver.longhorn.io/csi.sock&#xA;Then the longhorn-csi-plugin-* pod on node-1 should be restarted.&#xA;And the csi-provisioner-* pod on node-1 should be restarted.&#xA;And the csi-resizer-* pod on node-1 should be restarted.&#xA;And the csi-snapshotter-* pod on node-1 should be restarted.&#xA;And the csi-attacher-* pod on node-1 should be restarted.</description>
    </item>
    <item>
      <title>Test engine binary recovery</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-engine-binary-recovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-engine-binary-recovery/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/4380&#xA;Steps Test remove engine binary on host should recover Given EngineImage custom resource deployed&#xA;&amp;gt; kubectl -n longhorn-system get engineimage NAME STATE IMAGE REFCOUNT BUILDDATE AGE ei-b907910b deployed longhornio/longhorn-engine:master-head 0 3d23h 2m25s And engine image pods Ready are 1/1.&#xA;&amp;gt; kubectl -n longhorn-system get pod | grep engine-image engine-image-ei-b907910b-g4kpd 1/1 Running 0 2m43s engine-image-ei-b907910b-46k6t 1/1 Running 0 2m43s engine-image-ei-b907910b-t6wnd 1/1 Running 0 2m43s When Delete engine binary on host</description>
    </item>
    <item>
      <title>Test filesystem trim</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-filesystem-trim/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-filesystem-trim/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/836&#xA;Case 1: Test filesystem trim during writing Given A 10G volume created.&#xA;And Volume attached to node-1.&#xA;And Make a filesystem like EXT4 or XFS for the volume.&#xA;And Mount the filesystem on a mount point.&#xA;Then Run the below shell script with the correct mount point specified:&#xA;#!/usr/bin/env bash MOUNT_POINT=${1} dd if=/dev/urandom of=/mnt/data bs=1M count=8000 sync CKSUM=`md5sum /mnt/data | awk &amp;#39;{print $1}&amp;#39;` for INDEX in {1..10..1}; do rm -rf ${MOUNT_POINT}/data dd if=/mnt/data of=${MOUNT_POINT}/data &amp;amp; RAND_SLEEP_INTERVAL=$(($(($RANDOM%50))+10)) sleep ${RAND_SLEEP_INTERVAL} fstrim ${MOUNT_POINT} while [ `ps aux | grep &amp;#34;dd if&amp;#34; | grep -v grep | wc -l` -eq &amp;#34;1&amp;#34; ] do sleep 1 done CUR_CKSUM=`md5sum ${MOUNT_POINT}/data | awk &amp;#39;{print $1}&amp;#39;` if [ $CUR_CKSUM !</description>
    </item>
    <item>
      <title>Test helm on Rancher deployed Windows Cluster</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-helm-install-on-rancher-deployed-windows-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-helm-install-on-rancher-deployed-windows-cluster/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/4246&#xA;Test Install Given Rancher cluster.&#xA;And 3 new instances for the Windows cluster following Architecture Requirements.&#xA;And docker installed on the 3 Windows cluster instances.&#xA;And Disabled Private IP Address Checks for the 3 Windows cluster instances.&#xA;And Created new Custom Windows cluster with Rancher.&#xA;Select Flannel for Network Provider Enable Windows Support&#xA;And Added the 3 nodes to the Rancher Windows cluster.&#xA;Add Linux Master Node</description>
    </item>
    <item>
      <title>Test Longhorn system backup should sync from the remote backup target</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-system-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-system-backup/</guid>
      <description>Steps Given Custom resource SystemBackup (foo) exist in AWS S3,&#xA;And System backup (foo) downloaded from AWS S3.&#xA;And Custom resource SystemBackup (foo) deleted.&#xA;When Upload the system backup (foo) to AWS S3.&#xA;And Create a new custom resource SystemBackup(foo).&#xA;This needs to be done before the system backup gets synced to the cluster.&#xA;Then Should see the synced messages in the custom resource SystemBackup(foo).&#xA;Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Syncing 9m29s longhorn-system-backup-controller Syncing system backup from backup target Normal Synced 9m28s longhorn-system-backup-controller Synced system backup from backup target </description>
    </item>
    <item>
      <title>Test Node ID Change During Backing Image Creation</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-node-id-change-during-backing-image-creation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-node-id-change-during-backing-image-creation/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/4887&#xA;Steps Given A relatively large file so that uploading it would take several minutes at least.&#xA;And Upload the file as a backing image.&#xA;And Monitor the longhorn manager pod logs.&#xA;When Add new nodes for the cluster or new disks for the existing Longhorn nodes during the upload.&#xA;Then Should see the upload success.&#xA;And Should not see error messages like below in the longhorn manager pods.</description>
    </item>
    <item>
      <title>Test Online Expansion</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-online-expansion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-online-expansion/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/1674&#xA;Test online expansion with continuous reading/writing Given Prepare a relatively large file (5Gi for example) with the checksum calculated.&#xA;And Create and attach a volume.&#xA;And Monitor the instance manager pod logs.&#xA;When Use dd to copy data from the file to the Longhorn block device.&#xA;dd if=/mnt/data of=/dev/longhorn/vol bs=1M And Do online expansion for the volume during the copying.&#xA;Then The expansion should success. The corresponding block device on the attached node is expanded.</description>
    </item>
    <item>
      <title>Test replica scale-down warning</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-replica-scale-down-warning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-replica-scale-down-warning/</guid>
      <description>Related issue https://github.com/longhorn/longhorn/issues/4120&#xA;Steps Given Replica Auto Balance set to least-effort or best-effort.&#xA;And Volume with 3 replicas created.&#xA;And Volume attached to node-1.&#xA;And Monitor node-1 manager pod events.&#xA;kubectl alpha events -n longhorn-system pod &amp;lt;node-1 manager pod&amp;gt; -w When Update replica count to 1.&#xA;Then Should see Normal replice delete event.&#xA;Normal Delete Engine/t1-e-6a846a7a Removed unknown replica tcp://10.42.2.94:10000 from engine And Should not see Warning unknown replica detect event.</description>
    </item>
    <item>
      <title>Test upgrade for migrated Longhorn on Rancher</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-upgrade-for-migrated-longhorn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-upgrade-for-migrated-longhorn/</guid>
      <description>Related discussion https://github.com/longhorn/longhorn/discussions/4198&#xA;Context: since few customers used our broken chart longhorn 100.2.1+up1.3.1 on Rancher (Now fixed) with the workaround. We would like to verify the future upgrade path for those customers.&#xA;Steps Set up a cluster of Kubernetes 1.20. Adding this repo to the apps section in new rancher UI repo: https://github.com/PhanLe1010/charts.git branch: release-v2.6-longhorn-1.3.1. Access old rancher UI by navigating to &amp;lt;your-rancher-url&amp;gt;/g. Install Longhorn 1.0.2. Create/attach some volumes. Create a few recurring snapshot/backup job that run every minutes.</description>
    </item>
  </channel>
</rss>
