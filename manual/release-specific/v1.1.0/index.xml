<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>v1.1.0 on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/</link>
    <description>Recent content in v1.1.0 on Longhorn Manual Test Cases</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Prometheus Support</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/prometheus_support/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/prometheus_support/</guid>
      <description>&lt;p&gt;Prometheus Support allows user to monitor the longhorn metrics. The details are available at &lt;a href=&#34;https://longhorn.io/docs/1.1.0/monitoring/&#34;&gt;https://longhorn.io/docs/1.1.0/monitoring/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;monitor-longhorn&#34;&gt;Monitor longhorn&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy the Prometheus-operator, ServiceMonitor pointing to longhorn-backend and Prometheus as mentioned in the doc.&lt;/li&gt;&#xA;&lt;li&gt;Create an ingress pointing to Prometheus service.&lt;/li&gt;&#xA;&lt;li&gt;Access the Prometheus web UI using the ingress created in the step 2.&lt;/li&gt;&#xA;&lt;li&gt;Select the metrics from below to monitor the longhorn resources.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;longhorn_volume_actual_size_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_volume_capacity_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_volume_robustness&lt;/li&gt;&#xA;&lt;li&gt;longhorn_volume_state&lt;/li&gt;&#xA;&lt;li&gt;longhorn_instance_manager_cpu_requests_millicpu&lt;/li&gt;&#xA;&lt;li&gt;longhorn_instance_manager_cpu_usage_millicpu&lt;/li&gt;&#xA;&lt;li&gt;longhorn_instance_manager_memory_requests_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_instance_manager_memory_usage_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_manager_cpu_usage_millicpu&lt;/li&gt;&#xA;&lt;li&gt;longhorn_manager_memory_usage_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_count_total&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_status&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_cpu_capacity_millicpu&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_cpu_usage_millicpu&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_memory_capacity_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_memory_usage_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_storage_capacity_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_storage_reservation_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_storage_usage_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_disk_capacity_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_disk_reservation_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_disk_usage_bytes&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Deploy workloads which use Longhorn volumes into the cluster. Verify that there is no abnormal data. e.g: volume capacity is 0, cpu usage is over 4000 milicpu etc.&lt;/li&gt;&#xA;&lt;li&gt;Attach a volume to a node. Detach the volume and attach it to a different node. Verify that the volume&amp;rsquo;s information is reported by at most 1 longhorn-manager at any time.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;configure-prometheus-alert-manager&#34;&gt;Configure Prometheus alert manager&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy the Alertmanager as mentioned in the doc.&lt;/li&gt;&#xA;&lt;li&gt;Modify the alert configuration file and set email or slack.&lt;/li&gt;&#xA;&lt;li&gt;Deploy a service using node port to access web UI of the alert manager as mentioned in the doc.&lt;/li&gt;&#xA;&lt;li&gt;Follow the steps from the doc to create PrometheusRule and configure the Prometheus server.&lt;/li&gt;&#xA;&lt;li&gt;Go beyond the threshold set for PrometheusRule in the step 4.&lt;/li&gt;&#xA;&lt;li&gt;Verify the email or slack, user should get the alert message.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;monitor-with-grafana&#34;&gt;Monitor with Grafana&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a ConfigMap referring to the Prometheus. (Refer the doc)&lt;/li&gt;&#xA;&lt;li&gt;Deploy the Grafana and a service to access the UI.&lt;/li&gt;&#xA;&lt;li&gt;Go to Grafana dashboard and import prebuilt longhorn example.&lt;/li&gt;&#xA;&lt;li&gt;Verify the graphs and data are available to monitor.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;monitor-with-rancher-app&#34;&gt;Monitor with Rancher app&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a cluster in Rancher. (1 etcd/control plane and 3 worker nodes)&lt;/li&gt;&#xA;&lt;li&gt;Deploy longhorn v1.1.0.&lt;/li&gt;&#xA;&lt;li&gt;Enable the monitoring for a project.&lt;/li&gt;&#xA;&lt;li&gt;Deploy the ServiceMonitor pointing to longhorn-backend.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: monitoring.coreos.com/v1&#xA;kind: ServiceMonitor&#xA;metadata:&#xA;  name: longhorn-prometheus-servicemonitor&#xA;  namespace: longhorn-system&#xA;  labels:&#xA;    name: longhorn-prometheus-servicemonitor&#xA;spec:&#xA;  selector:&#xA;  matchLabels:&#xA;    app: longhorn-manager&#xA;  namespaceSelector:&#xA;    matchNames:&#xA;    - longhorn-system&#xA;  endpoints:&#xA;  - port: manager&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Access the url provided by the app to access Prometheus or Grafana.&lt;/li&gt;&#xA;&lt;li&gt;Verify the longhorn metrics are available to monitor.&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;a href=&#34;https://v1-1-0.longhorn.io/docs/1.1.0/monitoring/kubelet-volume-metrics/&#34;&gt;kubelet_volume_*&lt;/a&gt; metrics are available if Rancher 2.5 monitoring app is deployed.&lt;/li&gt;&#xA;&lt;li&gt;Import &lt;a href=&#34;https://grafana.com/grafana/dashboards/13032&#34;&gt;Longhorn Example dashboard&lt;/a&gt;. Verify that the graph looks good.&lt;/li&gt;&#xA;&lt;li&gt;Setup alert and alert rules in Rancher monitoring app. Verify that alerts are working ok.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Recurring backup job interruptions</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/recurring-backup-job-interruptions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/recurring-backup-job-interruptions/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related Issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1882&#34;&gt;https://github.com/longhorn/longhorn/issues/1882&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;scenario-1--allow-recurring-job-while-volume-is-detached-disabled-attached-pod-scaled-down-while-the-recurring-backup-was-in-progress&#34;&gt;Scenario 1- &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt; disabled, attached pod scaled down while the recurring backup was in progress.&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume, attach to a pod of a statefulSet, and write 800 Mi data into it.&lt;/li&gt;&#xA;&lt;li&gt;Set a recurring job.&lt;/li&gt;&#xA;&lt;li&gt;While the recurring job is in progress, scale down the pod to 0 of the statefulSet.&lt;/li&gt;&#xA;&lt;li&gt;Volume first detached and cron job gets finished saying unable to complete the backup.&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume again gets auto attached to another node and cron job gets recreated.&lt;/li&gt;&#xA;&lt;li&gt;Verify after backup completion, the volume gets detached.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;scenario-2--allow-recurring-job-while-volume-is-detached-enabled-attached-pod-scaled-down-while-the-recurring-backup-was-in-progress&#34;&gt;Scenario 2- &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt; enabled, attached pod scaled down while the recurring backup was in progress.&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a volume, attach to a pod, and write 800 Mi data into it.&lt;/li&gt;&#xA;&lt;li&gt;Set a recurring job.&lt;/li&gt;&#xA;&lt;li&gt;While the recurring job is in progress, scale down the pod to 0.&lt;/li&gt;&#xA;&lt;li&gt;Volume first detached and cron job gets completed saying unable to complete the backup.&lt;/li&gt;&#xA;&lt;li&gt;Verify volume again gets auto attached to another node and cron job gets recreated.&lt;/li&gt;&#xA;&lt;li&gt;Verify after backup completion, the volume gets detached.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;scenario-3--cron-job-and-volume-attached-to-the-same-node-node-is-powered-down-and-volume-detached-manually&#34;&gt;Scenario 3- Cron job and volume attached to the same node, Node is powered down and volume detached manually.&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Attach a volume to pod of a statefulSet, write data into it and set a recurring backup.&lt;/li&gt;&#xA;&lt;li&gt;Detach from the pod by scaling down the statefulSet.&lt;/li&gt;&#xA;&lt;li&gt;The attached node to volume is power down when the recurring job backup was in progress.&lt;/li&gt;&#xA;&lt;li&gt;The volume is manually detached while the cron job remains in unknown state.&lt;/li&gt;&#xA;&lt;li&gt;The cron job remains in unknown state for about 7 mins and then another pod gets created.&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume get attached to another node and once the job is completed, volume gets detached.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;scenario-4--cron-job-and-volume-attached-to-different-node-node-is-powered-down&#34;&gt;Scenario 4- Cron job and volume attached to different node, Node is powered down.&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Attach a volume to pod of a statefulSet, write data into it and set a recurring backup.&lt;/li&gt;&#xA;&lt;li&gt;Detach from the pod by scaling down the statefulSet.&lt;/li&gt;&#xA;&lt;li&gt;The attached node to volume is power down when the recurring job backup was in progress.&lt;/li&gt;&#xA;&lt;li&gt;Another cron job pod gets created with logs in the previous pod as below.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;time=&amp;#34;2020-11-09T07:39:10Z&amp;#34; level=info msg=&amp;#34;Automatically attach volume volume-test-2 to node node-1&amp;#34;&#xA;time=&amp;#34;2020-11-09T07:39:12Z&amp;#34; level=info msg=&amp;#34;Volume volume-test-2 is in state attached&amp;#34;&#xA;time=&amp;#34;2020-11-09T07:39:12Z&amp;#34; level=info msg=&amp;#34;Running recurring backup for volume volume-test-2&amp;#34;&#xA;time=&amp;#34;2020-11-09T07:39:22Z&amp;#34; level=debug msg=&amp;#34;Creating backup , current progress 0&amp;#34;&#xA;time=&amp;#34;2020-11-09T07:39:27Z&amp;#34; level=debug msg=&amp;#34;Creating backup , current progress 4&amp;#34;&#xA;time=&amp;#34;2020-11-09T07:40:42Z&amp;#34; level=info msg=&amp;#34;Automatically detach the volume volume-test-2&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume get attached to another node and once the job is completed, volume gets detached.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;scenario-5--cron-job-and-volume-attached-to-the-same-node-node-is-restarted&#34;&gt;Scenario 5- Cron job and volume attached to the same node, Node is restarted.&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Attach a volume to pod of a statefulSet, write data into it and set a recurring backup.&lt;/li&gt;&#xA;&lt;li&gt;Detach from the pod by scaling down the statefulSet.&lt;/li&gt;&#xA;&lt;li&gt;The attached node to volume is power down when the recurring job backup was in progress.&lt;/li&gt;&#xA;&lt;li&gt;The volume is manually detached while the cron job remains in unknown state.&lt;/li&gt;&#xA;&lt;li&gt;Power on the node.&lt;/li&gt;&#xA;&lt;li&gt;The cron job which was stuck in unknown state get removed and new cron job get recreated.&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume get attached to another node and the backup job is completed.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;scenario-6--cron-job-and-volume-attached-to-the-samedifferent-node-node-is-powered-down-and-pod-deletion-policy-when-node-is-downis-set-as-delete-both-statefulset-and-deployment-pod&#34;&gt;Scenario 6- Cron job and volume attached to the same/different node, Node is powered down and &lt;code&gt;Pod Deletion Policy When Node is Down&lt;/code&gt;is set as &lt;code&gt;delete-both-statefulset-and-deployment-pod&lt;/code&gt;&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Attach a volume to pod of a statefulSet, write data into it and set a recurring backup.&lt;/li&gt;&#xA;&lt;li&gt;The attached node to volume is power down when the recurring job backup was in progress.&lt;/li&gt;&#xA;&lt;li&gt;The cron job (if on the same node) and the pod remains in unknown state for about 7 mins and then another pod gets created for the cron job (if on the same node) and statefulSet. If cron job is on another node, it fails to complete the backup and tries to create new job to complete backup which fails with error &lt;code&gt;level=fatal msg=&amp;quot;Error taking snapshot: failed to complete backupAndCleanup for pvc-4feb233e-9503-4d4b-8cda-a5bdf005b146: could not get volume-head for volume pvc-4feb233e-9503-4d4b-8cda-a5bdf005b146: Bad response statusCode [500]. Status [500 Internal Server Error]. Body: [message=fail to get snapshot: cannot get client for volume pvc-4feb233e-9503-4d4b-8cda-a5bdf005b146: engine is not running, code=Server Error, detail=] from [http://longhorn-backend:9500/v1/volumes/pvc-4feb233e-9503-4d4b-8cda-a5bdf005b146?action=snapshotGet]&amp;quot;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;After 7 min the cron job takes over the creation of another pod of stateful set and the volume gets auto attached to another node, completes the backup and gets detached.&lt;/li&gt;&#xA;&lt;li&gt;Verify the statefulSet pod successfully reattaches to the volume after sometime.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Reusing failed replica for rebuilding</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/reusing-failed-replica-for-rebuilding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/reusing-failed-replica-for-rebuilding/</guid>
      <description>&lt;h2 id=&#34;longhorn-upgrade-with-node-down-and-removal&#34;&gt;Longhorn upgrade with node down and removal&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Launch Longhorn v1.0.x&lt;/li&gt;&#xA;&lt;li&gt;Create and attach a volume, then write data to the volume.&lt;/li&gt;&#xA;&lt;li&gt;Directly remove a Kubernetes node, and shut down a node.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the related replicas failure. Then record &lt;code&gt;replica.Spec.DiskID&lt;/code&gt; for the failed replicas.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade to Longhorn master&lt;/li&gt;&#xA;&lt;li&gt;Verify the Longhorn node related to the removed node is gone.&lt;/li&gt;&#xA;&lt;li&gt;Verify&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;code&gt;replica.Spec.DiskID&lt;/code&gt; on the down node is updated and the field of the replica on the gone node is unchanged.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;replica.Spec.DataPath&lt;/code&gt; for all replicas becomes empty.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Remove all unscheduled replicas.&lt;/li&gt;&#xA;&lt;li&gt;Power on the down node. Wait for the failed replica on the down node being reused.&lt;/li&gt;&#xA;&lt;li&gt;Wait for a new replica being replenished and available.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;replica-not-available-for-reuse-after-disk-migration&#34;&gt;Replica not available for reuse after disk migration&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy longhorn v1.1.0&lt;/li&gt;&#xA;&lt;li&gt;Create and attach a volume, then write data to the volume.&lt;/li&gt;&#xA;&lt;li&gt;Directly remove a Kubernetes node which has a replica on it.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the related replicas failure.&lt;/li&gt;&#xA;&lt;li&gt;Verify the Longhorn node related to the removed node is gone.&lt;/li&gt;&#xA;&lt;li&gt;Ssh to the node and crash the replica folder or make it readonly.&lt;/li&gt;&#xA;&lt;li&gt;Add the node in the cluster again.&lt;/li&gt;&#xA;&lt;li&gt;Verify a new replica being rebuilt and available.&lt;/li&gt;&#xA;&lt;li&gt;Verify the data of the replica.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Support Kubelet Volume Metrics</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/kubelet_volume_metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/kubelet_volume_metrics/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;&#xA;&lt;p&gt;Kubelet exposes &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/4b24dca228d61f4d13dcd57b46465b0df74571f6/pkg/kubelet/metrics/collectors/volume_stats.go#L27&#34;&gt;kubelet_volume_stats_* metrics&lt;/a&gt;.&#xA;Those metrics measure PVC&amp;rsquo;s filesystem related information inside a Longhorn block device.&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-steps&#34;&gt;Test steps:&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a cluster and set up this monitoring system: &lt;a href=&#34;https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack&#34;&gt;https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Install Longhorn. Deploy some workloads using Longhorn volumes.&#xA;Make sure there are some workloads using Longhorn PVCs in  &lt;code&gt;volumeMode: Block&lt;/code&gt; and some workloads using Longhorn PVCs in &lt;code&gt;volumeMode: Filesystem&lt;/code&gt;.&#xA;See &lt;a href=&#34;https://longhorn.io/docs/1.0.2/references/examples/&#34;&gt;https://longhorn.io/docs/1.0.2/references/examples/&lt;/a&gt; for examples.&lt;/li&gt;&#xA;&lt;li&gt;Create ingress to Prometheus server and Grafana.&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Prometheus server, verify that all Longhorn PVCs in &lt;code&gt;volumeMode: Filesystem&lt;/code&gt; show up in metrics: &lt;code&gt;kubelet_volume_stats_capacity_bytes kubelet_volume_stats_available_bytes kubelet_volume_stats_used_bytes kubelet_volume_stats_inodes kubelet_volume_stats_inodes_free kubelet_volume_stats_inodes_used&lt;/code&gt;.&lt;br&gt;&#xA;Verify that all Longhorn PVCs in &lt;code&gt;volumeMode: Block&lt;/code&gt; do not show up.&lt;/li&gt;&#xA;&lt;li&gt;Write/Delete files in Longhorn volumes.&#xA;Verify that the Prometheus server shows the correct status of Longhorn PVCs.&#xA;Verify that alerts get fired when we go above &lt;a href=&#34;https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/templates/prometheus/rules/kubernetes-storage.yaml?rgh-link-date=2020-09-24T13%3A00%3A06Z&#34;&gt;the thresholds&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Grafana, navigate to &lt;code&gt;Kubernetes/Persistent Volumes&lt;/code&gt; dashboard.&#xA;Verify that graphs report correct data.&lt;/li&gt;&#xA;&lt;li&gt;Negative test case:&#xA;use Longhorn UI to delete a volume of a running pod,&#xA;verify that the PVC corresponding volume still exists but it stops showing up in &lt;code&gt;kubelet_volume_stats_*&lt;/code&gt; metrics&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test Additional Printer Columns</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/additional-printer-columns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/additional-printer-columns/</guid>
      <description>&lt;p&gt;For each of the case below:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Fresh installation of Longhorn. (make sure to delete all Longhorn CRDs before installation)&lt;/li&gt;&#xA;&lt;li&gt;Upgrade from older version.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Run:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl get &amp;lt;LONGHORN-CRD&amp;gt; -n longhorn-system&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Verify that the output contains information as specify in the &lt;code&gt;additionalPrinerColumns&lt;/code&gt;&#xA;at &lt;a href=&#34;https://github.com/longhorn/longhorn-manager/blob/master/deploy/install/01-prerequisite/03-crd.yaml&#34;&gt;here&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Instance Manager IP Sync</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/instance-manager-ip-sync/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/instance-manager-ip-sync/</guid>
      <description>&lt;h2 id=&#34;test-step&#34;&gt;Test step:&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Launch longhorn system&lt;/li&gt;&#xA;&lt;li&gt;Create and attach a volume&lt;/li&gt;&#xA;&lt;li&gt;Follow this &lt;a href=&#34;https://github.com/longhorn/longhorn/wiki/dev:-How-to-modify-the-status-subresource-with-%60kubectl-edit%60-(CRD)&#34;&gt;doc&lt;/a&gt; to manually modify the IP of one instance-manager-r. e.g.,&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -k -XPATCH -H &amp;#34;Accept: application/json&amp;#34; -H &amp;#34;Content-Type: application/merge-patch+json&amp;#34; -H &amp;#34;Authorization: Bearer kubeconfig-xxxxxx&amp;#34; --data &amp;#39;{&amp;#34;status&amp;#34;:{&amp;#34;ip&amp;#34;:&amp;#34;1.1.1.1&amp;#34;}}&amp;#39; https://172.104.72.64/k8s/clusters/c-znrxc/apis/longhorn.io/v1beta1/namespaces/longhorn-system/instancemanagers/instance-manager-r-63ece607/status&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol&gt;&#xA;&lt;li&gt;Notice that the bearer token &lt;code&gt;kubeconfig-xxx&lt;/code&gt; can be found in your kube config file&lt;/li&gt;&#xA;&lt;li&gt;Remember to add &lt;code&gt;/status&lt;/code&gt; at the end of the URL&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Verify the IP of the instance manager still matches the pod IP&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume can be detached.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test ISCSI Installation on EKS</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/iscsi_installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/iscsi_installation/</guid>
      <description>&lt;p&gt;This is for EKS or similar users who doesn&amp;rsquo;t need to log into each host to install &amp;lsquo;ISCSI&amp;rsquo; individually.&lt;/p&gt;&#xA;&lt;p&gt;Test steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create an EKS cluster with 3 nodes.&lt;/li&gt;&#xA;&lt;li&gt;Run the following command to install iscsi on every nodes.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/iscsi/longhorn-iscsi-installation.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;In Longhorn Manager Repo Directory run:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl apply -Rf ./deploy/install/&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;Longhorn should be able installed successfully.&lt;/li&gt;&#xA;&lt;li&gt;Try to create a pod with a pvc:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/examples/simple_pvc.yaml&#xA;kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/examples/simple_pod.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;6&#34;&gt;&#xA;&lt;li&gt;Check the pod is created successfully with a valid Longhorn volume mounted.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test Read Write Many Feature</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/rwx_feature/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/rwx_feature/</guid>
      <description>&lt;h1 id=&#34;prerequisite&#34;&gt;Prerequisite:&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Set up a Cluster of 4 nodes (1 etc/control plane and 3 workers)&lt;/li&gt;&#xA;&lt;li&gt;Deploy Latest Longhorn-master&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-statefulsetdeployment-with-single-pod-with-volume-attached-in-rwx-mode&#34;&gt;Create StatefulSet/Deployment with single pod with volume attached in RWX mode.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 1 pod.&lt;/li&gt;&#xA;&lt;li&gt;Verify that a PVC, ShareManger pod, CRD and volume in Longhorn get created.&lt;/li&gt;&#xA;&lt;li&gt;Verify share-manager pod come up healthy.&lt;/li&gt;&#xA;&lt;li&gt;Verify there is directory with the name of PVC exists in the ShareManager mount point.&#xA;Example - &lt;code&gt;ls /export/pvc-8c3481c7-4127-47c3-b840-5f41dc9d603e&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod and verify the same data reflects in the ShareManager.&lt;/li&gt;&#xA;&lt;li&gt;Verify the longhorn volume, it should reflect the correct size.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-statefulsetdeployment-with-more-than-1-pod-with-volume-attached-in-rwx-mode&#34;&gt;Create StatefulSet/Deployment with more than 1 pod with volume attached in RWX mode.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Verify that 2 volumes in Longhorn get created.&lt;/li&gt;&#xA;&lt;li&gt;Verify there is directory with the name of PVC exists in the ShareManager mount point i.e. &lt;code&gt;export&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn UI shows all the pods name attached to the volume.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in all the pod and verify all the data reflects in the ShareManager.&lt;/li&gt;&#xA;&lt;li&gt;Verify the longhorn volume, it should reflect the correct size.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-statefulsetdeployment-with-the-existing-pvc-of-a-rwx-volume&#34;&gt;Create StatefulSet/Deployment with the existing PVC of a RWX volume.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 1 pod.&lt;/li&gt;&#xA;&lt;li&gt;Verify that a PVC, ShareManger pod, CRD and volume in Longhorn get created.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod and verify the same data reflects in the ShareManager.&lt;/li&gt;&#xA;&lt;li&gt;Create another StatefulSet/Deployment using the above created PVC.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the new pod, the same should be reflected in the ShareManager pod.&lt;/li&gt;&#xA;&lt;li&gt;Verify the longhorn volume, it should reflect the correct size.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;scale-up-statefulsetdeployment-with-one-pod-attached-with-volume-in-rwx-mode&#34;&gt;Scale up StatefulSet/Deployment with one pod attached with volume in RWX mode.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 1 pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod and verify the same data reflects in the ShareManager.&lt;/li&gt;&#xA;&lt;li&gt;Scale up the StatefulSet/Deployment.&lt;/li&gt;&#xA;&lt;li&gt;Verify a new volume gets created.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the new pod, the same should be reflected in the ShareManager pod.&lt;/li&gt;&#xA;&lt;li&gt;Verify the longhorn volume, it should reflect the correct size.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;scale-down-statefulsetdeployment-attached-with-volume-in-rwx-mode-to-zero&#34;&gt;Scale down StatefulSet/Deployment attached with volume in RWX mode to zero.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 1 pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod and verify the same data reflects in the ShareManager.&lt;/li&gt;&#xA;&lt;li&gt;Scale down the StatefulSet/Deployment to zero&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager pod gets deleted.&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume should be in detached state.&lt;/li&gt;&#xA;&lt;li&gt;Create a new StatefulSet/Deployment with the existing PVC.&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager should get created and volume should become attached.&lt;/li&gt;&#xA;&lt;li&gt;Verify the data.&lt;/li&gt;&#xA;&lt;li&gt;Delete the newly created StatefulSet/Deployment.&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager pod gets deleted again.&lt;/li&gt;&#xA;&lt;li&gt;Scale up the first StatefulSet/Deployment.&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager should get created and volume should become attached.&lt;/li&gt;&#xA;&lt;li&gt;Verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;delete-the-workload-statefulsetdeployment-attached-with-rwx-volume&#34;&gt;Delete the Workload StatefulSet/Deployment attached with RWX volume.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod and verify the same data reflects in the ShareManager.&lt;/li&gt;&#xA;&lt;li&gt;Delete the workload.&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager pod gets deleted but the CRD should not be deleted.&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager.status.state == &amp;ldquo;stopped&amp;rdquo;. &lt;code&gt;kubectl get ShareManager -n longhorn-system&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume should be in detached state.&lt;/li&gt;&#xA;&lt;li&gt;Create another StatefulSet with existing PVC.&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager should get created and volume should become attached.&lt;/li&gt;&#xA;&lt;li&gt;Verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;take-snapshot-and-backup-of-a-rwx-volume-in-longhorn&#34;&gt;Take snapshot and backup of a RWX volume in Longhorn.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a snapshot and a backup.&lt;/li&gt;&#xA;&lt;li&gt;Write some more data into the pod.&lt;/li&gt;&#xA;&lt;li&gt;Revert to snapshot 1 and verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;restore-a-backup-taken-from-a-rwx-volume-in-longhorn&#34;&gt;Restore a backup taken from a RWX volume in Longhorn.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup of the RWX volume.&lt;/li&gt;&#xA;&lt;li&gt;Restore from the backup and select access mode as &lt;code&gt;rwx&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Verify the restored volume has &lt;code&gt;volume.spec.accessMode&lt;/code&gt; as &lt;code&gt;rwx&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Create PV/PVC with &lt;code&gt;accessMode&lt;/code&gt; as &lt;code&gt;rwx&lt;/code&gt; for restored volume or create PV/PVC using Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Attach a pod to the PVC created and verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;restore-an-rwx-backup-into-an-rwo-volume&#34;&gt;Restore an RWX backup into an RWO volume.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup of the RWX volume.&lt;/li&gt;&#xA;&lt;li&gt;Restore from the backup and select access mode as &lt;code&gt;rwo&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Verify the restored volume has &lt;code&gt;volume.spec.accessMode&lt;/code&gt; as &lt;code&gt;rwo&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Create a PV and PVC with &lt;code&gt;accessMode&lt;/code&gt; as &lt;code&gt;rwo&lt;/code&gt; for the restored volume or create them using Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Attach a pod to the PVC and verify the data.&lt;/li&gt;&#xA;&lt;li&gt;Try to attach the PVC to another pod on another node, user should get &lt;code&gt;multi attach&lt;/code&gt; error.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;restore-an-rwo-backup-into-an-rwx-volume&#34;&gt;Restore an RWO backup into an RWX volume.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWO mode using longhorn class by selecting the option &lt;code&gt;read write once&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 1 pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup of the RWO volume.&lt;/li&gt;&#xA;&lt;li&gt;Restore from the backup and select access mode as &lt;code&gt;rwx&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Create a PV and PVC with &lt;code&gt;accessMode&lt;/code&gt; as &lt;code&gt;rwx&lt;/code&gt; for the restored volume or create them using Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify the restored volume has &lt;code&gt;volume.spec.accessMode&lt;/code&gt; as &lt;code&gt;rwx&lt;/code&gt; now.&lt;/li&gt;&#xA;&lt;li&gt;Attach a pod to the PVC and verify the data.&lt;/li&gt;&#xA;&lt;li&gt;Attach more pods to the PVC, verify the volume is accessible from multiple pods.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-pv-and-pvc-using-longhorn-ui-for-rwxrwo-volume&#34;&gt;Create PV and PVC using Longhorn UI for RWX/RWO volume&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create an RWX volume using Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Select the volume and create PV/PVC.&lt;/li&gt;&#xA;&lt;li&gt;Verify the PV and PVC are created with &lt;code&gt;rwx&lt;/code&gt; access mode.&lt;/li&gt;&#xA;&lt;li&gt;Create an RWO volume using Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Select the volume and create PV/PVC.&lt;/li&gt;&#xA;&lt;li&gt;Verify the PV and PVC are created with &lt;code&gt;rwo&lt;/code&gt; access mode.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-rwx-dr-volume-of-a-rwx-volume-in-longhorn&#34;&gt;Create RWX DR volume of a RWX volume in Longhorn.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup of the volume.&lt;/li&gt;&#xA;&lt;li&gt;Create a DR volume of the backup by selecting &lt;code&gt;rwx&lt;/code&gt; in access mode.&lt;/li&gt;&#xA;&lt;li&gt;Write more data in the pods and take more backups.&lt;/li&gt;&#xA;&lt;li&gt;Verify the DR volume is getting synced with latest backup.&lt;/li&gt;&#xA;&lt;li&gt;Activate the DR volume, attach it to multiple pods and verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-rwo-dr-volume-of-a-rwx-volume-in-longhorn&#34;&gt;Create RWO DR volume of a RWX volume in Longhorn.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup of the volume.&lt;/li&gt;&#xA;&lt;li&gt;Create a DR volume of the backup by selecting &lt;code&gt;rwo&lt;/code&gt; in access mode.&lt;/li&gt;&#xA;&lt;li&gt;Write more data in the pods and take more backups.&lt;/li&gt;&#xA;&lt;li&gt;Verify the DR volume is getting synced with latest backup.&lt;/li&gt;&#xA;&lt;li&gt;Activate the DR volume, attach it to a pod and verify the data.&lt;/li&gt;&#xA;&lt;li&gt;Try to attach it to multiple pods, it should show &lt;code&gt;multi attach error&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-rwx-dr-volume-of-a-rwo-volume-in-longhorn&#34;&gt;Create RWX DR volume of a RWO volume in Longhorn.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWO mode using longhorn class by selecting the option &lt;code&gt;read write once&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 1 pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup of the volume.&lt;/li&gt;&#xA;&lt;li&gt;Create a DR volume of the backup by selecting &lt;code&gt;rwx&lt;/code&gt; in access mode.&lt;/li&gt;&#xA;&lt;li&gt;Write more data in the pod and take more backups.&lt;/li&gt;&#xA;&lt;li&gt;Verify the DR volume is getting synced with latest backup.&lt;/li&gt;&#xA;&lt;li&gt;Activate the DR volume, attach it to multiple pods and verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;expand-the-rwx-volume&#34;&gt;Expand the RWX volume.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Scale down the StatefulSet/Deployment.&lt;/li&gt;&#xA;&lt;li&gt;Once the volume is detached, expand the volume.&lt;/li&gt;&#xA;&lt;li&gt;Scale up the StatefulSet/Deployment and verify that user is able to write data in the expanded volume.&lt;/li&gt;&#xA;&lt;li&gt;Verify the new size of the volume (same approach as in writing the data).&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;recurring-backupsnapshot-with-rwx-volume&#34;&gt;Recurring Backup/Snapshot with RWX volume.&lt;/h1&gt;&#xA;&lt;p&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&#xA;2. Attach the PVC to a StatefulSet/Deployment with 2 pods.&#xA;3. Write some data in the pod.&#xA;4. Schedule a recurring backup/Snapshot.&#xA;5. Verify the recurring jobs are getting created and is taking backup/snapshot successfully.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test uninstallation</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/uninstallation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/uninstallation/</guid>
      <description>&lt;h2 id=&#34;stability-of-uninstallation&#34;&gt;Stability of uninstallation&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Launch Longhorn system.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Use scripts to continuously create then delete multiple DaemonSets.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;e.g., putting the following python test into the manager integration test directory and run it:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from common import get_apps_api_client # NOQA&#xA;&#xA;&#xA;def test_uninstall_script():&#xA;    apps_api = get_apps_api_client()&#xA;    while True:&#xA;        for i in range(10):&#xA;            name = &amp;#34;ds-&amp;#34; + str(i)&#xA;            try:&#xA;                ds = apps_api.read_namespaced_daemon_set(name, &amp;#34;default&amp;#34;)&#xA;                if ds.status.number_ready == ds.status.number_ready:&#xA;                    apps_api.delete_namespaced_daemon_set(name, &amp;#34;default&amp;#34;)&#xA;            except Exception:&#xA;                apps_api.create_namespaced_daemon_set(&#xA;                    &amp;#34;default&amp;#34;, ds_manifest(name))&#xA;&#xA;&#xA;def ds_manifest(name):&#xA;    return {&#xA;        &amp;#39;apiVersion&amp;#39;: &amp;#39;apps/v1&amp;#39;,&#xA;        &amp;#39;kind&amp;#39;: &amp;#39;DaemonSet&amp;#39;,&#xA;        &amp;#39;metadata&amp;#39;: {&#xA;            &amp;#39;name&amp;#39;: name&#xA;        },&#xA;        &amp;#39;spec&amp;#39;: {&#xA;            &amp;#39;selector&amp;#39;: {&#xA;                &amp;#39;matchLabels&amp;#39;: {&#xA;                    &amp;#39;app&amp;#39;: name&#xA;                }&#xA;            },&#xA;            &amp;#39;template&amp;#39;: {&#xA;                &amp;#39;metadata&amp;#39;: {&#xA;                    &amp;#39;labels&amp;#39;: {&#xA;                        &amp;#39;app&amp;#39;: name&#xA;                    }&#xA;                },&#xA;                &amp;#39;spec&amp;#39;: {&#xA;                    &amp;#39;terminationGracePeriodSeconds&amp;#39;: 10,&#xA;                    &amp;#39;containers&amp;#39;: [{&#xA;                        &amp;#39;image&amp;#39;: &amp;#39;busybox&amp;#39;,&#xA;                        &amp;#39;imagePullPolicy&amp;#39;: &amp;#39;IfNotPresent&amp;#39;,&#xA;                        &amp;#39;name&amp;#39;: &amp;#39;sleep&amp;#39;,&#xA;                        &amp;#39;args&amp;#39;: [&#xA;                            &amp;#39;/bin/sh&amp;#39;,&#xA;                            &amp;#39;-c&amp;#39;,&#xA;                            &amp;#39;while true;do date;sleep 5; done&amp;#39;&#xA;                        ],&#xA;                    }]&#xA;                }&#xA;            },&#xA;        }&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Start to uninstall longhorn.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Upgrade Longhorn with modified Storage Class</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/upgrade_with_modified_storageclass/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/upgrade_with_modified_storageclass/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;&#xA;&lt;p&gt;Longhorn can be upgraded with modified Storage Class.&lt;/p&gt;&#xA;&lt;h2 id=&#34;related-issue&#34;&gt;Related Issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1527&#34;&gt;https://github.com/longhorn/longhorn/issues/1527&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-steps&#34;&gt;Test steps:&lt;/h2&gt;&#xA;&lt;h3 id=&#34;kubectl-apply--f&#34;&gt;Kubectl apply -f&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn v1.0.2&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.0.2/deploy/longhorn.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a statefulset using &lt;code&gt;longhorn&lt;/code&gt; storageclass for PVCs. Set the scale to 1.&lt;/li&gt;&#xA;&lt;li&gt;Observe that there is a workload pod (&lt;code&gt;pod-1&lt;/code&gt;) is using 1 volume (&lt;code&gt;vol-1&lt;/code&gt;) with 3 replicas.&lt;/li&gt;&#xA;&lt;li&gt;In Longhorn repo, on &lt;code&gt;master&lt;/code&gt; branch. Modify &lt;code&gt;numberOfReplicas: &amp;quot;1&amp;quot;&lt;/code&gt; in &lt;code&gt;https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml&lt;/code&gt;. Upgrade Longhorn to master by running&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn&lt;/code&gt; storage class now has the field &lt;code&gt;numberOfReplicas: 1&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Scale up the deployment to 2. Verify that:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;pod-1&lt;/code&gt; is using volume &lt;code&gt;vol-1&lt;/code&gt; with 3 replicas.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;pod-2&lt;/code&gt; is using volume &lt;code&gt;vol-2&lt;/code&gt; with 1 replica.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Scale up the deployment to 0 then back to 2. Verify that:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;pod-1&lt;/code&gt; is using volume &lt;code&gt;vol-1&lt;/code&gt; with 3 replicas.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;pod-2&lt;/code&gt; is using volume &lt;code&gt;vol-2&lt;/code&gt; with 1 replica.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;helm&#34;&gt;Helm&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Follow this instruction to install Longhorn v1.0.2 using Helm &lt;a href=&#34;https://longhorn.io/docs/1.0.2/deploy/install/install-with-helm/#installing-longhorn&#34;&gt;https://longhorn.io/docs/1.0.2/deploy/install/install-with-helm/#installing-longhorn&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
