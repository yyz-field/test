<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Functional test cases on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/</link>
    <description>Recent content in Functional test cases on Longhorn Manual Test Cases</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1. Deployment of Longhorn</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/deployment/</guid>
      <description>&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&lt;p&gt;Longhorn v1.1.2 and above - Support Kubernetes 1.18+&lt;/p&gt;&#xA;&lt;p&gt;Longhorn v1.0.0 to v1.1.1 - Support Kubernetes 1.14+. Default 1.16+&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Install using Rancher Apps &amp;amp; MarketPlace App (Default)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Install using Helm chart from &lt;a href=&#34;https://github.com/longhorn/longhorn/tree/master/chart&#34;&gt;https://github.com/longhorn/longhorn/tree/master/chart&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Install using YAML from &lt;a href=&#34;https://github.com/longhorn/longhorn/blob/master/deploy/longhorn.yaml&#34;&gt;https://github.com/longhorn/longhorn/blob/master/deploy/longhorn.yaml&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Note: Longhorn UI can scale to multiple instances for HA purposes.&lt;/p&gt;&#xA;&lt;h2 id=&#34;uninstallation&#34;&gt;Uninstallation&lt;/h2&gt;&#xA;&lt;p&gt;Make sure all the CRDs and other resources are cleaned up, following the uninstallation instruction.&#xA;&lt;a href=&#34;https://longhorn.io/docs/1.2.2/deploy/uninstall/&#34;&gt;https://longhorn.io/docs/1.2.2/deploy/uninstall/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;customizable-default-settings&#34;&gt;Customizable Default Settings&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://longhorn.io/docs/1.2.2/references/settings/&#34;&gt;https://longhorn.io/docs/1.2.2/references/settings/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Make sure the settings are updated if it’s the fresh installation of Longhorn.&lt;/p&gt;</description>
    </item>
    <item>
      <title>2. UI</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/ui/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/ui/</guid>
      <description>&lt;h2 id=&#34;accessibility-of-longhorn-ui&#34;&gt;Accessibility of Longhorn UI&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1.&lt;/td&gt;&#xA;          &lt;td&gt;Access Longhorn UI using rancher proxy&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a cluster (3 worker nodes and 1 etcd/control plane) in rancher, Go to the default project.&lt;br&gt;2.  Go to App, Click the launch app.&lt;br&gt;3.  Select longhorn.&lt;br&gt;4.  Select &lt;code&gt;Rancher-Proxy&lt;/code&gt; under the Longhorn UI service.&lt;br&gt;5.  Once the app is deployed successfully, click the &lt;a href=&#34;https://173.255.255.35/k8s/clusters/c-qnl4b/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/&#34;&gt;/index.html&lt;/a&gt; link appears in App page.&lt;br&gt;6.  The page should redirect to longhorn UI - &lt;a href=&#34;https://173.255.255.35/k8s/clusters/c-qnl4b/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/#/dashboard&#34;&gt;https://rancher/k8s/clusters/c-aaaa/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/#/dashboard&lt;/a&gt;&lt;br&gt;7.  Verify all the pages, refresh each page and verify. Create a volume and check the volume detail page also.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2.&lt;/td&gt;&#xA;          &lt;td&gt;Access Longhorn UI under Kubectl proxy&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a cluster (3 worker nodes and 1 etcd/control plane) using rke.&lt;br&gt;2.  Start kubectl proxy by command &lt;code&gt;kubectl proxy&lt;/code&gt;.&lt;br&gt;3.  It should start proxy locally on 8001 port.&lt;br&gt;4.  Navigate to &lt;a href=&#34;http://localhost:8001/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:http/proxy/&#34;&gt;http://localhost:8001/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:http/proxy/&lt;/a&gt;&lt;br&gt;5.  The above link should open the longhorn UI.&lt;br&gt;6.  Verify all the pages, refresh each page and verify. Create a volume and check the volume detail page also.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3.&lt;/td&gt;&#xA;          &lt;td&gt;Access Longhorn UI with node port&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a cluster (3 worker nodes and 1 etcd/control plane) in rancher, Go to the default project.&lt;br&gt;2.  Go to App, Click the launch app.&lt;br&gt;3.  Select longhorn.&lt;br&gt;4.  Select &lt;code&gt;NodePort&lt;/code&gt; under the Longhorn UI service.&lt;br&gt;5.  Once the app is deployed successfully, click the link like &lt;a href=&#34;http://104.131.80.163:32059/&#34;&gt;32059/tcp&lt;/a&gt; appears in App page.&lt;br&gt;6.  The page should redirect to longhorn UI - &lt;a href=&#34;http://104.131.80.163:32059/#/dashboard&#34;&gt;http://node-ip:32059/#/dashboard&lt;/a&gt;&lt;br&gt;7.  Verify all the pages, refresh each page and verify. Create a volume and check the volume detail page also.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4.&lt;/td&gt;&#xA;          &lt;td&gt;Access Longhorn UI with ingress controller&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a cluster(3 worker nodes and 1 etcd/control plane).&lt;br&gt;2.  Deploy longhorn.&lt;br&gt;3.  Create ingress controller. refer &lt;a href=&#34;https://longhorn.io/docs/1.0.1/deploy/accessing-the-ui/longhorn-ingress/&#34;&gt;https://longhorn.io/docs/1.0.1/deploy/accessing-the-ui/longhorn-ingress/&lt;/a&gt;&lt;br&gt;4.  If cluster is imported/created in rancher create ingress using rancher UI by selecting &lt;code&gt;Target Backend&lt;/code&gt; as &lt;code&gt;longhorn frontend&lt;/code&gt; and path &lt;code&gt;/&lt;/code&gt;&lt;br&gt;5.  Access the ingress. It should redirect to longhorn UI.&lt;br&gt;6.  Verify all the pages, refresh each page and verify. Create a volume and check the volume detail page also.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5.&lt;/td&gt;&#xA;          &lt;td&gt;Access Longhorn UI with a load balancer&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a cluster (3 worker nodes and 1 etcd/control plane) in rancher.&lt;br&gt;2.  Create a route 53 entry pointing to worker nodes of the cluster in AWS.&lt;br&gt;3.  Deploy longhorn from catalog library and mention the route 53 entry in the load balancer.&lt;br&gt;4.  Go to the link that appears on the app page for the longhorn app.&lt;br&gt;5.  The page to redirect to longhorn UI with URL as route 53 entry.&lt;br&gt;6.  Verify all the pages, refresh each page and verify. Create a volume and check the volume detail page also.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6.&lt;/td&gt;&#xA;          &lt;td&gt;Access Longhorn UI with reverse proxy&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a cluster (3 worker nodes and 1 etcd/control plane) in rancher, Go to the default project.&lt;br&gt;2.  Go to App, Click the launch app.&lt;br&gt;3.  Select longhorn.&lt;br&gt;4.  Select &lt;code&gt;NodePort&lt;/code&gt; under the Longhorn UI service.&lt;br&gt;5.  Install nginx in local system.&lt;br&gt;6.  Set the &lt;code&gt;proxy_pass&lt;/code&gt; of &lt;a href=&#34;http://104.131.80.163:32059/#/dashboard&#34;&gt;http://node-ip:32059&lt;/a&gt; in ngnix.conf file as per below example.&lt;br&gt;7.  Start nginx&lt;br&gt;8.  Access the port given in &lt;code&gt;listen&lt;/code&gt; parameter from nginx.conf. ex - //localhost:822&lt;br&gt;9.  The page should redirect to longhorn UI&lt;br&gt;10.  Verify all the pages, refresh each page and verify. Create a volume and check the volume detail page also.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;nginx.conf example&lt;/p&gt;</description>
    </item>
    <item>
      <title>3. Volume</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/volume/</guid>
      <description>&lt;h3 id=&#34;test-cases-for-volume&#34;&gt;Test cases for Volume&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Check volume Details&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Longhorn Nodes has node tags&lt;br&gt;*   Node Disks has disk tags&lt;br&gt;*   Backup target is set to NFS server, or S3 compatible target&lt;br&gt;&lt;br&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Check volume details page&lt;br&gt;3.  Create volume backup&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume Details&lt;br&gt;    *   &lt;code&gt;State&lt;/code&gt; should be &lt;code&gt;Attached&lt;/code&gt;&lt;br&gt;    *   &lt;code&gt;Health&lt;/code&gt; should be healthy&lt;br&gt;    *   &lt;code&gt;Frontend&lt;/code&gt; should be &lt;code&gt;Block Device&lt;/code&gt;&lt;br&gt;    *   &lt;code&gt;Attached Node &amp;amp; Endpoint&lt;/code&gt; should be node name that volume is attached to and PATH of the volume device file on that node.&lt;br&gt;    *   &lt;code&gt;Size&lt;/code&gt; should match volume size specified in Create Volume step&lt;br&gt;    *   &lt;code&gt;Actual Size&lt;/code&gt; should be &lt;code&gt;0Bi&lt;/code&gt; (No data has been written to the volume yet)&lt;br&gt;    *   &lt;code&gt;Engine Image&lt;/code&gt; should be &lt;code&gt;longhornio/longhorn-engine:&amp;lt;LONGHORN_VERSION&amp;gt;&lt;/code&gt;&lt;br&gt;    *   &lt;code&gt;Created&lt;/code&gt; should indicate time since volume is created.&lt;br&gt;    *   &lt;code&gt;Node Tags&lt;/code&gt; should be empty (no node tags has been specified during creation)&lt;br&gt;    *   &lt;code&gt;Disk Tags&lt;/code&gt; should be empty (no disk tags has been specified during creation)&lt;br&gt;    *   &lt;code&gt;Last Backup&lt;/code&gt; should be empty (no backup has been created)&lt;br&gt;    *   &lt;code&gt;Last Backup At&lt;/code&gt; should be empty (no backup has been created)&lt;br&gt;    *   &lt;code&gt;Instance Manager&lt;/code&gt; should contain instance manager image name&lt;br&gt;    *   &lt;code&gt;Namespace&lt;/code&gt; should match namespace specified in Volume Create step.&lt;br&gt;    *   &lt;code&gt;PVC Name&lt;/code&gt; should be empty (no PV has been created for that volume yet)&lt;br&gt;    *   &lt;code&gt;PV Name&lt;/code&gt; should be empty (no PV has been created for that volume yet)&lt;br&gt;    *   &lt;code&gt;PV Status&lt;/code&gt; should be empty (no PV/PVC has been created for that volume yet)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Filter Volumes&lt;/td&gt;&#xA;          &lt;td&gt;User should be able to filter volumes using the following filters&lt;br&gt;&lt;br&gt;*   Name&lt;br&gt;*   Node&lt;br&gt;*   Status (Healthy, In progress, Degraded, Faulted, detached)&lt;br&gt;*   Namespace&lt;br&gt;*   Node redundancy (Yes, Limited, No)&lt;br&gt;*   PV Name&lt;br&gt;*   PVC Name&lt;br&gt;*   Node tag&lt;br&gt;*   Disk tag&lt;br&gt;&lt;br&gt;Notes:&lt;br&gt;&lt;br&gt;*   Limited node redundancy: at least one healthy replica is running at the same node as another&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume list should match filtering criteria applied.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Delete multiple volumes&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;    *   Create multiple volumes&lt;br&gt;&lt;br&gt;1.  Select multiple volumes and delete&lt;/td&gt;&#xA;          &lt;td&gt;*   Volumes should be deleted&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Attach multiple volumes&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;    *   Create multiple volumes&lt;br&gt;&lt;br&gt;1.  Select multiple volumes and Attach them to a node&lt;/td&gt;&#xA;          &lt;td&gt;*   All Volumes should be attached to the same node specified in volume attach request.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Attach multiple volumes in maintenance mode&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;    *   Create multiple volumes&lt;br&gt;&lt;br&gt;1.  Select multiple volumes and Attach them to a node in maintenance mode&lt;/td&gt;&#xA;          &lt;td&gt;*   All Volumes should be attached in maintenance mode to the same node specified in volume attach request.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Detach multiple volumes&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;    *   Multiple attached volumes&lt;br&gt;*   Select multiple volumes and detach&lt;/td&gt;&#xA;          &lt;td&gt;*   Volumes should be detached&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Backup multiple Volumes&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;    *   Longhorn should be configured to point to a backupstore&lt;br&gt;    *   Multiple volumes existed and attached to node/used buy kubernetes workload&lt;br&gt;    *   Write some data to multiple volumes and compute it’s checksum&lt;br&gt;*   Select multiple volumes and Create a backup&lt;br&gt;*   restore volumes backups and check its data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume backups should be created&lt;br&gt;*   Restored volumes from backup should contain the same data when backup is created&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Create PV/PVC for multiple volumes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create multiple volumes&lt;br&gt;&lt;br&gt;1.  Select multiple volumes&lt;br&gt;2.  Create a PV, specify filesysem&lt;br&gt;3.  Check PV in Lonhgorn UI and in Kubernetes&lt;br&gt;4.  Create PVC&lt;br&gt;5.  Check PVC in Lonhgorn UI and in Kubernetes&lt;br&gt;6.  Delete PVC&lt;br&gt;7.  Check PV in Lonhgorn UI and in Kubernetes&lt;/td&gt;&#xA;          &lt;td&gt;*   For all selected volumes&lt;br&gt;    *   PV should created&lt;br&gt;    *   PV/PVC status in UI should be &lt;code&gt;Available&lt;/code&gt;&lt;br&gt;    *   PV &lt;code&gt;spec.csi.fsType&lt;/code&gt; should match filesystem specified in PV creation request&lt;br&gt;    *   PV &lt;code&gt;spec.storageClassName&lt;/code&gt; should match the setting in &lt;code&gt;Default Longhorn Static StorageClass Name&lt;/code&gt;&lt;br&gt;    *   PV &lt;code&gt;spec.csi.volumeHandle&lt;/code&gt; should be the volume name&lt;br&gt;    *   PV/PVC status in UI should be &lt;code&gt;Bound&lt;/code&gt; in Longhorn UI&lt;br&gt;    *   PVC namespace should match namespace specified in PVC creation request&lt;br&gt;    *   After Deleting PVC, PV/PVC status should be &lt;code&gt;Relased&lt;/code&gt; in Longhorn UI.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Volume expansion&lt;/td&gt;&#xA;          &lt;td&gt;Check Multiple Volume expansion test cases work for multiple volumes&lt;br&gt;&lt;br&gt;&lt;a href=&#34;https://rancher.atlassian.net/wiki/spaces/LON/pages/354453117/Volume+detail+page&#34;&gt;Test Cases in Volume Details page&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Volume expansion should work for multiple volumes.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Engine Offline Upgrade For Multiple Volumes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Volume is consumed by Kubernetes deployment workload&lt;br&gt;*   Volume use old Longhorn Engine&lt;br&gt;&lt;br&gt;1.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;2.  Scale down deployment , volume gets detached&lt;br&gt;3.  Upgrade Longhorn engine image to use new deployed engine image&lt;br&gt;4.  Scale up deployment, volume gets attached&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume read/write operations should work before and after engine upgrade.&lt;br&gt;*   Old Engine &lt;code&gt;Reference Count&lt;/code&gt; will be decreased by 1&lt;br&gt;*   New Engine &lt;code&gt;Reference Count&lt;/code&gt; will be increased by 1&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;Show System Hidden&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;:&lt;br&gt;&lt;br&gt;*   Volume is created and attached to a pod.&lt;br&gt;&lt;br&gt;1.  Click the volume appearing on volume list page, it takes user to volume.&lt;br&gt;2.  Take snapshot and upgrade the replicas.&lt;br&gt;3.  Under snapshot section, enable option &amp;lsquo;Show System Hidden&lt;/td&gt;&#xA;          &lt;td&gt;Enabling this option will show system created snapshots while rebuilding of replicas.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;Event log&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;:&lt;br&gt;&lt;br&gt;*   Volume is created and attached to a pod.&lt;br&gt;&lt;br&gt;1.  Click event log to expand&lt;/td&gt;&#xA;          &lt;td&gt;Verify details appearing in logs.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;replica&#34;&gt;Replica&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Automated ? / test name&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Replica list&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a volume and change the default number of replicas&lt;br&gt;2.  Attach volume to a node&lt;br&gt;3.  Check replica list&lt;/td&gt;&#xA;          &lt;td&gt;*   Number of replicas should match number specified in volume creation request&lt;br&gt;*   All replicas should be &lt;code&gt;Running&lt;/code&gt;, and &lt;code&gt;Healthy&lt;/code&gt;&lt;br&gt;*   Replica info also should contain, &lt;code&gt;Node Name&lt;/code&gt;, &lt;code&gt;Replica Instance Manager Name&lt;/code&gt;, &lt;code&gt;Replica Path&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;test_volume_update_replica_count&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Update volume replica count (increase)&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a volume&lt;br&gt;2.  Attach volume to a node&lt;br&gt;3.  Increase replica count +1&lt;/td&gt;&#xA;          &lt;td&gt;*   New system hidden snapshot should be created&lt;br&gt;*   A new replica should be created&lt;br&gt;*   New replicas should be &lt;code&gt;Running&lt;/code&gt; &amp;amp; &lt;code&gt;Rebuilding&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;test_volume_update_replica_count&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Update volume replica count (decrease)&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a volume&lt;br&gt;2.  Attach volume to a node&lt;br&gt;3.  decrease replica count by +1&lt;br&gt;4.  Delete a replica&lt;/td&gt;&#xA;          &lt;td&gt;*   After decreasing replica count, nothing should happen&lt;br&gt;*   Deleting a replica will not trigger replica rebuild&lt;/td&gt;&#xA;          &lt;td&gt;test_volume_update_replica_count&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;snapshot&#34;&gt;Snapshot&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Create Snapshot&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume head should have parent as snapshot#1&lt;br&gt;*   Snapshot should be created&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Revert Snapshot&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a deployment workload with &lt;code&gt;nReplicas = 1&lt;/code&gt; using Longhorn volume&lt;br&gt;2.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;3.  Write some other data, compute it’s checksum (checksum#2)&lt;br&gt;4.  Create a snapshot (snapshot#1)&lt;br&gt;5.  Scale down deployment &lt;code&gt;nReplicas = 0&lt;/code&gt;&lt;br&gt;6.  Attach volume in &lt;code&gt;maintenance mode&lt;/code&gt;&lt;br&gt;7.  Revert to (snapshot#1)&lt;br&gt;8.  Detach volume&lt;br&gt;9.  Scale back deployment &lt;code&gt;nReplicas = 1&lt;/code&gt;&lt;br&gt;10.  Compute data checksum (checksum#3)&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume head should have parent as snapshot#1&lt;br&gt;*   Volume state should be &lt;code&gt;Detached&lt;/code&gt; after scaling down deployment &lt;code&gt;nReplicas = 0&lt;/code&gt;&lt;br&gt;*   In Volume Details &lt;code&gt;Attached Node&lt;/code&gt; should be Node name which volume is attached to, without an &lt;code&gt;Endpoint&lt;/code&gt; (block device path)&lt;br&gt;*   Data checksum after revert should match data checksum when taking snapshot &lt;code&gt;checksum#3 == checksum#1&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Delete Snapshot&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)&lt;br&gt;5.  Write data to volume, compute it’s checksum (checksum#4) → live data&lt;br&gt;6.  Delete (snapshot#2)&lt;br&gt;7.  Revert to (snapshot#3)&lt;br&gt;8.  Revert to (snapshot#1)&lt;/td&gt;&#xA;          &lt;td&gt;*   Snapshot#2 will be deleted, verify in replica /var/lib/rancher/longhorn/replicas/&lt;br&gt;*   After reverting to snapshot#3 verify the data.&lt;br&gt;*   After reverting to snapshot#1 data checksum should match (checksum#1)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Delete Snapshot while rebuilding replicas&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume (1GB+), compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Delete a replica&lt;br&gt;5.  while replica is rebuilding, try to delete (snapshot#1)&lt;/td&gt;&#xA;          &lt;td&gt;*   New system snapshot should be created&lt;br&gt;*   Will &lt;strong&gt;NOT&lt;/strong&gt; be able to delete snapshot#1&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Snapshot Purge&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume (1GB+), compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Delete a replica&lt;br&gt;5.  After rebuild is complete, delete (snapshot#1)&lt;/td&gt;&#xA;          &lt;td&gt;*   New system snapshot should be created&lt;br&gt;*   Snapshot#1 will be delete&lt;br&gt;*   Snapshot purge process will be triggered&lt;br&gt;*   Only system snapshot should be present&lt;br&gt;*   You will not be able revert to the system snapshot&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Create recurring snapshots&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a deployment workload with &lt;code&gt;nReplicas = 1&lt;/code&gt; using Longhorn volume&lt;br&gt;2.  Write data to volume , compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a recurring snapshot &lt;code&gt;every 5 minutes&lt;/code&gt;. and set retain count to &lt;code&gt;5&lt;/code&gt;&lt;br&gt;4.  Wait for 2 recurring snapshots to triggered (snapshot#1, snapshot#2 )&lt;br&gt;5.  Scale down deployment &lt;code&gt;nReplicas = 0&lt;/code&gt;&lt;br&gt;6.  Attach volume in &lt;code&gt;maintenance mode&lt;/code&gt;&lt;br&gt;7.  Revert to (snapshot#1)&lt;br&gt;8.  Scale back deployment &lt;code&gt;nReplicas = 1&lt;/code&gt;&lt;br&gt;9.  Wait for another recurring snapshots to triggered (snapshot#3)&lt;br&gt;10.  Delete (snapshot#1)&lt;/td&gt;&#xA;          &lt;td&gt;*   Snapshots (snapshot#1, snapshot#2) should be created&lt;br&gt;*   Before deleting (snapshot#1), Parent snapshot of (snapshot#3) should be (snapshot#1)&lt;br&gt;*   After deleting (snapshot#1), Parent snapshot of (snapshot#3) should be starting point.&lt;br&gt;*   Only max of &lt;code&gt;5&lt;/code&gt; snapshots should be retained&lt;br&gt;*   Oldest snapshot will be removed when number of snapshots created by recurring job exceeds retain count&lt;br&gt;*   only snapshots generated by recurring job is affected by retain count, user can created manual snapshots and it will not be deleted automatically.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Disabling/Deleting recurring snapshots&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a deployment workload with &lt;code&gt;nReplicas = 1&lt;/code&gt; using Longhorn volume&lt;br&gt;2.  Write data to volume , compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a recurring snapshot &lt;code&gt;every 5 minutes&lt;/code&gt;. and set retain count to &lt;code&gt;5&lt;/code&gt;&lt;br&gt;4.  Wait for 2 recurring snapshots to triggered (snapshot#1,snapshot#2 )&lt;br&gt;5.  Delete the recurring snapshots&lt;/td&gt;&#xA;          &lt;td&gt;*   Recurring snapshots will stop after deletion of it.&lt;br&gt;*   Existing snapshots should retain.&lt;br&gt;*   User should be able to take snapshot manually.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Operation with volume created using rancher UI&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create pv/pvc in rancher UI.&lt;br&gt;2.  Deploy a workload with PVC created in rancher UI.&lt;br&gt;3.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;4.  In longhorn UI, create a snapshot.&lt;br&gt;5.  Write data to volume again.&lt;br&gt;6.  Revert to snapshot.&lt;br&gt;7.  Delete the snapshot.&lt;/td&gt;&#xA;          &lt;td&gt;*   User should be able to create snapshot.&lt;br&gt;*   User should to revert to snapshot created verify this by checksum.&lt;br&gt;*   User should be able to delete the snapshot. Verify this in replicas in the nodes.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Delete the last snapshot&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume(more than 4k), compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)&lt;br&gt;5.  Write data to volume, compute it’s checksum (checksum#4) → live data&lt;br&gt;6.  Delete (snapshot#3)&lt;/td&gt;&#xA;          &lt;td&gt;*   Data from last snapshot should not get lost.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;Multi branch snapshot&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume(more than 4k), compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)&lt;br&gt;5.  Write data to volume, compute it’s checksum (checksum#4) → live data&lt;br&gt;6.  Revert to snapshot#2&lt;br&gt;7.  Write data to volume, compute it’s checksum.&lt;br&gt;8.  Repeat steps 2,3 two more times to have (snapshot#4, snapshot#5) with different data files (checksum#5, checksum#6)&lt;br&gt;9.  Revert to snapshot#3&lt;/td&gt;&#xA;          &lt;td&gt;*   Verify the data in any of the replica&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;Backup from a snapshot&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)&lt;br&gt;5.  Write data to volume, compute it’s checksum (checksum#4) → live data&lt;br&gt;6.  Take backup from a snapshot#2.&lt;br&gt;7.  Restore from the backup taken&lt;/td&gt;&#xA;          &lt;td&gt;Verify the data of the backup, it should match data from snapshot#2&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;volume-expansion&#34;&gt;Volume Expansion&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Volume &lt;strong&gt;Online expansion&lt;/strong&gt; for attached volume&lt;br&gt;&lt;br&gt;&lt;strong&gt;(Not Supported for now)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create multiple 4 volumes, each of size 5 GB, Attach them to nodes&lt;br&gt;2.  Format each volume using one of the following formats (ext 2/3/4 &amp;amp; xfs)&lt;br&gt;3.  Mount volumes to directories on the nodes.&lt;br&gt;4.  Check volume size and used space using &lt;code&gt;df -h&lt;/code&gt; command&lt;br&gt;5.  Write 4 GB data file to each volume&lt;br&gt;6.  For each volume, from Operation menu, Click &lt;code&gt;Expand Volume&lt;/code&gt;, set size to &lt;code&gt;10 GB&lt;/code&gt;, and click &lt;code&gt;OK&lt;/code&gt;&lt;br&gt;7.  Check volume size and used space using &lt;code&gt;df -h&lt;/code&gt; command&lt;br&gt;8.  Add more 4 GB data file to each volume&lt;br&gt;9.  Check volume size and used space using &lt;code&gt;df -h&lt;/code&gt; command&lt;br&gt;10.  Check volume size expanded&lt;/td&gt;&#xA;          &lt;td&gt;*   Volumes should be expanded to the new size&lt;br&gt;*   Volume read/write operations should work after size expansion&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Volume &lt;strong&gt;Online expansion&lt;/strong&gt; for volume consumed by Kubernetes workload&lt;br&gt;&lt;br&gt;&lt;strong&gt;→ Kubernetes Version: 1.15&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a &lt;code&gt;2GB&lt;/code&gt; volume used by a Kubernetes workload&lt;br&gt;2.  Expand Volume size to &lt;code&gt;10 GB&lt;/code&gt;&lt;br&gt;3.  In Kubernetes, edit PV/PVC capacity to match new volume size.&lt;br&gt;4.  Check volume size using &lt;code&gt;df -h&lt;/code&gt; command from Kubernetes workload&lt;br&gt;5.  Write 8 GB data file to volume, and compute its checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   When resizing, A message indicate that &lt;code&gt;The capacity of related PV and PVC will not be updated&lt;/code&gt;&lt;br&gt;*   Volumes should be expanded to the new size&lt;br&gt;*   Volume read/write operations should work after size expansion&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Volume &lt;strong&gt;Online expansion&lt;/strong&gt; for volume consumed by Kubernetes workload&lt;br&gt;&lt;br&gt;&lt;strong&gt;→ Kubernetes Version: 1.16+&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   PVC is dynamically provisioned by the StorageClass.&lt;br&gt;*   The Kubernetes is version 1.16+ or the feature gate for volume expansion is enabled.&lt;br&gt;*   The StorageClass should support resize, &lt;code&gt;allowVolumeExpansion: true&lt;/code&gt; is set in the StorageClass&lt;br&gt;&lt;br&gt;1.  Create a &lt;code&gt;2GB&lt;/code&gt; volume used by a Kubernetes workload&lt;br&gt;2.  Expand Volume size to &lt;code&gt;10 GB&lt;/code&gt;&lt;br&gt;3.  Check volume size using &lt;code&gt;df -h&lt;/code&gt; command from Kubernetes workload&lt;br&gt;4.  Write 8 GB data file to volume, and compute its checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   When resizing, A message indicate that &lt;code&gt;The capacity of related PV and PVC will not be updated&lt;/code&gt;&lt;br&gt;*   Volumes should be expanded to the new size&lt;br&gt;*   Volume read/write operations should work after size expansion&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Volume Offline expansion&lt;br&gt;&lt;br&gt;&lt;strong&gt;Kubernetes Version: &amp;lt; 1.16&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create and attach a volume&lt;br&gt;2.  Format and mount the volume. Fill up the volume and get the checksum (checksum#1)&lt;br&gt;3.  Unmount and detach the volume.&lt;br&gt;4.  Expand the volume and wait for the expansion complete.&lt;br&gt;5.  Reattach and remount the volume. Check the checksum and if the filesystem is expanded.&lt;br&gt;6.  Fill up the expanded parts and get the checksum. (checksum#2)&lt;br&gt;7.  Unmount and detach the volume.&lt;br&gt;8.  Launch a workload for it on a different node. Check data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume size should be expanded&lt;br&gt;*   Volume read/write operations should work after size expansion&lt;br&gt;*   After expansion, data checksum should match checksum#1&lt;br&gt;*   Final data checksum should match checksum#2&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Volume expansion with revert and backup&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create and attach a volume.&lt;br&gt;2.  Format and mount the volume. Fill up the volume and get the checksum. (checksum#1)&lt;br&gt;3.  Create the 1st snapshot and backup. (snapshot#1 &amp;amp; backup#1)&lt;br&gt;4.  Expand the volume. Fill up the expanded part and get the checksum (checksum#2)&lt;br&gt;5.  Create the 2nd snapshot and backup. (snapshot#2 &amp;amp; backup#2)&lt;br&gt;6.  Check if the backup volume size is expanded.&lt;br&gt;7.  Restore backup#2 to a volume and check its data&lt;br&gt;8.  Clean up then refill the volume. Get the checksum. (checksum#3)&lt;br&gt;9.  Create the 3rd snapshot and backup. (snapshot#3 &amp;amp; backup#3)&lt;br&gt;10.  Revert to the 2nd snapshot. Check the checksum.&lt;br&gt;11.  Revert to the 1st snapshot. Check the checksum and if we can still use the expanded part.&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume should be expanded&lt;br&gt;*   backup#2 size should be expanded and match volume new expanded size&lt;br&gt;*   Restored volume data from backup#2 should match checksum#2&lt;br&gt;*   After reverting to snapshot#1, data checksum should match checksum#1&lt;br&gt;*   After reverting to snapshot#1 expanded size should be usable.&lt;br&gt;*   Volume read/write operations should work after expansion and revert.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;rwx-volume-native-support-starting-with-v110&#34;&gt;RWX Volume native support starting with v1.1.0&lt;/h2&gt;&#xA;&lt;h3 id=&#34;prerequisite&#34;&gt;Prerequisite:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Longhorn is deployed in a cluster having 4 nodes (1 etcd/control plane and 3 worker)&lt;/li&gt;&#xA;&lt;li&gt;NFS-Common is installed on the nodes.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Scenario&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Steps&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Create StatefulSet/Deployment with single pod with volume attached in RWX mode&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 1 pod.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Verify that a PVC, ShareManger pod, CRD and volume in Longhorn get created.&lt;br&gt;4.  Verify there is directory with the name of PVC exists in the ShareManager mount point i.e. &lt;code&gt;export&lt;/code&gt;&lt;br&gt;5.  Write some data in the pod and verify the same data reflects in the ShareManager.&lt;br&gt;6.  Verify the longhorn volume, it should reflect the correct size.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Create StatefulSet/Deployment with more than 1 pod with volume attached in RWX mode.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with multiple pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Verify that one volume per pod in Longhorn gets created.&lt;br&gt;4.  Verify there is directory with the name of PVC exists in the ShareManager mount point i.e. &lt;code&gt;export&lt;/code&gt;&lt;br&gt;5.  Verify that Longhorn UI shows all the pods name attached to the volume.&lt;br&gt;6.  Write some data in all the pod and verify all the data reflects in the ShareManager.&lt;br&gt;7.  Verify the longhorn volume, it should reflect the correct size.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Create StatefulSet/Deployment with the existing PVC of a RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 1 pod.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Verify that a PVC, ShareManger pod, CRD and volume in Longhorn get created.&lt;br&gt;4.  Write some data in the pod and verify the same data reflects in the ShareManager.&lt;br&gt;5.  Create another StatefulSet/Deployment using the above created PVC.&lt;br&gt;6.  Write some data in the new pod, the same should be reflected in the ShareManager pod.&lt;br&gt;7.  Verify the longhorn volume, it should reflect the correct size.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Scale up StatefulSet/Deployment with one pod attached with volume in RWX mode.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 1 pod.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod and verify the same data reflects in the ShareManager.&lt;br&gt;4.  Scale up the StatefulSet/Deployment.&lt;br&gt;5.  Verify a new volume gets created.&lt;br&gt;6.  Write some data in the new pod, the same should be reflected in the ShareManager pod.&lt;br&gt;7.  Verify the longhorn volume, it should reflect the correct size.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Scale down StatefulSet/Deployment attached with volume in RWX mode to zero.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 1 pod.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod and verify the same data reflects in the ShareManager.&lt;br&gt;4.  Scale down the StatefulSet/Deployment to zero&lt;br&gt;5.  Verify the ShareManager pod gets deleted.&lt;br&gt;6.  Verify the volume should be in detached state.&lt;br&gt;7.  Create a new StatefulSet/Deployment with the existing PVC with different mount point.&lt;br&gt;8.  Verify the ShareManager should get created and volume should become attached.&lt;br&gt;9.  Verify the data.&lt;br&gt;10.  Delete the newly created StatefulSet/Deployment.&lt;br&gt;11.  Verify the ShareManager pod gets deleted again.&lt;br&gt;12.  Scale up the first StatefulSet/Deployment.&lt;br&gt;13.  Verify the ShareManager should get created and volume should become attached.&lt;br&gt;14.  Verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Delete the Workload StatefulSet/Deployment attached with RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 1 pod.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod and verify the same data reflects in the ShareManager.&lt;br&gt;4.  Delete the workload.&lt;br&gt;5.  Verify the ShareManager pod gets deleted but the CRD should not be deleted.&lt;br&gt;6.  Verify the volume should be in detached state.&lt;br&gt;7.  Create another StatefulSet with existing PVC.&lt;br&gt;8.  Verify the ShareManager should get created and volume should become attached.&lt;br&gt;9.  Verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Take snapshot and backup of a RWX volume in Longhorn.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Take a snapshot and a backup.&lt;br&gt;5.  Write some more data into the pod.&lt;br&gt;6.  Revert to snapshot 1 and verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Restore a backup taken from a RWX volume in Longhorn.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Take a backup of a RWX volume.&lt;br&gt;5.  Restore from the backup and attach the volume to a pod.&lt;br&gt;6.  Verify the data and the volume should be read write once.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Create DR volume of a RWX volume in Longhorn.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Take a backup of the volume.&lt;br&gt;5.  Create a DR volume of the backup.&lt;br&gt;6.  Write more data in the pods and take more backups.&lt;br&gt;7.  Verify the DR volume is getting synced with latest backup.&lt;br&gt;8.  Activate the DR volume and verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Expand the RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Expand the volume.&lt;br&gt;5.  Verify that user is able to write data in the expanded volume.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;Recurring Backup/Snapshot with RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Schedule a recurring backup/Snapshot.&lt;br&gt;5.  Verify the recurring jobs are getting created and is taking backup/snapshot successfully.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;Deletion of the replica of a Longhorn RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Delete one of the replica and verify that the rebuild of replica is working fine.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;Parallel writing&lt;/td&gt;&#xA;          &lt;td&gt;1.  Write data in multiple pods attached to the same volume at the same time.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;14&lt;/td&gt;&#xA;          &lt;td&gt;Data locality with RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Enable &lt;code&gt;Data-locality&lt;/code&gt;&lt;br&gt;5.  Disable &lt;code&gt;Node soft anti-affinity&lt;/code&gt;.&lt;br&gt;6.  Disable the node where the volume is attached for some time.&lt;br&gt;7.  Wait for replica to be rebuilt on another node.&lt;br&gt;8.  Enable the node scheduling and verify a replica gets rebuilt on the attached node.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;15&lt;/td&gt;&#xA;          &lt;td&gt;Node eviction with RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Do a node eviction and verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;16&lt;/td&gt;&#xA;          &lt;td&gt;Auto salvage feature on an RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Crash all the replicas and verify the auto-salvage works fine.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;17&lt;/td&gt;&#xA;          &lt;td&gt;RWX volume with &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt; enabled.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Set a recurring backup and scale down all the pods.&lt;br&gt;5.  Verify the volume get attached at scheduled time and backup/snapshot get created.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;18&lt;/td&gt;&#xA;          &lt;td&gt;RWX volume with Toleration.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Set some Toleration.&lt;br&gt;5.  Verify the ShareManager pods have the toleration and annotation updated.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;19&lt;/td&gt;&#xA;          &lt;td&gt;Detach/Delete operation on an RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Detach action on the Longhorn UI should not work on RWX volume.&lt;br&gt;2.  On deletion of the RWX volume, the ShareManager CRDs should also get deleted.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;20&lt;/td&gt;&#xA;          &lt;td&gt;Crash instance e manager of the RWX volume&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Crash the instance manager.&lt;br&gt;5.  On crashing the IM, the ShareManager pods should be immediately redeployed.&lt;br&gt;6.  Based on the setting &lt;code&gt;Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly&lt;/code&gt;, the workload pods will get redeployed.&lt;br&gt;7.  On recreating on workload pods, the volume should get attached successfully.&lt;br&gt;8.  If &lt;code&gt;Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly&lt;/code&gt; is disabled, user should see I/O error on the mounted point.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;21&lt;/td&gt;&#xA;          &lt;td&gt;Reboot the ShareManager and workload node&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Reboot the ShareManager node.&lt;br&gt;5.  The ShareManager pod should move to another node.&lt;br&gt;6.  As the instance e manager is on the same node and based on setting &lt;code&gt;Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly&lt;/code&gt;, the workload should be redeployed and volume should be available to user.&lt;br&gt;7.  Reboot the workload node.&lt;br&gt;8.  On restart on the node, pods should get attached to the volume. Verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;22&lt;/td&gt;&#xA;          &lt;td&gt;Power down the ShareManager and workload node.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Power down the ShareManager node.&lt;br&gt;5.  The ShareManager pod should move to another node.&lt;br&gt;6.  As the instance manager is on the same node and based on the setting &lt;code&gt;Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly&lt;/code&gt;, the workload should be redeployed and volume should be available to user.&lt;br&gt;7.  Power down the workload node.&lt;br&gt;8.  The workload pods should move to another node based on &lt;code&gt;Pod Deletion Policy When Node is Down&lt;/code&gt; setting.&lt;br&gt;9.  Once the pods are up, they should get attached to the volume. Verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;23&lt;/td&gt;&#xA;          &lt;td&gt;Kill the nfs process in the ShareManager&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Kill the NFS server in the ShareManager pod.&lt;br&gt;5.  The NFS server should retry to come up.&lt;br&gt;6.  Volume should continue to accessible.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;24&lt;/td&gt;&#xA;          &lt;td&gt;Delete the ShareManager CRD.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Delete the ShareManager CRD.&lt;br&gt;5.  A new ShareManager CRD should be created.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;25&lt;/td&gt;&#xA;          &lt;td&gt;Delete the ShareManager pod.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Delete the ShareManager pod.&lt;br&gt;5.  A new ShareManager pod should be immediately created.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;26&lt;/td&gt;&#xA;          &lt;td&gt;Drain the ShareManager node.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Drain the ShareManager pod node.&lt;br&gt;5.  The volume should get detached first, then the shareManager pod should move to another node and Volume should get reattached.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;27&lt;/td&gt;&#xA;          &lt;td&gt;Disk full on the ShareManager node.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod and make the disk almost full.&lt;br&gt;4.  Verify the RWX volume is not failed.&lt;br&gt;5.  Verify the creation of snapshot/backup.&lt;br&gt;6.  Try to write more data, and the it should error out &lt;code&gt;no space left&lt;/code&gt;.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;28&lt;/td&gt;&#xA;          &lt;td&gt;Scheduling failure with RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Disable 1 node.&lt;br&gt;2.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;3.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;4.  Verify the RWX volume gets created with degraded state.&lt;br&gt;5.  Write some data in the pod.&lt;br&gt;6.  Enable the node and the volume should become healthy.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;29&lt;/td&gt;&#xA;          &lt;td&gt;Add a node in the cluster.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Add a node in the cluster.&lt;br&gt;2.  Create multiple statefulSet/deployment with RWX volume.&lt;br&gt;3.  Verify that the ShareManager pod is able to scheduled on the new node.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;30&lt;/td&gt;&#xA;          &lt;td&gt;Delete a node from the cluster.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Delete the ShareManager node from the cluster.&lt;br&gt;5.  Verify the ShareManager pod move to new node and volume continues to be accessible.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;31&lt;/td&gt;&#xA;          &lt;td&gt;RWX with Linux/SLES OS&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;32&lt;/td&gt;&#xA;          &lt;td&gt;RWX with K3s set up&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;33&lt;/td&gt;&#xA;          &lt;td&gt;RWX in Air gap set up.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;34&lt;/td&gt;&#xA;          &lt;td&gt;RWX in PSP enabled set up.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>5. Kubernetes</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/kubernetes/</guid>
      <description>&lt;h3 id=&#34;dynamic-provisioning-with-storageclass&#34;&gt;Dynamic provisioning with StorageClass&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Can create and use volume using StorageClass&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Can create a new StorageClass use new parameters and it will take effect on the volume created by the storage class.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;If the PV reclaim policy is delete, once PVC and PV are deleted, Longhorn volume should be deleted.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;static-provisioning-using-longhorn-created-pvpvc&#34;&gt;Static provisioning using Longhorn created PV/PVC&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;PVC can be used by the new workload&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Delete the PVC will not result in PV deletion&lt;/p&gt;</description>
    </item>
    <item>
      <title>6. Backup</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/backup/</guid>
      <description>&lt;h2 id=&#34;automation-tests&#34;&gt;Automation Tests&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test name&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;tag&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;test_backup&lt;/td&gt;&#xA;          &lt;td&gt;Test basic backup&lt;br&gt;&lt;br&gt;Setup:&lt;br&gt;&lt;br&gt;1.  Create a volume and attach to the current node&lt;br&gt;2.  Run the test for all the available backupstores.&lt;br&gt;&lt;br&gt;Steps:&lt;br&gt;&lt;br&gt;1.  Create a backup of volume&lt;br&gt;2.  Restore the backup to a new volume&lt;br&gt;3.  Attach the new volume and make sure the data is the same as the old one&lt;br&gt;4.  Detach the volume and delete the backup.&lt;br&gt;5.  Wait for the restored volume&amp;rsquo;s &lt;code&gt;lastBackup&lt;/code&gt; to be cleaned (due to remove the backup)&lt;br&gt;6.  Delete the volume&lt;/td&gt;&#xA;          &lt;td&gt;Backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;test_backup_labels&lt;/td&gt;&#xA;          &lt;td&gt;Test that the proper Labels are applied when creating a Backup manually.&lt;br&gt;&lt;br&gt;1.  Create a volume&lt;br&gt;2.  Run the following steps on all backupstores&lt;br&gt;3.  Create a backup with some random labels&lt;br&gt;4.  Get backup from backupstore, verify the labels are set on the backups&lt;/td&gt;&#xA;          &lt;td&gt;Backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;test_deleting_backup_volume&lt;/td&gt;&#xA;          &lt;td&gt;Test deleting backup volumes&lt;br&gt;&lt;br&gt;1.  Create volume and create backup&lt;br&gt;2.  Delete the backup and make sure it&amp;rsquo;s gone in the backupstore&lt;/td&gt;&#xA;          &lt;td&gt;Backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;test_listing_backup_volume&lt;/td&gt;&#xA;          &lt;td&gt;Test listing backup volumes&lt;br&gt;&lt;br&gt;1.  Create three volumes: &lt;code&gt;volume1/2/3&lt;/code&gt;&lt;br&gt;2.  Setup NFS backupstore since we can manipulate the content easily&lt;br&gt;3.  Create snapshots for all three volumes&lt;br&gt;4.  Rename &lt;code&gt;volume1&lt;/code&gt;&amp;lsquo;s &lt;code&gt;volume.cfg&lt;/code&gt; to &lt;code&gt;volume.cfg.tmp&lt;/code&gt; in backupstore&lt;br&gt;5.  List backup volumes. Make sure &lt;code&gt;volume1&lt;/code&gt; errors out but found other two&lt;br&gt;6.  Restore &lt;code&gt;volume1&lt;/code&gt;&amp;lsquo;s &lt;code&gt;volume.cfg&lt;/code&gt;.&lt;br&gt;7.  Make sure now backup volume &lt;code&gt;volume1&lt;/code&gt; can be found and deleted&lt;br&gt;8.  Delete backups for &lt;code&gt;volume2/3&lt;/code&gt;, make sure they cannot be found later&lt;/td&gt;&#xA;          &lt;td&gt;Backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;test_ha_backup_deletion_recovery&lt;/td&gt;&#xA;          &lt;td&gt;[HA] Test deleting the restored snapshot and rebuild&lt;br&gt;&lt;br&gt;Backupstore: all&lt;br&gt;&lt;br&gt;1.  Create volume and attach it to the current node.&lt;br&gt;2.  Write &lt;code&gt;data&lt;/code&gt; to the volume and create snapshot &lt;code&gt;snap2&lt;/code&gt;&lt;br&gt;3.  Backup &lt;code&gt;snap2&lt;/code&gt; to create a backup.&lt;br&gt;4.  Create volume &lt;code&gt;res_volume&lt;/code&gt; from the backup. Check volume &lt;code&gt;data&lt;/code&gt;.&lt;br&gt;5.  Check snapshot chain, make sure &lt;code&gt;backup_snapshot&lt;/code&gt; exists.&lt;br&gt;6.  Delete the &lt;code&gt;backup_snapshot&lt;/code&gt; and purge snapshots.&lt;br&gt;7.  After purge complete, delete one replica to verify rebuild works.&lt;/td&gt;&#xA;          &lt;td&gt;Backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;test_backup_kubernetes_status&lt;/td&gt;&#xA;          &lt;td&gt;Test that Backups have KubernetesStatus stored properly when there is an associated PersistentVolumeClaim and Pod.&lt;br&gt;&lt;br&gt;1.  Setup a random backupstore&lt;br&gt;2.  Set settings Longhorn Static StorageClass to &lt;code&gt;longhorn-static-test&lt;/code&gt;&lt;br&gt;3.  Create a volume and PV/PVC. Verify the StorageClass of PVC&lt;br&gt;4.  Create a Pod using the PVC.&lt;br&gt;5.  Check volume&amp;rsquo;s Kubernetes status to reflect PV/PVC/Pod correctly.&lt;br&gt;6.  Create a backup for the volume.&lt;br&gt;7.  Verify the labels of created backup reflect PV/PVC/Pod status.&lt;br&gt;8.  Restore the backup to a volume. Wait for restoration to complete.&lt;br&gt;9.  Check the volume&amp;rsquo;s Kubernetes Status&lt;br&gt;    1.  Make sure the &lt;code&gt;lastPodRefAt&lt;/code&gt; and &lt;code&gt;lastPVCRefAt&lt;/code&gt; is snapshot created time&lt;br&gt;        &lt;br&gt;10.  Delete the backup and restored volume.&lt;br&gt;11.  Delete PV/PVC/Pod.&lt;br&gt;12.  Verify volume&amp;rsquo;s Kubernetes Status updated to reflect history data.&lt;br&gt;13.  Attach the volume and create another backup. Verify the labels&lt;br&gt;14.  Verify the volume&amp;rsquo;s Kubernetes status.&lt;br&gt;15.  Restore the previous backup to a new volume. Wait for restoration.&lt;br&gt;16.  Verify the restored volume&amp;rsquo;s Kubernetes status.&lt;br&gt;    1.  Make sure &lt;code&gt;lastPodRefAt&lt;/code&gt; and &lt;code&gt;lastPVCRefAt&lt;/code&gt; matched volume on step 12&lt;/td&gt;&#xA;          &lt;td&gt;Backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;test_restore_inc&lt;/td&gt;&#xA;          &lt;td&gt;Test restore from disaster recovery volume (incremental restore)&lt;br&gt;&lt;br&gt;Run test against all the backupstores&lt;br&gt;&lt;br&gt;1.  Create a volume and attach to the current node&lt;br&gt;2.  Generate &lt;code&gt;data0&lt;/code&gt;, write to the volume, make a backup &lt;code&gt;backup0&lt;/code&gt;&lt;br&gt;3.  Create three DR(standby) volumes from the backup: &lt;code&gt;sb_volume0/1/2&lt;/code&gt;&lt;br&gt;4.  Wait for all three DR volumes to finish the initial restoration&lt;br&gt;5.  Verify DR volumes&amp;rsquo;s &lt;code&gt;lastBackup&lt;/code&gt; is &lt;code&gt;backup0&lt;/code&gt;&lt;br&gt;6.  Verify snapshot/pv/pvc/change backup target are not allowed as long as the DR volume exists&lt;br&gt;7.  Activate standby &lt;code&gt;sb_volume0&lt;/code&gt; and attach it to check the volume data&lt;br&gt;8.  Generate &lt;code&gt;data1&lt;/code&gt; and write to the original volume and create &lt;code&gt;backup1&lt;/code&gt;&lt;br&gt;9.  Make sure &lt;code&gt;sb_volume1&lt;/code&gt;&amp;lsquo;s &lt;code&gt;lastBackup&lt;/code&gt; field has been updated to &lt;code&gt;backup1&lt;/code&gt;&lt;br&gt;10.  Wait for &lt;code&gt;sb_volume1&lt;/code&gt; to finish incremental restoration then activate&lt;br&gt;11.  Attach and check &lt;code&gt;sb_volume1&lt;/code&gt;&amp;rsquo;s data&lt;br&gt;12.  Generate &lt;code&gt;data2&lt;/code&gt; and write to the original volume and create &lt;code&gt;backup2&lt;/code&gt;&lt;br&gt;13.  Make sure &lt;code&gt;sb_volume2&lt;/code&gt;&amp;lsquo;s &lt;code&gt;lastBackup&lt;/code&gt; field has been updated to &lt;code&gt;backup1&lt;/code&gt;&lt;br&gt;14.  Wait for &lt;code&gt;sb_volume2&lt;/code&gt; to finish incremental restoration then activate&lt;br&gt;15.  Attach and check &lt;code&gt;sb_volume2&lt;/code&gt;&amp;rsquo;s data&lt;br&gt;16.  Create PV, PVC and Pod to use &lt;code&gt;sb_volume2&lt;/code&gt;, check PV/PVC/POD are good&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Disaster Recovery&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;test_recurring_job&lt;/td&gt;&#xA;          &lt;td&gt;Test recurring job&lt;br&gt;&lt;br&gt;1.  Setup a random backupstore&lt;br&gt;2.  Create a volume.&lt;br&gt;3.  Create two jobs 1 job 1: snapshot every one minute, retain 2 1 job 2: backup every two minutes, retain 1&lt;br&gt;4.  Attach the volume.&lt;br&gt;5.  Sleep for 5 minutes&lt;br&gt;6.  Verify we have 4 snapshots total&lt;br&gt;    1.  2 snapshots, 1 backup, 1 volume-head&lt;br&gt;        &lt;br&gt;7.  Update jobs to replace the backup job&lt;br&gt;    1.  New backup job run every one minute, retain 2&lt;br&gt;        &lt;br&gt;8.  Sleep for 5 minutes.&lt;br&gt;9.  We should have 6 snapshots&lt;br&gt;    1.  2 from job_snap, 1 from job_backup, 2 from job_backup2, 1 volume-head&lt;br&gt;        &lt;br&gt;10.  Make sure we have no more than 5 backups.&lt;br&gt;    1.  old backup job may have at most 1 backups&lt;br&gt;        &lt;br&gt;    2.  new backup job may have at most 3 backups&lt;br&gt;        &lt;br&gt;11.  Make sure we have no more than 2 backups in progress&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Recurring Job&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;test_recurring_job_in_storageclass&lt;/td&gt;&#xA;          &lt;td&gt;Test create volume with StorageClass contains recurring jobs&lt;br&gt;&lt;br&gt;1.  Create a StorageClass with recurring jobs&lt;br&gt;2.  Create a StatefulSet with PVC template and StorageClass&lt;br&gt;3.  Verify the recurring jobs run correctly.&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Recurring Job&lt;br&gt;&lt;br&gt;Kubernetes&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;test_recurring_job_in_volume_creation&lt;/td&gt;&#xA;          &lt;td&gt;Test create volume with recurring jobs&lt;br&gt;&lt;br&gt;1.  Create volume with recurring jobs though Longhorn API&lt;br&gt;2.  Verify the recurring jobs run correctly&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Recurring Job&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;test_recurring_job_kubernetes_status&lt;/td&gt;&#xA;          &lt;td&gt;Test RecurringJob properly backs up the KubernetesStatus&lt;br&gt;&lt;br&gt;1.  Setup a random backupstore.&lt;br&gt;2.  Create a volume.&lt;br&gt;3.  Create a PV from the volume, and verify the PV status.&lt;br&gt;4.  Create a backup recurring job to run every 2 minutes.&lt;br&gt;5.  Verify the recurring job runs correctly.&lt;br&gt;6.  Verify the backup contains the Kubernetes Status labels&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Recurring Job&lt;br&gt;&lt;br&gt;Volume: Kubernetes Status&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;test_recurring_job_labels&lt;/td&gt;&#xA;          &lt;td&gt;Test a RecurringJob with labels&lt;br&gt;&lt;br&gt;1.  Set a random backupstore&lt;br&gt;2.  Create a backup recurring job with labels&lt;br&gt;3.  Verify the recurring jobs runs correctly.&lt;br&gt;4.  Verify the labels on the backup is correct&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Recurring Job&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;test_recurring_jobs_maximum_retain&lt;/td&gt;&#xA;          &lt;td&gt;Test recurring jobs&amp;rsquo; maximum retain&lt;br&gt;&lt;br&gt;1.  Create two jobs, with retain 30 and 21.&lt;br&gt;2.  Try to apply the jobs to a volume. It should fail.&lt;br&gt;3.  Reduce retain to 30 and 20.&lt;br&gt;4.  Now the jobs can be applied the volume&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Recurring Job&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;backup-create-operations-test-cases&#34;&gt;Backup create operations test cases&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Create backup from existing snapshot&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Backup target is set to NFS server, or S3 compatible target.&lt;br&gt;&lt;br&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Create a backup from (snapshot#1)&lt;br&gt;5.  Restore backup to a different volume&lt;br&gt;6.  Attach volume to a node and check it’s data, and compute it’s checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   Backup should be created&lt;br&gt;*   Restored volume data checksum should match (checksum#1)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Create volume backup for a volume attached to a node&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Backup target is set to NFS server, or S3 compatible target.&lt;br&gt;&lt;br&gt;1.  Create a volume, attach it to a node&lt;br&gt;2.  Format volume using ext4/xfs filesystem and mount it to a directory on the node&lt;br&gt;3.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;4.  Create a backup&lt;br&gt;5.  Restore backup to a different volume&lt;br&gt;6.  Attach volume to a node and check it’s data, and compute it’s checksum&lt;br&gt;7.  Check volume backup labels&lt;/td&gt;&#xA;          &lt;td&gt;*   Backup should be created&lt;br&gt;*   Restored volume data checksum should match (checksum#1)&lt;br&gt;*   backup should have no backup labels&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Create volume backup used by Kubernetes workload&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Backup target is set to NFS server, or S3 compatible target.&lt;br&gt;&lt;br&gt;1.  Create a deployment workload with &lt;code&gt;nReplicas = 1&lt;/code&gt; using Longhorn volume&lt;br&gt;2.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a backup&lt;br&gt;4.  Check backup labels&lt;br&gt;5.  Scale down deployment &lt;code&gt;nReplicas = 0&lt;/code&gt;&lt;br&gt;6.  Delete Longhorn volume&lt;br&gt;7.  Restore backup to a volume with the same deleted volume name&lt;br&gt;8.  Scale back deployment &lt;code&gt;nReplicas = 1&lt;/code&gt;&lt;br&gt;9.  Check volume data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   Backup labels should contain the following informations about workload that was using the volume at time of backup.&lt;br&gt;    *   Namespace&lt;br&gt;        &lt;br&gt;    *   PV Name&lt;br&gt;        &lt;br&gt;    *   PVC Name&lt;br&gt;        &lt;br&gt;    *   PV Status&lt;br&gt;        &lt;br&gt;    *   Workloads Status&lt;br&gt;        &lt;br&gt;        *   Pod Name  &lt;br&gt;            Workload Name  &lt;br&gt;            Workload Type  &lt;br&gt;            Pod Status&lt;br&gt;            &lt;br&gt;*   After volume restore, data checksum should match (checksum#1)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Create volume backup with customized labels&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Backup target is set to NFS server, or S3 compatible target.&lt;br&gt;&lt;br&gt;1.  Create a volume, attach it to a node&lt;br&gt;2.  Create a backup, add customized labels  &lt;br&gt;    key: &lt;code&gt;K1&lt;/code&gt; value: &lt;code&gt;V1&lt;/code&gt;&lt;br&gt;3.  Check volume backup labels&lt;/td&gt;&#xA;          &lt;td&gt;*   Backup should be created with customized labels&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Create recurring backups&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a deployment workload with &lt;code&gt;nReplicas = 1&lt;/code&gt; using Longhorn volume&lt;br&gt;2.  Write data to volume , compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a recurring backup &lt;code&gt;every 5 minutes&lt;/code&gt;. and set retain count to &lt;code&gt;5&lt;/code&gt;&lt;br&gt;4.  add customized labels key: &lt;code&gt;K1&lt;/code&gt; value: &lt;code&gt;V1&lt;/code&gt;&lt;br&gt;5.  Wait for recurring backup to triggered (backup#1, backup#2 )&lt;br&gt;6.  Scale down deployment &lt;code&gt;nReplicas = 0&lt;/code&gt;&lt;br&gt;7.  Delete the volume.&lt;br&gt;8.  Restore backup to a volume with the same deleted volume name&lt;br&gt;9.  Scale back deployment &lt;code&gt;nReplicas = 1&lt;/code&gt;&lt;br&gt;10.  Check volume data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   backups should be created with Kubernetes status labels and customized labels&lt;br&gt;*   After volume restore, data checksum should match (checksum#1)&lt;br&gt;*   after restoring the backup recurring backups should continue to be created&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Backup created using Longhorn behind proxy&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Setup a Proxy on an instance (Optional: use squid)&lt;br&gt;*   Create a single node cluster in EC2&lt;br&gt;*   Deploy Longhorn&lt;br&gt;&lt;br&gt;1.  Block outgoing traffic except for the proxy instance.&lt;br&gt;2.  Create AWS secret in longhorn.&lt;br&gt;3.  In UI Settings page, set backupstore target and backupstore credential secret&lt;br&gt;4.  Create a volume, attach it to a node, format the volume, and mount it to a directory.&lt;br&gt;5.  Write some data to the volume, and create a backup.&lt;/td&gt;&#xA;          &lt;td&gt;*   Ensure backup is created&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Backup created in a backup store supports Virtual Hosted Style&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create an OSS bucket in Alibaba Cloud(Aliyun)&lt;br&gt;2.  Create a secret without &lt;code&gt;VIRTUAL_HOSTED_STYLE&lt;/code&gt; for the OSS bucket.&lt;br&gt;3.  Set backup target and the secret in Longhorn UI.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Backup created in a backup store supports both Virtual Hosted style and traditional&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create an S3 bucket in AWS.&lt;br&gt;2.  Create a secret without &lt;code&gt;VIRTUAL_HOSTED_STYLE&lt;/code&gt; for the S3 bucket.&lt;br&gt;3.  Set backup target and the secret in Longhorn UI.&lt;br&gt;4.  Verify backup list/create/delete/restore work fine without the configuration.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;backup-restore-operations-test-cases&#34;&gt;Backup restore operations test cases&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Filter backup using backup name&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;&lt;br&gt;1.  Filter backups by volume name&lt;/td&gt;&#xA;          &lt;td&gt;*   volumes should be filtered using full/partial volume names&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Restore last backup with different name&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create a Volume, attach it to a node, write some data (300MB+), compute it’s checksum and create a backup (repeat for 3 times).&lt;br&gt;*   Volume now has multiple backups (backup#1, backup#2, backup#3) respectively.&lt;br&gt;&lt;br&gt;1.  Restore latest volume backup using &lt;strong&gt;different&lt;/strong&gt; name than it’s original&lt;br&gt;2.  After restore complete, attach the volume to a node, and check data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   New Volume should be created and attached to a node in maintenance mode&lt;br&gt;*   Restore process should be triggered restoring latest backup content to the volume&lt;br&gt;*   After restore is completed, volume is detached from the node&lt;br&gt;*   data checksum should match data checksum for (backup#3)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Restore specific with different name&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create a Volume, attach it to a node, write some data (300MB+), compute it’s checksum and create a backup (repeat for 3 times).&lt;br&gt;*   Volume now has multiple backups (backup#1, backup#2, backup#3) respectively.&lt;br&gt;&lt;br&gt;1.  Restore restore the second backup (backup#2) using &lt;strong&gt;different&lt;/strong&gt; name than it’s original&lt;br&gt;2.  After restore complete, attach the volume to a node, and check data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   New Volume should be created and attached to a node in maintenance mode&lt;br&gt;*   Restore process should be triggered restoring latest backup content to the volume&lt;br&gt;*   After restore is completed, volume is detached from the node&lt;br&gt;*   data checksum should match data checksum for (backup#2)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Volume backup URL&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;&lt;br&gt;1.  get backup URL&lt;/td&gt;&#xA;          &lt;td&gt;*   Backup URL should point to a link to backup on configured backupstore&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Restore backup with different number of replicas&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;&lt;br&gt;1.  Restore a backup and set different number of replicas&lt;/td&gt;&#xA;          &lt;td&gt;*   Restored volume replica count should match the number in restore backup request&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Restore backup with Different Node tags&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;*   Longhorn Nodes should have Node Tags&lt;br&gt;&lt;br&gt;1.  Restore a backup and set node tags&lt;/td&gt;&#xA;          &lt;td&gt;*   Restored volume replicas should scheduled only to nodes have Node Tags match Tags specified in restore backup request&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Restore backup with Different Disk Tags&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;*   Longhorn Nodes Disks should have Disk Tags&lt;br&gt;&lt;br&gt;1.  Restore a backup and set disk tags&lt;/td&gt;&#xA;          &lt;td&gt;*   Restored volume replicas should scheduled only to disks have Disk Tags match Tags specified in restore backup request&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Restore backup with both Node and Disk Tags&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;*   Longhorn Nodes should have Node Tags&lt;br&gt;*   Longhorn Nodes Disks should have Disk Tags&lt;br&gt;&lt;br&gt;1.  Restore a backup and set both Node and Disk tags&lt;/td&gt;&#xA;          &lt;td&gt;*   Restored volume replicas should scheduled only to nodes that have both Node and Disk tags specified in restore backup request.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Restore last backup with same previous name (Volume already exists)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create a Volume, attach it to a node, write some data (300MB+), compute it’s checksum and create a backup (repeat for 3 times).&lt;br&gt;*   Volume now has multiple backups (backup#1, backup#2, backup#3) respectively.&lt;br&gt;&lt;br&gt;1.  Restore latest volume backup using &lt;strong&gt;same&lt;/strong&gt; original volume name&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume can’t be restored&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Restore last backup with same previous name&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create a Volume, attach it to a node, write some data (300MB+), compute it’s checksum and create a backup (repeat for 3 times).&lt;br&gt;*   Volume now has multiple backups (backup#1, backup#2, backup#3) respectively.&lt;br&gt;*   Detach and delete volume&lt;br&gt;&lt;br&gt;1.  Restore latest volume backup using &lt;strong&gt;same&lt;/strong&gt; original volume name&lt;br&gt;2.  After restore complete, attach the volume to a node, and check data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   New Volume with same old name should be created and attached to a node in maintenance mode&lt;br&gt;*   Restore process should be triggered restoring latest backup content to the volume&lt;br&gt;*   After restore is completed, volume is detached from the node&lt;br&gt;*   data checksum should match data checksum for (backup#3)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;Restore volume used by Kubernetes workload with same previous name&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create a deployment workload using a Longhorn volume, write some data (300MB+), compute it’s checksum and create a backup (repeat for 3 times).&lt;br&gt;*   Volume now has multiple backups (backup#1, backup#2, backup#3) respectively.&lt;br&gt;*   Scale down the deployment to zero&lt;br&gt;*   Delete volume&lt;br&gt;&lt;br&gt;1.  Restore latest volume backup using &lt;strong&gt;same&lt;/strong&gt; original volume name&lt;br&gt;2.  After restore complete, scale up the deployment&lt;/td&gt;&#xA;          &lt;td&gt;*   New Volume with same old name should be created and attached to a node in maintenance mode&lt;br&gt;*   Restore process should be triggered restoring latest backup content to the volume&lt;br&gt;*   After restore is completed, volume is detached from the node&lt;br&gt;*   Old &lt;code&gt;PV/PVC , Namespace &amp;amp; Attached To&lt;/code&gt; information should be restored&lt;br&gt;*   Volume should be accessible from the deployment pod&lt;br&gt;*   Data checksum should match data checksum for (backup#3)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;Restore volume used by Kubernetes workload with different name&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create a deployment workload using a Longhorn volume, write some data (300MB+), compute it’s checksum and create a backup (repeat for 3 times).&lt;br&gt;*   Volume now has multiple backups (backup#1, backup#2, backup#3) respectively.&lt;br&gt;*   Scale down the deployment to zero&lt;br&gt;*   Delete volume&lt;br&gt;&lt;br&gt;1.  Restore latest volume backup using &lt;strong&gt;different&lt;/strong&gt; name than its original&lt;br&gt;2.  After restore complete&lt;br&gt;    1.  Delete old PVC&lt;br&gt;        &lt;br&gt;    2.  Create a new PV for volume&lt;br&gt;        &lt;br&gt;    3.  Create a new PVC with same old PVC name&lt;br&gt;        &lt;br&gt;3.  scale up the deployment&lt;/td&gt;&#xA;          &lt;td&gt;*   New Volume with same old name should be created and attached to a node in maintenance mode&lt;br&gt;*   Restore process should be triggered restoring latest backup content to the volume&lt;br&gt;*   After restore is completed, volume is detached from the node&lt;br&gt;*   Old &lt;code&gt;Namespace &amp;amp; Attached To&lt;/code&gt; information should be restored&lt;br&gt;*   &lt;code&gt;PV/PVC&lt;/code&gt; information should be empty after restore completed  &lt;br&gt;    old PV &lt;code&gt;spec.csi.volumeHandle&lt;/code&gt;will not match the new volume name&lt;br&gt;*   After New PV/PVC is created, deployment pod should be able to claim the new PVC and access volume with new name.&lt;br&gt;*   Data checksum should match data checksum for (backup#3)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;Restore last backup (batch operation)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;&lt;br&gt;1.  select multiple volumes, restore the latest backup for all of them&lt;/td&gt;&#xA;          &lt;td&gt;*   New volumes with same old volume names should be created, attached to nodes and restore process should be triggered&lt;br&gt;*   &lt;code&gt;PV/PVC&lt;/code&gt; information should be restored for volumes that had PV/PVC created&lt;br&gt;*   &lt;code&gt;Namespace &amp;amp; Attached To&lt;/code&gt; information should be restored for volumes that had been used by kubnernetes workload at the time of backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;14&lt;/td&gt;&#xA;          &lt;td&gt;Delete All Volume Backups&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;&lt;br&gt;1.  Delete All backups for a volume&lt;br&gt;2.  Check backupstore, and confirm backups has been deleted&lt;/td&gt;&#xA;          &lt;td&gt;*   Backups should not be delete from Longhorn UI, and also from backupstore.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;15&lt;/td&gt;&#xA;          &lt;td&gt;Restore backup created using Longhorn behind proxy.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Setup a Proxy on an instance (Optional: use squid)&lt;br&gt;*   Create a single node cluster in EC2&lt;br&gt;*   Deploy Longhorn&lt;br&gt;&lt;br&gt;1.  Block outgoing traffic except for the proxy instance.&lt;br&gt;2.  Create AWS secret in longhorn as follows:&lt;br&gt;3.  In UI Settings page, set backupstore target and backupstore credential secret&lt;br&gt;4.  Create a volume, attach it to a node, format the volume, and mount it to a directory.&lt;br&gt;5.  Write some data to the volume, and create a backup.&lt;br&gt;6.  Wait for backup to complete, and the try to restore the backup to a volume with different name.&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume should get restored successfully.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;disaster-recovery-test-cases&#34;&gt;Disaster Recovery test cases&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Tests Prerequisite&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>7. Node</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/node/</guid>
      <description>&lt;h2 id=&#34;ui-specific-test-cases&#34;&gt;&lt;strong&gt;UI specific test cases&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Storage details&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;    *   Longhorn Installed&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Verify the allocated/used storage show the right data in node details page.&lt;br&gt;2.  Create a volume of 20 GB and attach to a pod and verify the storage allocated/used is shown correctly.&lt;/td&gt;&#xA;          &lt;td&gt;Without any volume, allocated should be 0 and on creating new volume it should be updated as per volume present.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Filters applied to node list&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;    *   Longhorn Installed&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  In longhorn UI node tab- Change the filter based on name/status etc. Verify the nodes are appearing properly.&lt;/td&gt;&#xA;          &lt;td&gt;Nodes satisfying filter should only get displayed on page.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Sort the nodes view&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;    *   Longhorn Installed&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  In longhorn UI node tab- Click title to sort the nodes appearing based on status/name etc&lt;/td&gt;&#xA;          &lt;td&gt;Nodes list should get sorted ascending/descending based on status/name stc&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Expand All&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;    *   Longhorn Installed&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  In longhorn UI node tab- Click ‘expand all’ button.&lt;/td&gt;&#xA;          &lt;td&gt;All nodes should get expanded and show disks details&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;additional-tests-for-ui&#34;&gt;Additional Tests for UI&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Scenario&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Readiness column&lt;/td&gt;&#xA;          &lt;td&gt;1.  Click On a node’s readiness state say “Ready”&lt;br&gt;2.  Verify components window opens&lt;br&gt;3.  Verify Engine image and instance manager details are seen&lt;/td&gt;&#xA;          &lt;td&gt;Components window should open with details of Engine image and instance manager&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Replicas column&lt;/td&gt;&#xA;          &lt;td&gt;1.  Click on the count (number of replicas) for a node in the replica column&lt;br&gt;2.  Verify the list of replicas on the node is available&lt;br&gt;3.  Verify user is able to delete a replica on the node by selecting the replica and clicking on delete&lt;br&gt;4.  Verify the replica is deleted by navigating to the specific Volume → Volume details page → replicas&lt;/td&gt;&#xA;          &lt;td&gt;*   User should be able to view all the replicas on the node&lt;br&gt;*   User should be able to delete replica on the node&lt;br&gt;*   User should be able to see the replica deleted in the volume → Volume details page → replicas page&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;test-cases&#34;&gt;Test cases&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Node scheduling&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;    *   Longhorn Deployed with 3 nodes&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Disable Node Scheduling on a node&lt;br&gt;2.  Create a volume with 3 replicas, and attach it to a node&lt;br&gt;3.  Re-enabled node scheduling on the node&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume should be created and attached&lt;br&gt;*   Volume replicas should be scheduled to Schedulable nodes only&lt;br&gt;*   Re-enabling node scheduling will not affect existing scheduled replicas, it will only affect new replicas being created, or rebuilt.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Disk Scheduling&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;    *   Longhorn Deployed with 3 nodes&lt;br&gt;        &lt;br&gt;    *   Add additional disk (Disk#1) ,attach it and mounted to Node-01.&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Create a New Disk, Keep Disk Scheduling disabled&lt;br&gt;2.  Create a volume (vol#1), set replica count to &lt;code&gt;4&lt;/code&gt; and attach it to a node&lt;br&gt;3.  Check (vol#1) replica paths&lt;br&gt;4.  Enable Scheduling on (disk#1)&lt;br&gt;5.  Create a volume (vol#2), set replica count to &lt;code&gt;4&lt;/code&gt; and attach it to a node&lt;br&gt;6.  Check (vol#2) replica paths&lt;/td&gt;&#xA;          &lt;td&gt;*   (vol#1) replicas should be scheduled only to Disks withe Scheduling enabled, no replicas should be scheduled to (disk#1)&lt;br&gt;*   One of (vol#2) replica paths will be scheduled to (disk#1)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Volume Created with Node Tags&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;    *   Longhorn Deployed with 3 nodes&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Create Node tags as follows:&lt;br&gt;    1.  Node-01: fast&lt;br&gt;        &lt;br&gt;    2.  Node-02: slow&lt;br&gt;        &lt;br&gt;    3.  Node-02: fast&lt;br&gt;        &lt;br&gt;2.  Create a volume (vol#1), set Node tags to slow&lt;br&gt;3.  Create a volume (vol#2), set Node tags to fast&lt;br&gt;4.  Check Volumes replicas paths&lt;br&gt;5.  Check Volume detail &lt;code&gt;Node Tags&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;*   vol#1 replicas should only be scheduled to Node-02&lt;br&gt;*   vol#2 replicas should only be scheduled to Node-01 and Node-03&lt;br&gt;*   Node Tag volume detail should contain Node tag specified in volume creation request.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Volumes created with Disk Tags&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;    *   Longhorn Deployed with 3 nodes, with default disks (disk#01-1, disk#02-1, disk#03-1)&lt;br&gt;        &lt;br&gt;    *   &lt;code&gt;disk#0X-Y&lt;/code&gt; indicate that disk is attached to &lt;code&gt;Node-0X&lt;/code&gt; , and it is disk number &lt;code&gt;Y&lt;/code&gt; on that node.&lt;br&gt;        &lt;br&gt;*   Create 3 additional disks (disk#01-2, disk#02-2, disk#03-2), attach each one to a different node, and mount it to a directory on that node.&lt;br&gt;&lt;br&gt;1.  Create Disk tags as follows:&lt;br&gt;    1.  disk#01-1: fast&lt;br&gt;        &lt;br&gt;    2.  disk#01-2: fast&lt;br&gt;        &lt;br&gt;    3.  disk#02-1: slow&lt;br&gt;        &lt;br&gt;    4.  disk#02-2: slow&lt;br&gt;        &lt;br&gt;    5.  disk#03-1: fast&lt;br&gt;        &lt;br&gt;    6.  disk#01-2: fast&lt;br&gt;        &lt;br&gt;2.  Create a volume (vol#1), set Disk tags to slow&lt;br&gt;3.  Create a volume (vol#2), set Disk tags to fast&lt;br&gt;4.  Check Volumes replicas paths&lt;br&gt;5.  Check Volume detail &lt;code&gt;Disk Tags&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;*   vol#1 replicas should only be scheduled to disks have slow tag (disk#02-1 and disk#02-2)&lt;br&gt;*   vol#2 replicas should can be scheduled to disks have fast Tag  &lt;br&gt;    (disk#01-1, disk#01-2, disk#03-1, disk#03-2)&lt;br&gt;*   Disk Tag volume detail should contain Disk tag specified in volume creation request.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Volumes created with both DIsk and Node Tags&lt;/td&gt;&#xA;          &lt;td&gt;*   Create a volume, set Disk and node tags, and attach it to a node&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume replicas should be scheduled only to node that have Node tags, and only on disks that have Disk tags specified on volume creation request&lt;br&gt;*   If No Node match both Node and Disk tags, volume replicas will not be created.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Remove Disk From Node&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;    *   Longhorn Deployed with 3 nodes&lt;br&gt;        &lt;br&gt;    *   Add additional disk (Disk#1) ,attach it and mounted to Node-01.&lt;br&gt;        &lt;br&gt;    *   Some replicas should be scheduled to Disk#1&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Disable Scheduling on disk#1&lt;br&gt;2.  Delete all replicas scheduled to disk#1, replicas should start to rebuild on other disks&lt;br&gt;3.  Delete disk from node&lt;/td&gt;&#xA;          &lt;td&gt;*   Stopping Disk scheduling will prevent replicas to be scheduled on it&lt;br&gt;*   Disk can’t be deleted if at least one replicas is still scheduled to it.&lt;br&gt;*   Disk can be delete only after all replica have been rescheduled to other disks.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Power off a node&lt;/td&gt;&#xA;          &lt;td&gt;1.  Power off a node&lt;/td&gt;&#xA;          &lt;td&gt;*   Node should report down on Node page&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Delete Longhorn Node&lt;/td&gt;&#xA;          &lt;td&gt;1.  Disable Scheduling on the node&lt;br&gt;2.  Delete all replicas on the node to be rescheduled to another nodes&lt;br&gt;3.  Detach all volume attached to the node, re-attach them on other nodes&lt;br&gt;4.  Delete Node from Kubernetes&lt;br&gt;5.  Delete Node From Longhorn&lt;/td&gt;&#xA;          &lt;td&gt;*   Node can’t be deleted if Node Scheduling is enabled on that node&lt;br&gt;*   Node can’t be deleted unless it all replicas are deleted from that node&lt;br&gt;*   Node can’t be deleted unless it all attached volumes get detached from that node&lt;br&gt;*   Node can’t be deleted unless it has been deleted from Kubernetes first&lt;br&gt;*   After node is deleted from Kubernetes, node should report down on Longhorn&lt;br&gt;*   Node should be deleted from Longhorn&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Default Disk on Labeled Nodes&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;    *   Create 3 node k8s cluster&lt;br&gt;        &lt;br&gt;    *   Create &lt;code&gt;/home/longhorn&lt;/code&gt; directory on all 3 nodes&lt;br&gt;        &lt;br&gt;    *   Add new disk to each node, format it with &lt;code&gt;ext4&lt;/code&gt;, and mount it to &lt;code&gt;/mnt/disk&lt;/code&gt;&lt;br&gt;        &lt;br&gt;*   Use the following label and annotations for nodes&lt;br&gt;&lt;br&gt;Node-01 &amp;amp; Node-03&lt;br&gt;&lt;br&gt;&lt;code&gt;labels:&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;node.longhorn.io/create-``default``-disk:&lt;/code&gt; &lt;code&gt;&amp;quot;config&amp;quot;&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;annotations:&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;node.longhorn.io/``default``-disks-config:&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;&#39;[{``&amp;quot;path&amp;quot;``:``&amp;quot;/home/longhorn&amp;quot;``,``&amp;quot;allowScheduling&amp;quot;``:``true``,&lt;/code&gt; &lt;code&gt;&amp;quot;tags&amp;quot;``:[``&amp;quot;ssd&amp;quot;``,&lt;/code&gt; &lt;code&gt;&amp;quot;fast&amp;quot;``]},&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;{``&amp;quot;path&amp;quot;``:``&amp;quot;/mnt/disk&amp;quot;``,``&amp;quot;allowScheduling&amp;quot;``:``true``,``&amp;quot;storageReserved&amp;quot;``:``1024``,``&amp;quot;tags&amp;quot;``:[``&amp;quot;ssd&amp;quot;``,``&amp;quot;fast&amp;quot;``]}]&#39;&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;node.longhorn.io/``default``-node-tags:&lt;/code&gt; &lt;code&gt;&#39;[&amp;quot;fast&amp;quot;, &amp;quot;storage&amp;quot;]&#39;&lt;/code&gt;&lt;br&gt;&lt;br&gt;Node-02&lt;br&gt;&lt;br&gt; &lt;code&gt;labels:&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;node.longhorn.io/create-``default``-disk:&lt;/code&gt; &lt;code&gt;&amp;quot;config&amp;quot;&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;annotations:&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;node.longhorn.io/``default``-disks-config:&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;&#39;[{``&amp;quot;path&amp;quot;``:``&amp;quot;/home/longhorn&amp;quot;``,``&amp;quot;allowScheduling&amp;quot;``:``true``,&lt;/code&gt; &lt;code&gt;&amp;quot;tags&amp;quot;``:[``&amp;quot;hdd&amp;quot;``,&lt;/code&gt; &lt;code&gt;&amp;quot;slow&amp;quot;``]},&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;{``&amp;quot;path&amp;quot;``:``&amp;quot;/mnt/disk&amp;quot;``,``&amp;quot;allowScheduling&amp;quot;``:``true``,``&amp;quot;storageReserved&amp;quot;``:``1024``,``&amp;quot;tags&amp;quot;``:[``&amp;quot;hdd&amp;quot;``,``&amp;quot;slow&amp;quot;``]}]&#39;&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;node.longhorn.io/``default``-node-tags:&lt;/code&gt; &lt;code&gt;&#39;[&amp;quot;slow&amp;quot;, &amp;quot;storage&amp;quot;]&#39;&lt;/code&gt; &lt;br&gt;&lt;br&gt;1.  Set &lt;code&gt;create-default-disk-labeled-nodes: True&lt;/code&gt; in &lt;code&gt;longhorn-default-setting&lt;/code&gt; config map&lt;br&gt;2.  Deploy Longhorn&lt;/td&gt;&#xA;          &lt;td&gt;*   Longhorn Should be deployed successfully&lt;br&gt;*   Node-01 &amp;amp; Node-03&lt;br&gt;    *   should be tagged with &lt;code&gt;fast&lt;/code&gt; and &lt;code&gt;storage&lt;/code&gt; tags&lt;br&gt;        &lt;br&gt;    *   Disk scheduling should be allowed on both disks&lt;br&gt;        &lt;br&gt;    *   Disks should be tagged with &lt;code&gt;ssd&lt;/code&gt; and &lt;code&gt;fast&lt;/code&gt; tags&lt;br&gt;        &lt;br&gt;    *   1024 MB is reserved storage on &lt;code&gt;/mnt/disk&lt;/code&gt;&lt;br&gt;        &lt;br&gt;*   Node-02&lt;br&gt;    *   should be tagged with &lt;code&gt;Slow&lt;/code&gt; and &lt;code&gt;storage&lt;/code&gt; tags&lt;br&gt;        &lt;br&gt;    *   should be tagged with &lt;code&gt;slow&lt;/code&gt; and &lt;code&gt;storage&lt;/code&gt; tags&lt;br&gt;        &lt;br&gt;    *   Disk scheduling should be allowed on both disks&lt;br&gt;        &lt;br&gt;    *   Disks should be tagged with &lt;code&gt;hdd&lt;/code&gt; and &lt;code&gt;slow&lt;/code&gt; tags&lt;br&gt;        &lt;br&gt;    *   1024 MB is reserved storage on &lt;code&gt;/mnt/disk&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Default Data Path&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create 3 node k8s cluster&lt;br&gt;*   Create &lt;code&gt;/home/longhorn&lt;/code&gt; directory on all 3 nodes&lt;br&gt;&lt;br&gt;  &lt;br&gt;&lt;br&gt;1.  Set &lt;code&gt;defaultDataPath&lt;/code&gt; to &lt;code&gt;/home/longhorn/&lt;/code&gt; in &lt;code&gt;longhorn-default-setting&lt;/code&gt; ConfigMap&lt;br&gt;2.  Deploy Longhorn&lt;br&gt;3.  Create a volume, attach it to a node&lt;/td&gt;&#xA;          &lt;td&gt;*   In Longhorn Setting, &lt;code&gt;Default Data Path&lt;/code&gt; should be &lt;code&gt;/home/longhorn&lt;/code&gt;&lt;br&gt;*   All volumes replicas paths should begin with &lt;code&gt;/home/longhorn&lt;/code&gt; prefix&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;Update Taint Toleration Setting&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   All Longhorn volumes should be detached then Longhorn components will be restarted to apply new tolerations.&lt;br&gt;*   Notice that &amp;ldquo;&lt;a href=&#34;http://kubernetes.io&#34;&gt;kubernetes.io&lt;/a&gt;&amp;rdquo; is used as the key of all Kubernetes default tolerations, please do not contain this substring in your toleration setting.&lt;br&gt;*   In Longhorn taint tolerations are column separated&lt;br&gt;&lt;br&gt;1.  Using Kubernetes, taint &lt;strong&gt;Some&lt;/strong&gt; nodes  &lt;br&gt;    For example, &lt;code&gt;key1=value1:NoSchedule&lt;/code&gt; &lt;code&gt;key2=value2:NoExecute&lt;/code&gt;&lt;br&gt;2.  Update Taint Toleration Setting with &lt;code&gt;key1=value1:NoSchedule;key2:NoExecute&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;*   Longhorn Components will be restarted&lt;br&gt;*   Longhorn Components should be rescheduled to tainted nodes nodes only.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;Default Taint Toleration Setting&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create 3 node k8s cluster&lt;br&gt;*   Using Kubernetes, taint &lt;strong&gt;Some&lt;/strong&gt; nodes  &lt;br&gt;    For example, &lt;code&gt;key1=value1:NoSchedule&lt;/code&gt; &lt;code&gt;key2=value2:NoExecute&lt;/code&gt;&lt;br&gt;&lt;br&gt;1.  Set &lt;code&gt;taint-toleration&lt;/code&gt; to &lt;code&gt;key1=value1:NoSchedule;key2:NoExecute&lt;/code&gt; in &lt;code&gt;longhorn-default-setting&lt;/code&gt; ConfigMap&lt;br&gt;2.  Deploy Longhorn&lt;/td&gt;&#xA;          &lt;td&gt;*   Longhorn components should be only deployed to tainted nodes&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;Node Readiness&lt;/td&gt;&#xA;          &lt;td&gt;*   Delete the following Longhorn components pods&lt;br&gt;    *   Engine image&lt;br&gt;        &lt;br&gt;    *   Instance Manager (engine)&lt;br&gt;        &lt;br&gt;    *   Instance Manager (replica)&lt;/td&gt;&#xA;          &lt;td&gt;*   Deleting any components should be reflected on node Readiness&lt;br&gt;*   Deleted component must be redeployed&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;14&lt;/td&gt;&#xA;          &lt;td&gt;Storage Minimal Available Percentage Setting&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;    *   Longhorn Installed&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Change Storage Minimal Available Percentage to &lt;code&gt;50%&lt;/code&gt;&lt;br&gt;2.  Fill up node disk up to 55% of it’s capacity&lt;/td&gt;&#xA;          &lt;td&gt;*   Storage Minimal Available Percentage &lt;strong&gt;default value&lt;/strong&gt; is &lt;code&gt;25%&lt;/code&gt;&lt;br&gt;*   Filled Disk Should be &lt;code&gt;Unschedulable&lt;/code&gt;&lt;br&gt;*   If Node has only one disk, Node also should be &lt;code&gt;Unschedulable&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;15&lt;/td&gt;&#xA;          &lt;td&gt;Storage Over Provisioning Percentage&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;    *   Longhorn Installed&lt;br&gt;        &lt;br&gt;    *   Assume Nodes has disks with 100GB size&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Change &lt;code&gt;Storage Over Provisioning Percentage&lt;/code&gt; to &lt;code&gt;300&lt;/code&gt;&lt;br&gt;2.  Check Node Disks available size for allocation&lt;/td&gt;&#xA;          &lt;td&gt;*   Storage Over Provisioning Percentage default value is &lt;code&gt;200&lt;/code&gt;&lt;br&gt;*   Disk storage that can be allocated relative to the hard drive&amp;rsquo;s capacity should be 3x disk size == &lt;code&gt;300 GB&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;additional-tests&#34;&gt;Additional Tests&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Scenario&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Create Default Disk on Labeled Nodes - True&lt;/td&gt;&#xA;          &lt;td&gt;1.  In Longhorn Setting - set Create Default Disk on Labeled Nodes - True&lt;br&gt;2.  Scale up the number of worker nodes in Rancher&lt;br&gt;3.  The node is displayed in Longhorn UI when it comes up “Active” in rancher.&lt;br&gt;4.  Verify the node’s status is “Disabled”&lt;br&gt;5.  Verify the default disk is NOT created in → Node → Edit Node and Disk&lt;br&gt;6.  Add label &lt;code&gt;node.longhorn.io/create-default-disk=true&lt;/code&gt; on the node&lt;br&gt;7.  Verify the node is seen as “Schedulable” on longhorn UI. Verify Node → Edit Node and Disk, the default disk is created&lt;br&gt;8.  Add a node tag to this node n1&lt;br&gt;9.  Create a volume - volume-1, add node tag n1&lt;br&gt;10.  Attach it to the same node&lt;br&gt;11.  Verify the replica is running successfully.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create Default Disk on Labeled Nodes should be set to True&lt;br&gt;2.  When a new node is added, the node should show up as disabled on Longhorn UI&lt;br&gt;3.  The node should NOT have any default disk&lt;br&gt;4.  Label should be added on the node.&lt;br&gt;5.  The node status changes to “Schedulable” and default disk is created on the node.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;nodedisk-eviction-test-cases&#34;&gt;Node/Disk Eviction Test cases&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Scenario&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Steps&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Attached volume replica evict from node&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;1.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Enable soft anti-affinity to True.&lt;br&gt;4.  Evict replicas from one node&lt;br&gt;5.  Verify the data after the replica is evited.&lt;/td&gt;&#xA;          &lt;td&gt;1.  A replica should be rebuilt in any other node except the evicted node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Interrupt the rebuild after the eviction of replica from node&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;1.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Enable soft anti-affinity to True.&lt;br&gt;4.  Evict replicas from one node&lt;br&gt;5.  When the replica is rebuilding, delete it.&lt;/td&gt;&#xA;          &lt;td&gt;1.  A replica should be rebuilt in any other node except the evicted node.&lt;br&gt;2.  On the deletion of the replica, the system should start to rebuild a new replica.&lt;br&gt;3.  The replica from the evicted node will be removed.&lt;br&gt;4.  Data should be intact.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Node evicted while restore rebuilding is in progress&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;1.  Restore a volume (3 replicas).&lt;br&gt;2.  While it is restoring, evict replicas from one node.&lt;br&gt;3.  When the replica is rebuilding, delete it.&lt;br&gt;1.  The rebuilding replica should be completed and then a new replica should get created on another node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Evict node with multiple replicas of the same volume&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;1.  Enable soft anti-affinity to True.&lt;br&gt;2.  Create a volume (3 replicas) and make sure two replicas exist on the same node.&lt;br&gt;3.  Write data to it and compute md5sum.&lt;br&gt;4.  Evict replicas from the node where the two replicas exist.&lt;br&gt;5.  Verify the data after the replicas are evicted.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Two replicas should be rebuilt in any other node except the evicted node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Evict node with soft anti-affinity as false&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;1.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Evict replicas from one node&lt;br&gt;4.  Verify the data after the replica is evited.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Eviction should stuck.&lt;br&gt;2.  Volume scheduling should fail.&lt;br&gt;3.  There should be logs like &lt;pre&gt;&lt;code&gt;[longhorn-manager-dkn9c] time=``&amp;quot;2020-09-09T00:23:56Z&amp;quot;&lt;/code&gt; &lt;code&gt;level=debug msg=``&amp;quot;Creating one more replica for eviction&amp;quot;&lt;/code&gt;&lt;br&gt;&lt;code&gt;[longhorn-manager-dkn9c] time=``&amp;quot;2020-09-09T00:23:56Z&amp;quot;&lt;/code&gt; &lt;code&gt;level=error msg=``&amp;quot;There&#39;s no available disk for replica vol-1-r-6268393a, size 2147483648&amp;quot;&lt;/code&gt;&lt;br&gt;&lt;code&gt;[longhorn-manager-dkn9c] time=``&amp;quot;2020-09-09T00:23:56Z&amp;quot;&lt;/code&gt; &lt;code&gt;level=error msg=``&amp;quot;unable to schedule replica vol-1-r-6268393a of volume vol-1&amp;quot;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Add node after evicting node with soft anti-affinity as false&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;1.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Evict replicas from one node&lt;br&gt;4.  Add a worker node to the cluster.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Eviction should stuck but recover after the additional node is available for scheduling.&lt;br&gt;2.  Volume scheduling should fail initially but should be successful once the additional node is available.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Multi operation&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Enable soft anti-affinity to True.&lt;br&gt;4.  Select two nodes and evict replicas from them.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Replica eviction should happen one by one from nodes&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Replica eviction from disk with soft anti-affinity as True.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Add an additional disk to a node.&lt;br&gt;2.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;3.  Write data to it and compute md5sum.&lt;br&gt;4.  Enable soft anti-affinity to True.&lt;br&gt;5.  Evict from the additional disk.&lt;/td&gt;&#xA;          &lt;td&gt;1.  A replica should be rebuilt in any other node except the evicted node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Replica eviction from disk with soft anti-affinity as False.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Add an additional disk to a node.&lt;br&gt;2.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;3.  Write data to it and compute md5sum.&lt;br&gt;4.  Enable soft anti-affinity to True.&lt;br&gt;5.  Evict from the additional disk.&lt;/td&gt;&#xA;          &lt;td&gt;1.  A replica should be rebuilt in another disk on the same node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Interrupt eviction&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Enable soft anti-affinity to True.&lt;br&gt;4.  Evict replicas from one node&lt;br&gt;5.  Stop eviction.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The replicas should evict one by one and once the eviction&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;Evict replica of volume with DR volume&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a DR volume (3 replicas)&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Enable soft anti-affinity to True.&lt;br&gt;4.  Evict replicas from one node&lt;br&gt;5.  Verify the data after the replica is evited.&lt;/td&gt;&#xA;          &lt;td&gt;1.  A replica should be rebuilt in any other node except the evicted node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;Evict replica of volume with the restored volume&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Restore a volume (3 replicas) from backup, attach it to a pod.&lt;br&gt;2.  Enable soft anti-affinity to True.&lt;br&gt;3.  Evict replicas from one node&lt;br&gt;4.  Verify the data after the replica is evited.&lt;/td&gt;&#xA;          &lt;td&gt;1.  A replica should be rebuilt in any other node except the evicted node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;Evict replica of volume with the detached volume&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Restore a volume (3 replicas) from backup.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Enable soft anti-affinity to True.&lt;br&gt;4.  Evict replicas from one node&lt;br&gt;5.  Verify the data after the replica is evited.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The volume should get attached to a node.&lt;br&gt;2.  A replica should be rebuilt in any other node except the evicted node.&lt;br&gt;3.  The replica from the evicted node should be removed.&lt;br&gt;4.  The volume should get detached.&lt;br&gt;5.  Data should be intact.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;14&lt;/td&gt;&#xA;          &lt;td&gt;On upgraded setup&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn v1.0.2 installed in set up of 4 worker nodes and 1 etc/control plane&lt;br&gt;2.  Create a volume, restored volume and DR volume&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Upgrade the longhorn to master.&lt;br&gt;2.  Create a volume and attach it to a pod.&lt;br&gt;3.  Write data to it and compute md5sum.&lt;br&gt;4.  Evict replicas from one node&lt;br&gt;5.  Verify the data after the replica is evited.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The replicas of volumes with v1.0.2 engine should also get evicted except the DR volume.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>8. Scheduling</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/scheduling/</guid>
      <description>&lt;h2 id=&#34;manual-test&#34;&gt;Manual Test&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test name&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expectation&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;EKS across zone scheduling&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;:&lt;br&gt;&lt;br&gt;*   EKS Cluster with 3 nodes across two AWS zones (zone#1, zone#2)&lt;br&gt;&lt;br&gt;1.  Create a volume with 2 replicas, and attach it to a node.&lt;br&gt;2.  Delete a replica scheduled to each zone, repeat it few times&lt;br&gt;3.  Scale volume replicas = 3&lt;br&gt;4.  Scale volume replicas to 4&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume replicas should be scheduled one per AWS zone&lt;br&gt;*   Deleting a replica in a zone should trigger a replica rebuild&lt;br&gt;*   new rebuilding replica should be scheduled to the same zone as the deleted replica&lt;br&gt;*   Scaling volume replicas to 3 will distribute replicas across all nodes&lt;br&gt;*   Scaling volume replicas to 4 will be governed by soft anti-affinity rule, so no guarantee on which node the new replica should be scheduled.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;anti-affinity-test&#34;&gt;Anti-affinity test&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expectation&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Automation test case&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Replica scheduling (soft anti-affinity enabled)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;*   &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt; setting is &lt;strong&gt;Enabled&lt;/strong&gt;&lt;br&gt;1.  Create a volume&lt;br&gt;2.  Attach volume to a node&lt;br&gt;3.  Increase replica count to exceed the number of Longhorn node count&lt;/td&gt;&#xA;          &lt;td&gt;*   New replicas will be scheduled to node&lt;br&gt;*   Volume Status will be &lt;code&gt;Healthy&lt;/code&gt;, with limited node redundancy hint icon&lt;br&gt;&lt;code&gt;Limited node redundancy: at least one healthy replica is running at the same node as another&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;test_soft_anti_affinity_scheduling&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Replica scheduling (soft anti-affinity disabled)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;*   &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt; setting is &lt;strong&gt;Enabled&lt;/strong&gt;&lt;br&gt;1.  Create a volume&lt;br&gt;2.  Attach volume to a node&lt;br&gt;3.  Increase replica count to exceed the number of Longhorn node count&lt;br&gt;4.  Disable &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt; setting&lt;br&gt;5.  Delete a replica&lt;br&gt;6.  Re-Enable &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt; setting&lt;/td&gt;&#xA;          &lt;td&gt;*   Replicas won’t be removed after disabling &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt;&lt;br&gt;*   when &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt; setting is disabled New Replicas will not be scheduled to nodes.&lt;br&gt;*   when &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt; setting is re-enabled, New Replicas can be scheduled to nodes.&lt;/td&gt;&#xA;          &lt;td&gt;test_hard_anti_affinity_scheduling&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;additional-tests&#34;&gt;Additional Tests&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Scenario&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Add Disk disk1, Disable scheduling for default disk -1&lt;/td&gt;&#xA;          &lt;td&gt;1.  By default the disk on a node is 0 default disk in in path - &lt;code&gt;/var/lib/longhorn/&lt;/code&gt;&lt;br&gt;2.  Add disk1 on the node&lt;br&gt;3.  Disable scheduling for the default disk&lt;br&gt;4.  Create a volume in Longhorn&lt;br&gt;5.  Verify the replicas are scheduled on disk1&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Add Disk disk1, Disable scheduling for default disk -2&lt;/td&gt;&#xA;          &lt;td&gt;Cluster spec - 3 worker nodes&lt;br&gt;&lt;br&gt;1.  Create a volume - 3 replicas in &lt;code&gt;/var/lib/longhorn/&lt;/code&gt; - default disk&lt;br&gt;2.  Add disk 1 on &lt;code&gt;/mnt/vol2&lt;/code&gt;on node 1&lt;br&gt;3.  Disable scheduling for the default disk&lt;br&gt;4.  enable scheduling for disk1&lt;br&gt;5.  Update the replicas to count = 4&lt;br&gt;6.  Say a replica is built on Node 2&lt;br&gt;7.  Delete the replica on node 1&lt;br&gt;8.  a new replica is rebuilt on node 1&lt;br&gt;9.  Verify replica is now available in &lt;code&gt;/mnt/vol2&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Replica when rebuilt on node 1 should be available on disk 1 - &lt;code&gt;/mnt/vol2&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Disable Scheduling On Cordoned Node&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;True&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;New volume&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Cordon a node W1&lt;br&gt;3.  Create a new volume with 4 replicas&lt;br&gt;4.  Verify the volume &lt;code&gt;vol-1&lt;/code&gt; is in detached state with error &lt;code&gt;Scheduling Failure Replica Scheduling Failure&lt;/code&gt;with the 4th replica in N/A state&lt;br&gt;5.  Add a new worker node W5 to the cluster&lt;br&gt;6.  &lt;code&gt;vol-1&lt;/code&gt; should become healthy.&lt;br&gt;7.  Attach it to a workload and verify data can be written into the volume&lt;/td&gt;&#xA;          &lt;td&gt;1.  &lt;code&gt;vol-1&lt;/code&gt; should be in detached state with error &lt;code&gt;Scheduling Failure Replica Scheduling Failure&lt;/code&gt;&lt;br&gt;2.  vol-1 should become healthy and should be used in a workload to write data into the volume&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;True&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;Existing volume&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Create a new volume with 4 replicas&lt;br&gt;3.  Volume vol-1 should be in a healthy detached state&lt;br&gt;4.  Attach it to a workload and verify data can be written into the volume&lt;br&gt;5.  cordon a worker node&lt;br&gt;6.  Use the. volume to a workload&lt;br&gt;7.  All the three replicas will be in running healthy state&lt;br&gt;8.  Delete replica on cordoned worker node&lt;br&gt;9.  Verify the volume &lt;code&gt;vol-1&lt;/code&gt; is in degraded state with error &lt;code&gt;Scheduling Failure Replica Scheduling Failure&lt;/code&gt;with the 4th replica in N/A state&lt;br&gt;10.  Add a new worker node W5 to the cluster&lt;br&gt;11.  Verify the repliica failed will be in rebuilding state now&lt;br&gt;12.  &lt;code&gt;vol-1&lt;/code&gt; should become healthy.&lt;br&gt;13.  Verify the data is consistent&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;False&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;New volume&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Cordon a node W1&lt;br&gt;3.  Create a new volume with 4 replicas&lt;br&gt;4.  &lt;code&gt;vol-1&lt;/code&gt; should be in healthy.&lt;br&gt;5.  Verify a replica is created on the cordoned worker node&lt;br&gt;6.  Attach it to a workload and verify data can be written into the volume&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;False&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;Existing volume&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;True&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Backup restore&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Cordon a node W1&lt;br&gt;3.  Create a backup restore volume from an existing backup.&lt;br&gt;4.  Give in the number of replicas - 4, volume name: &lt;code&gt;vol-2&lt;/code&gt;&lt;br&gt;5.  Verify the volume &lt;code&gt;vol-2&lt;/code&gt; is in detached state with error &lt;code&gt;Scheduling Failure Replica Scheduling Failure&lt;/code&gt;with the 4th replica in N/A state&lt;br&gt;6.  Verify no restoring should happen on the replicas.&lt;br&gt;7.  Add a new worker node W5 to the cluster&lt;br&gt;8.  &lt;code&gt;vol-2&lt;/code&gt; should start restoring now&lt;br&gt;9.  &lt;code&gt;vol-2&lt;/code&gt; should be in detached healthy state.&lt;br&gt;10.  attach to a workload and verify the checksum of data with that of the original one&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;False&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Backup restore&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Cordon a node W1&lt;br&gt;3.  Create a backup restore volume from an existing backup.&lt;br&gt;4.  Give in the number of replicas - 4, volume name: &lt;code&gt;vol-2&lt;/code&gt;&lt;br&gt;5.  Verify volume is in attached state and restoring should happen on the replicas&lt;br&gt;6.  &lt;code&gt;vol-2&lt;/code&gt; should be in detached healthy state. after restoration is complete&lt;br&gt;7.  attach to a workload and verify the checksum of data with that of the original one&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;True&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Create DR volume&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Cordon a node W1&lt;br&gt;3.  Create a &lt;code&gt;DRV&lt;/code&gt; from an existing backup.&lt;br&gt;4.  Give in the number of replicas - 4&lt;br&gt;5.  Verify the &lt;code&gt;DRV&lt;/code&gt; is in detached state with error &lt;code&gt;Scheduling Failure Replica Scheduling Failure&lt;/code&gt;with the 4th replica in N/A state&lt;br&gt;6.  Verify no restoring should happen on the replicas.&lt;br&gt;7.  Add a new worker node W5 to the cluster&lt;br&gt;8.  &lt;code&gt;DRV&lt;/code&gt; should start restoring now&lt;br&gt;9.  &lt;code&gt;DRV&lt;/code&gt; should be in healthy state.&lt;br&gt;10.  Activate the &lt;code&gt;DRV&lt;/code&gt; and verify it is in detached state&lt;br&gt;11.  attach to a workload and verify the checksum of data with that of the original one&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;False&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Create DR volume&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Cordon a node W1&lt;br&gt;3.  Create a &lt;code&gt;DRV&lt;/code&gt; from an existing backup.&lt;br&gt;4.  Give in the number of replicas - 4&lt;br&gt;5.  &lt;code&gt;DRV&lt;/code&gt; should start restoring now&lt;br&gt;6.  &lt;code&gt;DRV&lt;/code&gt; should be in healthy state.&lt;br&gt;7.  Activate the &lt;code&gt;DRV&lt;/code&gt; and verify it is in detached state&lt;br&gt;8.  attach to a workload and verify the checksum of data with that of the original one&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Replica node level soft anti affinity: &lt;strong&gt;False&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;New volume&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 3 worker nodes - custom cluster&lt;br&gt;2.  Create a volume with replicas - 4&lt;br&gt;3.  Volume should be in detached state with error - &lt;code&gt;Scheduling Failure Replica Scheduling Failure&lt;/code&gt;with the 4th replica in N/A state&lt;br&gt;4.  Add a worker node&lt;br&gt;5.  the volume should be in healthy state&lt;br&gt;6.  User should be able to use the volume on the workload&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Replica node level soft anti affinity: &lt;strong&gt;True&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;New volume&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 3 worker nodes - custom cluster&lt;br&gt;2.  Create a volume with replicas - 4&lt;br&gt;3.  the volume should be in healthy state. two replicas should be on the same host&lt;br&gt;4.  User should be able to use the volume on the workload&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>9. Upgrade</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/upgrade/</guid>
      <description>&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test name&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Higher version of Longhorn engine and lower version of volume&lt;/td&gt;&#xA;          &lt;td&gt;Test Longhorn upgrade&lt;br&gt;&lt;br&gt;1.  Create a volume, generate and write &lt;code&gt;data&lt;/code&gt; into the volume.&lt;br&gt;2.  Keep the volume attached, then upgrade Longhorn system.&lt;br&gt;3.  Write data in volume.&lt;br&gt;4.  Take snapshot#1. Compute the checksum#1&lt;br&gt;5.  Write data to volume. Compute the checksum#2&lt;br&gt;6.  Take backup&lt;br&gt;7.  Revert to snapshot#1&lt;br&gt;8.  Restore the backup.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Restore the backup taken with older engine version&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a volume, attach to a pod and write data into the volume. Compute md5sum of data&lt;br&gt;2.  Take a backup.&lt;br&gt;3.  Upgrade engine.&lt;br&gt;4.  Make the upgraded engine as default.&lt;br&gt;5.  Restore the backup, verify the checksum&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>Monitoring</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/monitoring/</guid>
      <description>&lt;h2 id=&#34;prometheus-support-test-cases&#34;&gt;Prometheus Support test cases&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install the Prometheus Operator (include a role and service account for it). For example:&lt;pre&gt;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: ClusterRoleBinding&#xA;metadata:&#xA;name: prometheus-operator&#xA;namespace: default&#xA;roleRef:&#xA;apiGroup: rbac.authorization.k8s.io&#xA;kind: ClusterRole&#xA;name: prometheus-operator&#xA;subjects:&lt;br&gt;  - kind: ServiceAccount&#xA;name: prometheus-operator&#xA;namespace: default&lt;br&gt;&amp;ndash;&#xA;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: ClusterRole&#xA;metadata:&#xA;name: prometheus-operator&#xA;namespace: default&#xA;rules:&lt;br&gt;  - apiGroups:&lt;br&gt;      - extensions&#xA;resources:&lt;br&gt;      - thirdpartyresources&#xA;verbs: [&amp;quot;&lt;em&gt;&amp;quot;]&lt;br&gt;  - apiGroups:&lt;br&gt;      - apiextensions.k8s.io&#xA;resources:&lt;br&gt;      - customresourcedefinitions&#xA;verbs: [&amp;quot;&lt;/em&gt;&amp;quot;]&lt;br&gt;  - apiGroups:&lt;br&gt;      - monitoring.coreos.com&#xA;resources:&lt;br&gt;      - alertmanagers&lt;br&gt;      - prometheuses&lt;br&gt;      - prometheuses/finalizers&lt;br&gt;      - servicemonitors&lt;br&gt;      - prometheusrules&lt;br&gt;      - podmonitors&#xA;verbs: [&amp;quot;&lt;em&gt;&amp;quot;]&lt;br&gt;  - apiGroups:&lt;br&gt;      - apps&#xA;resources:&lt;br&gt;      - statefulsets&#xA;verbs: [&amp;quot;&lt;/em&gt;&amp;quot;]&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - configmaps&lt;br&gt;      - secrets&#xA;verbs: [&amp;quot;*&amp;quot;]&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - pods&#xA;verbs: [&amp;ldquo;list&amp;rdquo;, &amp;ldquo;delete&amp;rdquo;]&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - services&lt;br&gt;      - endpoints&#xA;verbs: [&amp;ldquo;get&amp;rdquo;, &amp;ldquo;create&amp;rdquo;, &amp;ldquo;update&amp;rdquo;]&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - nodes&#xA;verbs: [&amp;ldquo;list&amp;rdquo;, &amp;ldquo;watch&amp;rdquo;]&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - namespaces&#xA;verbs: [&amp;ldquo;list&amp;rdquo;, &amp;ldquo;watch&amp;rdquo;]&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre&gt;&#xA;apiVersion: v1&#xA;kind: ServiceAccount&#xA;metadata:&#xA;  name: prometheus-operator&#xA;  namespace: default&#xA;---&#xA;apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  labels:&#xA;    app: prometheus-operator&#xA;  name: prometheus-operator&#xA;  namespace: default&#xA;spec:&#xA;  replicas: 1&#xA;  selector:&#xA;    matchLabels:&#xA;      app: prometheus-operator&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: prometheus-operator&#xA;    spec:&#xA;      containers:&#xA;        - args:&#xA;            - --kubelet-service=kube-system/kubelet&#xA;            - --config-reloader-image=quay.io/coreos/configmap-reload:v0.0.1&#xA;          image: quay.io/coreos/prometheus-operator:v0.36.0&#xA;          name: prometheus-operator&#xA;          ports:&#xA;            - containerPort: 8080&#xA;              name: http&#xA;          resources:&#xA;            limits:&#xA;              cpu: 200m&#xA;              memory: 100Mi&#xA;            requests:&#xA;              cpu: 100m&#xA;              memory: 50Mi&#xA;      securityContext:&#xA;        runAsNonRoot: true&#xA;        runAsUser: 65534&#xA;      serviceAccountName: prometheus-operator&lt;/pre&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;Install a Service Monitor pointing to &lt;code&gt;longhon-backend&lt;/code&gt; service by selecting &lt;code&gt;app: longhorn-manager&lt;/code&gt; label. For example:&lt;pre&gt;apiVersion: monitoring.coreos.com/v1&#xA;kind: ServiceMonitor&#xA;metadata:&#xA;name: longhorn-backend&#xA;labels:&#xA;team: backend&#xA;spec:&#xA;selector:&#xA;matchLabels:&#xA;app: longhorn-manager&#xA;namespaceSelector:&#xA;matchNames:&lt;br&gt;    - longhorn-system&#xA;endpoints:&lt;br&gt;  - port: manager&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Install Prometheus (include a role and service account for it). Include the above &lt;code&gt;service monitor&lt;/code&gt; in the Prometheus&amp;rsquo;s config. Expose to the Prometheus instance to outside using a service of type NodePort. For example:&lt;pre&gt;apiVersion: v1&#xA;kind: ServiceAccount&#xA;metadata:&#xA;name: prometheus&lt;br&gt;&amp;mdash;&#xA;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: ClusterRole&#xA;metadata:&#xA;name: prometheus&#xA;rules:&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - nodes&lt;br&gt;      - services&lt;br&gt;      - endpoints&lt;br&gt;      - pods&lt;br&gt;    verbs: [&amp;ldquo;get&amp;rdquo;, &amp;ldquo;list&amp;rdquo;, &amp;ldquo;watch&amp;rdquo;]&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - configmaps&#xA;verbs: [&amp;ldquo;get&amp;rdquo;]&lt;br&gt;  - nonResourceURLs: [&amp;quot;/metrics&amp;quot;, &amp;ldquo;/federate&amp;rdquo;]&#xA;verbs: [&amp;ldquo;get&amp;rdquo;]&lt;br&gt;&amp;mdash;&#xA;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: ClusterRoleBinding&#xA;metadata:&#xA;name: prometheus&#xA;roleRef:&#xA;apiGroup: rbac.authorization.k8s.io&#xA;kind: ClusterRole&#xA;name: prometheus&#xA;subjects:&lt;br&gt;  - kind: ServiceAccount&#xA;name: prometheus&#xA;namespace: default&lt;br&gt;&amp;mdash;&#xA;apiVersion: monitoring.coreos.com/v1&#xA;kind: Prometheus&#xA;metadata:&#xA;name: prometheus&#xA;spec:&#xA;serviceAccountName: prometheus&#xA;serviceMonitorSelector:&#xA;matchLabels:&#xA;team: backend&lt;br&gt;&amp;mdash;&#xA;apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;name: prometheus&#xA;spec:&#xA;type: NodePort&#xA;ports:&lt;br&gt;  - name: web&#xA;port: 9090&#xA;protocol: TCP&#xA;targetPort: web&#xA;ports:&lt;br&gt;    - port: 9090&#xA;selector:&#xA;prometheus: prometheus&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Find the &lt;code&gt;prometheus&lt;/code&gt; service and access Prometheus web UI using the nodeIP and the port&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Scenario&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Steps&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;All the Metrics are present&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;2.  Verify the metrics are available.&lt;/td&gt;&#xA;          &lt;td&gt;The below metrics should be available:&lt;br&gt;&lt;br&gt;1.  longhorn_volume_capacity_bytes&amp;lt;/pre&lt;br&gt;2.  longhorn_volume_usage_bytes&amp;lt;/pre&lt;br&gt;3.  longhorn_node_status&amp;lt;/pre&lt;br&gt;4.  onghorn_instance_manager_cpu_requests_millicpu&amp;lt;/pre&lt;br&gt;5.  longhorn_instance_manager_cpu_usage_millicpu&amp;lt;/pre&lt;br&gt;6.  longhorn_instance_manager_memory_requests_bytes&amp;lt;/pre&lt;br&gt;7.  longhorn_instance_manager_memory_usage_bytes&amp;lt;/pre&lt;br&gt;8.  longhorn_manager_cpu_usage_millicpu&amp;lt;/pre&lt;br&gt;9.  longhorn_manager_memory_usage_bytes&amp;lt;/pre&lt;br&gt;10.  longhorn_disk_capacity_bytes&amp;lt;/pre&lt;br&gt;11.  longhorn_disk_usage_bytes&amp;lt;/pre&lt;br&gt;12.  longhorn_node_capacity_bytes&amp;lt;/pre&lt;br&gt;13.  longhorn_node_usage_bytes&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_volume_capacity_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create 4 volumes of different sizes. (2, 3, 4, 5 Gi)&amp;lt;/pre&lt;br&gt;2.  Attach 1st volume to a pod and write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Attach 2nd volume to a pod and don’t write into.&amp;lt;/pre&lt;br&gt;4.  Leave the 3rd volume to the detached state.&amp;lt;/pre&lt;br&gt;5.  Attach the 4th volume to pod and write 1.5 Gi data into it. Detach the volume.&amp;lt;/pre&lt;br&gt;6.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;7.  Select &lt;code&gt;longhorn_volume_capacity_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the volumes should be identified by Prometheus&amp;lt;/pre&lt;br&gt;2.  All the volumes should show the capacity as 2 Gi&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_volume_usage_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create 4 volumes of different sizes. (2, 3, 4, 5 Gi)&amp;lt;/pre&lt;br&gt;2.  Attach 1st volume to a pod and write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Attach 2nd volume to a pod and don’t write into.&amp;lt;/pre&lt;br&gt;4.  Leave the 3rd volume to the detached state.&amp;lt;/pre&lt;br&gt;5.  Attach the 4th volume to pod and write 1.5 Gi data into it. Detach the volume.&amp;lt;/pre&lt;br&gt;6.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;7.  Select &lt;code&gt;longhorn_volume_usage_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the volumes should be identified by Prometheus&amp;lt;/pre&lt;br&gt;2.  Volume-1 should show 1 Gi&amp;lt;/pre&lt;br&gt;3.  Volume-2 should show 0 Gi&amp;lt;/pre&lt;br&gt;4.  Volume-3 should show 0 Gi&amp;lt;/pre&lt;br&gt;5.  Volume-4 should show 1.5 Gi&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_node_status&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Power down a node.&amp;lt;/pre&lt;br&gt;2.  Disable a node.&amp;lt;/pre&lt;br&gt;3.  Add a new node in the cluster.&amp;lt;/pre&lt;br&gt;4.  Delete a node from the cluster.&amp;lt;/pre&lt;br&gt;5.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;6.  Select &lt;code&gt;longhorn_node_status&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the nodes should be identified by Prometheus and one node should be shown in 3 rows based on the condition - &lt;code&gt;mountpropagation, ready, schedulable&lt;/code&gt;&amp;lt;/pre&lt;br&gt;2.  The correct status should be shown on Prometheus UI.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_instance_manager_cpu_requests_millicpu&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a volume and attach it to a pod.&amp;lt;/pre&lt;br&gt;2.  Write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Set multiple recurring backup on the volume.&amp;lt;/pre&lt;br&gt;4.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;5.  Select &lt;code&gt;longhorn_instance_manager_cpu_requests_millicpu&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The reading of cpu_requests should go up for the attached instance manager.&amp;lt;/pre&lt;br&gt;2.  The reading of other instance managers should not get impacted.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_instance_manager_cpu_usage_millicpu&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a volume and attach it to a pod.&amp;lt;/pre&lt;br&gt;2.  Write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Set multiple recurring backup on the volume.&amp;lt;/pre&lt;br&gt;4.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;5.  Select &lt;code&gt;longhorn_instance_manager_cpu_usage_millicpu&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The reading of cpu_usage should be shown correctly&amp;lt;/pre&lt;br&gt;2.  The reading of other instance managers should not get impacted.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_instance_manager_memory_requests_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a volume and attach it to a pod.&amp;lt;/pre&lt;br&gt;2.  Write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Set multiple recurring backup on the volume.&amp;lt;/pre&lt;br&gt;4.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;5.  Select &lt;code&gt;longhorn_instance_manager_memory_requests_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The reading of memory_requests should go up for the attached instance manager.&amp;lt;/pre&lt;br&gt;2.  The reading of other instance managers should not get impacted.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_instance_manager_memory_usage_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a volume and attach it to a pod.&amp;lt;/pre&lt;br&gt;2.  Write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Set multiple recurring backup on the volume.&amp;lt;/pre&lt;br&gt;4.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;5.  Select &lt;code&gt;longhorn_instance_manager_memory_usage_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The reading of memory_usage should go up for the attached instance manager.&amp;lt;/pre&lt;br&gt;2.  The reading of other instance managers should not get impacted.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_manager_cpu_usage_millicpu&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create 3 volumes of different sizes.&amp;lt;/pre&lt;br&gt;2.  Attach 1st volume to a pod and write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Leave the 2rd volume to the detached state.&amp;lt;/pre&lt;br&gt;4.  Attach the 3th volume to pod and write 1.5 Gi data into it. Attach the volume in maintenance mode.&amp;lt;/pre&lt;br&gt;5.  Set a recurring backup on volume 1st.&amp;lt;/pre&lt;br&gt;6.  Perform revert to snapshot with 3rd volume.&amp;lt;/pre&lt;br&gt;7.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;8.  Select &lt;code&gt;longhorn_manager_cpu_usage_millicpu&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Monitor the graph and the console on the Prometheus server, the cpu_usage should go up.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_manager_memory_usage_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create 3 volumes of different sizes.&amp;lt;/pre&lt;br&gt;2.  Attach 1st volume to a pod and write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Leave the 2rd volume to the detached state.&amp;lt;/pre&lt;br&gt;4.  Attach the 3th volume to pod and write 1.5 Gi data into it. Attach the volume in maintenance mode.&amp;lt;/pre&lt;br&gt;5.  Set a recurring backup on volume 1st.&amp;lt;/pre&lt;br&gt;6.  Perform revert to snapshot with 3rd volume.&amp;lt;/pre&lt;br&gt;7.  Try to make disk full of a node where &lt;code&gt;longhorn-manager&lt;/code&gt; is running.&amp;lt;/pre&lt;br&gt;8.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;9.  Select &lt;code&gt;longhorn_manager_memory_usage_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Monitor the graph and the console on the Prometheus server, the memory_usage should go up.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_disk_capacity_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create volumes and attach them to each node.&amp;lt;/pre&lt;br&gt;2.  Add an additional disk to all the nodes. (Different size)&amp;lt;/pre&lt;br&gt;3.  Write into the volumes.&amp;lt;/pre&lt;br&gt;4.  Power down a node.&amp;lt;/pre&lt;br&gt;5.  Disable a node.&amp;lt;/pre&lt;br&gt;6.  Add a new node in the cluster.&amp;lt;/pre&lt;br&gt;7.  Delete a node from the cluster.&amp;lt;/pre&lt;br&gt;8.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;9.  Select &lt;code&gt;longhorn_disk_capacity_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the disks should be identified by Prometheus.&amp;lt;/pre&lt;br&gt;2.  All the disks should show the correct total size of the disks.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_disk_usage_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create volumes and attach them to each node.&amp;lt;/pre&lt;br&gt;2.  Add an additional disk to all the nodes. (Different size)&amp;lt;/pre&lt;br&gt;3.  Write into the volumes.&amp;lt;/pre&lt;br&gt;4.  Power down a node.&amp;lt;/pre&lt;br&gt;5.  Disable a node.&amp;lt;/pre&lt;br&gt;6.  Add a new node in the cluster.&amp;lt;/pre&lt;br&gt;7.  Delete a node from the cluster.&amp;lt;/pre&lt;br&gt;8.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;9.  Select &lt;code&gt;longhorn_disk_usage_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the disks should be identified by Prometheus.&amp;lt;/pre&lt;br&gt;2.  All the disks should show the occupied size of the disks.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_node_capacity_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create volumes and attach them to each node.&amp;lt;/pre&lt;br&gt;2.  Add an additional disk to all the nodes. (Different size)&amp;lt;/pre&lt;br&gt;3.  Write into the volumes.&amp;lt;/pre&lt;br&gt;4.  Power down a node.&amp;lt;/pre&lt;br&gt;5.  Disable a node.&amp;lt;/pre&lt;br&gt;6.  Add a new node in the cluster.&amp;lt;/pre&lt;br&gt;7.  Delete a node from the cluster.&amp;lt;/pre&lt;br&gt;8.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;9.  Select &lt;code&gt;longhorn_node_capacity_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the nodes should be identified by Prometheus.&amp;lt;/pre&lt;br&gt;2.  All the nodes should show the total capacity available of disks available.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;14&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_node_usage_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create volumes and attach them to each node.&amp;lt;/pre&lt;br&gt;2.  Add an additional disk to all the nodes. (Different size)&amp;lt;/pre&lt;br&gt;3.  Write into the volumes.&amp;lt;/pre&lt;br&gt;4.  Power down a node.&amp;lt;/pre&lt;br&gt;5.  Disable a node.&amp;lt;/pre&lt;br&gt;6.  Add a new node in the cluster.&amp;lt;/pre&lt;br&gt;7.  Delete a node from the cluster.&amp;lt;/pre&lt;br&gt;8.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;9.  Select &lt;code&gt;longhorn_node_usage_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the nodes should be identified by Prometheus&amp;lt;/pre&lt;br&gt;2.  All the nodes should show the occupied space on all disks attached to the node.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;Note: More details can be found on &lt;a href=&#34;https://longhorn.io/docs/1.2.2/monitoring/&#34;&gt;https://longhorn.io/docs/1.2.2/monitoring/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
