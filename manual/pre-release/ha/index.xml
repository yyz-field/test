<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HA on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/</link>
    <description>Recent content in HA on Longhorn Manual Test Cases</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Backing Image Error Reporting and Retry</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/backing-image-error-reporting-and-retry/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/backing-image-error-reporting-and-retry/</guid>
      <description>&lt;h2 id=&#34;backing-image-with-an-invalid-url-schema&#34;&gt;Backing image with an invalid URL schema&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a backing image via a invalid download URL. e.g., &lt;code&gt;httpsinvalid://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/code&gt;, &lt;code&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.invalid.com/parrot.raw&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the download start. The backing image data source pod, which is used to download the file from the URL, should become &lt;code&gt;Failed&lt;/code&gt; then be cleaned up immediately.&lt;/li&gt;&#xA;&lt;li&gt;The corresponding and only entry in the disk file status should be &lt;code&gt;failed&lt;/code&gt;. The error message in this entry should explain why the downloading or the pod becomes failed.&lt;/li&gt;&#xA;&lt;li&gt;Check if there is a backoff window for the downloading retry. The initial duration is 1 minute. The max interval is 5 minute. This can be verified by checking the timestamp of the error message or the logs in the longhorn manager pods.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;backing-image-with-sync-failure&#34;&gt;Backing image with sync failure&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a backing image. Then create and attach a volume using this backing image.&lt;/li&gt;&#xA;&lt;li&gt;Exec into one of the worker node, remove the files in that backing image directory and set the directory as immutable.&#xA;The removal of the files will trigger the sync process to sync backing image file from another worker node. Setting immutable to the directory will make the sync process and the following retry failed.&#xA;Option 1. Run the following command to remove the content of the backing image work directory and set the directory as immutable, which should cause the sync to fail.&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rm /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;/backing*&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chattr +i /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;Option 2: Run the following command to remove the content of the backing image work directory and wait for the backing.tmp file to be generated, indicating that the sync has started and the file is transferring. Set the directory as immutable during the transfer should still fail the sync since it can&amp;rsquo;t rename the tmp file to remove the .tmp extent in the end.&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rm /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;/backing* &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; true; &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; ls -l /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;/backing.tmp &amp;gt;/dev/null 2&amp;gt;&amp;amp;1; &lt;span style=&#34;color:#66d9ef&#34;&gt;then&lt;/span&gt; ls -l /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;; break; &lt;span style=&#34;color:#66d9ef&#34;&gt;fi&lt;/span&gt;; &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chattr +i /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;Monitor the backing-image-manager pod log. Verify the backoff works for the sync retry as well.&lt;/li&gt;&#xA;&lt;li&gt;Unset the immutable flag for the backing image directory. Then the retry should succeed, and the volume should become &lt;code&gt;healthy&lt;/code&gt; again after the backing image re-sync complete.&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chattr -i /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Disk migration in AWS ASG</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/disk-migration-in-aws-asg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/disk-migration-in-aws-asg/</guid>
      <description>&lt;h2 id=&#34;some-longhorn-worker-nodes-in-aws-auto-scaling-group-is-in-replacement&#34;&gt;Some Longhorn worker nodes in AWS Auto Scaling group is in replacement&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Launch a Kubernetes cluster with the nodes in AWS Auto Scaling group. Make sure there is an additional EBS attached to instance with setting &lt;code&gt;Delete on Termination&lt;/code&gt; disabled.&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn v1.1.0 on the cluster and Set &lt;code&gt;ReplicaReplenishmentWaitInterval&lt;/code&gt;. Make sure it&amp;rsquo;s longer than the time needs for node replacement.&lt;/li&gt;&#xA;&lt;li&gt;Deploy some workloads using Longhorn volumes.&lt;/li&gt;&#xA;&lt;li&gt;Trigger the ASG instance refresh in AWS.&lt;/li&gt;&#xA;&lt;li&gt;Manually attach EBS to new instance and mount the disk.&lt;/li&gt;&#xA;&lt;li&gt;Add the disk in longhorn to make the existing replica available to be identified by Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Verify new replicas won&amp;rsquo;t be created before reaching &lt;code&gt;ReplicaReplenishmentWaitInterval&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Verify the failed replicas are reused after the node recovery.&lt;/li&gt;&#xA;&lt;li&gt;Verify if workloads still work fine with the volumes after the recovery.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;ebs-migration-in-aws-in-a-asg-set-up-using-script&#34;&gt;EBS migration in AWS in a ASG set up using script&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Launch a Kubernetes cluster with 1 node in AWS Auto Scaling group. Make sure there is an additional EBS attached to instance with setting &lt;code&gt;Delete on Termination&lt;/code&gt; disabled.&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn v1.1.0 on the cluster and Set &lt;code&gt;ReplicaReplenishmentWaitInterval&lt;/code&gt;. Make sure it&amp;rsquo;s longer than the time needs for node replacement.&lt;/li&gt;&#xA;&lt;li&gt;Deploy some workloads using Longhorn volumes.&lt;/li&gt;&#xA;&lt;li&gt;Modify launch template of ASG and provide mount command for EBS.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo mkdir -p /data&#xA;sudo mount /dev/xvdh /data&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Trigger ASG instance refresh with below script using AWS Cli, which will attach the existing EBS volume to the new instance.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;AWS_REGION=&amp;lt;Your aws region&amp;gt;&#xA;INSTANCE_NAME=&amp;lt;Instance name&amp;gt;&#xA;VOLUME_STATUS=&amp;#39;&amp;#39;&#xA;INSTANCE_ID=$(aws --region=$AWS_REGION ec2 describe-instances --filters &amp;#39;Name=tag:Name,Values=$INSTANCE_NAME&amp;#39; | jq -r &amp;#39;.Reservations[].Instances[].InstanceId&amp;#39;)&#xA;VOLUME_ID= $(aws --region=$AWS_REGION ec2 describe-volumes --filters &amp;#39;Name=&amp;#39;attachment.instance-id&amp;#39;,Values=&amp;#39;$INSTANCE_ID&amp;#39;&amp;#39; | jq -r &amp;#39;.Volumes[1].VolumeId&amp;#39;)&#xA;&#xA;aws autoscaling start-instance-refresh --auto-scaling-group-name my-asg --preferences &amp;#39;{&amp;#34;InstanceWarmup&amp;#34;: 300, &amp;#34;MinHealthyPercentage&amp;#34;: 90}&amp;#39;&#xA;&#xA;until [ &amp;#34;x$VOLUME_STATUS&amp;#34; == &amp;#34;xattached&amp;#34; ]; do&#xA;    VOL_STATUS=$(aws ec2 describe-volumes --volume-ids $VOL_ID --query &amp;#39;Volumes[0].State&amp;#39;)&#xA;    sleep 5&#xA;done&#xA;INSTANCE_ID=$(aws --region=$AWS_REGION ec2 describe-instances --filters &amp;#39;Name=tag:Name,Values=$INSTANCE_NAME&amp;#39; | jq -r &amp;#39;.Reservations[].Instances[].InstanceId&amp;#39;)&#xA;aws ec2 attach-volume --volume-id $VOLUME_ID --instance-id $INSTANCE_ID --device /dev/sdh&amp;#39;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Add the disk in longhorn to make the existing replica available to be identified by Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Verify new replicas won&amp;rsquo;t be created before reaching &lt;code&gt;ReplicaReplenishmentWaitInterval&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Verify the failed replicas are reused after the node recovery.&lt;/li&gt;&#xA;&lt;li&gt;Verify if workloads still work fine with the volumes after the recovery.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Note: 1 node cluster is taken only for testing purpose, in real scenario more complex script would be needed for cluster having multiple nodes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>HA Volume Migration</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/ha-volume-migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/ha-volume-migration/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a migratable volume:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy a migratable StorageClass. e.g., &lt;a href=&#34;https://github.com/longhorn/longhorn/blob/master/examples/rwx/storageclass-migratable.yaml&#34;&gt;https://github.com/longhorn/longhorn/blob/master/examples/rwx/storageclass-migratable.yaml&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a PVC with access mode &lt;code&gt;ReadWriteMany&lt;/code&gt; via this StorageClass.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Attach a volume to a node and wait for volume running. Then write some data into the volume. Here I would recommend directly restoring a volume (set &lt;code&gt;fromBackup&lt;/code&gt; in the StorageClass) and attach it instead.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Start the migration by request attaching to another node for the volume.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Trigger the following scenarios then confirm or rollback the migration:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Replica Rebuilding</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/replica-rebuilding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/replica-rebuilding/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create and attach a volume.&lt;/li&gt;&#xA;&lt;li&gt;Write a large amount of data to the volume.&lt;/li&gt;&#xA;&lt;li&gt;Disable disk scheduling and the node scheduling for one replica.&lt;/li&gt;&#xA;&lt;li&gt;Crash the replica progress. Verify&#xA;&lt;ol&gt;&#xA;&lt;li&gt;the corresponding replica will become ERROR.&lt;/li&gt;&#xA;&lt;li&gt;the volume will keep robustness Degraded.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Enable the disk scheduling. Verify nothing changes.&lt;/li&gt;&#xA;&lt;li&gt;Enable the node scheduling. Verify.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;the failed replica is reused by Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;the rebuilding progress in UI page looks good.&lt;/li&gt;&#xA;&lt;li&gt;the data content is correct after rebuilding.&lt;/li&gt;&#xA;&lt;li&gt;volume r/w works fine.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Direct delete one replica via UI. Verify&#xA;&lt;ol&gt;&#xA;&lt;li&gt;a new replica will be replenished immediately.&lt;/li&gt;&#xA;&lt;li&gt;the rebuilding progress in UI page looks good.&lt;/li&gt;&#xA;&lt;li&gt;the data content is correct after rebuilding.&lt;/li&gt;&#xA;&lt;li&gt;volume r/w works fine.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
