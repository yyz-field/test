<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Node Down on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/</link>
    <description>Recent content in Node Down on Longhorn Manual Test Cases</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[#1355](https://github.com/longhorn/longhorn/issues/1355) The node the restore volume attached to is down</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/restore-volume-node-down/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/restore-volume-node-down/</guid>
      <description>&lt;h3 id=&#34;case-1&#34;&gt;Case 1:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a backup.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Restore the above backup.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Power off the volume attached node during the restoring.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Wait for the Longhorn node down.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Wait for the restore volume being reattached and starting restoring volume with state &lt;code&gt;Degraded&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Wait for the restore complete.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&#xA;During the restoration process, if the engine process fails to communicate with a replica, all replicas will be marked as &lt;code&gt;ERR&lt;/code&gt;, and the volume&amp;rsquo;s &lt;code&gt;RestoreRequired&lt;/code&gt; status cannot be set to &lt;code&gt;false&lt;/code&gt;. Longhorn relies on the &lt;code&gt;RestoreRequired&lt;/code&gt; value to determine the completion of the restoration and whether the failed replicas can be automatically salvaged.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Backing Image on a down node</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/backing-image-on-a-down-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/backing-image-on-a-down-node/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Update the settings:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Disable &lt;code&gt;Node Soft Anti-affinity&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Set &lt;code&gt;Replica Replenishment Wait Interval&lt;/code&gt; to a relatively long value.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create a backing image. Wait for the backing image being ready in the 1st disk.&lt;/li&gt;&#xA;&lt;li&gt;Create 2 volumes with the backing image and attach them on different nodes. Verify:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the disk state map of the backing image contains the disks of all replicas, and the state is running for all disks.&lt;/li&gt;&#xA;&lt;li&gt;the backing image content is correct.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Write random data to the volumes.&lt;/li&gt;&#xA;&lt;li&gt;Power off 2 nodes. One node should contain one volume engine. Verify that&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the related disk file state in the backing image will become &lt;code&gt;Unknown&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;the volume on the running node still works fine but is state &lt;code&gt;Degraded&lt;/code&gt;, and the content is correct.&lt;/li&gt;&#xA;&lt;li&gt;the volume on the down node become &lt;code&gt;Unknown&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Power on the node that contains one volume engine. Verify&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the failed replica of the &lt;code&gt;Degraded&lt;/code&gt; volume can be reused.&lt;/li&gt;&#xA;&lt;li&gt;the volume on the down node will be recovered automatically. And the data is correct.&lt;/li&gt;&#xA;&lt;li&gt;the backing image will be recovered automatically.&lt;/li&gt;&#xA;&lt;li&gt;the backing image file on this node will be reused when the related backing image manager pod is recovered (by check the pod log).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Delete all volumes and the backing image. Verify the backing image manager can be deleted once forcing removing the related terminating pod.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;available-test-backing-image-urls&#34;&gt;Available test backing image URLs:&lt;/h4&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&#xA;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.raw&#xA;https://cloud-images.ubuntu.com/minimal/releases/focal/release-20200729/ubuntu-20.04-minimal-cloudimg-amd64.img&#xA;https://github.com/rancher/k3os/releases/download/v0.11.0/k3os-amd64.iso &#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Node drain and deletion test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/node-drain-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/node-drain-deletion/</guid>
      <description>&lt;h2 id=&#34;drain-with-force&#34;&gt;Drain with force&lt;/h2&gt;&#xA;&lt;p&gt;Make sure the volumes on the drained/removed node can be detached or recovered correctly. The related issue: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1214&#34;&gt;https://github.com/longhorn/longhorn/issues/1214&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy a cluster contains 3 worker nodes N1, N2, N3.&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Create a 1-replica deployment with a 3-replica Longhorn volume. The volume is attached to N1.&lt;/li&gt;&#xA;&lt;li&gt;Write some data to the volume and get the md5sum.&lt;/li&gt;&#xA;&lt;li&gt;Force drain and remove N2, which contains one replica only.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl drain &amp;lt;Node name&amp;gt; --delete-emptydir-data=true --force=true --grace-period=-1 --ignore-daemonsets=true --timeout=&amp;lt;Desired timeout in secs&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Wait for the volume Degraded.&lt;/li&gt;&#xA;&lt;li&gt;Force drain and remove N1, which is the node the volume is attached to.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl drain &amp;lt;Node name&amp;gt; --delete-emptydir-data=true --force=true --grace-period=-1 --ignore-daemonsets=true --timeout=&amp;lt;Desired timeout in secs&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify the instance manager pods are gone and not recreated after the drain.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the volume detaching then being recovered. Will get attached to the workload/node.&lt;/li&gt;&#xA;&lt;li&gt;Validate the volume content. The data is intact.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;drain-without-force&#34;&gt;Drain without force&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Cordon the node. Longhorn will automatically disable the node scheduling when a Kubernetes node is cordoned.&lt;/li&gt;&#xA;&lt;li&gt;Evict all the replicas from the node.&lt;/li&gt;&#xA;&lt;li&gt;Run the following command to drain the node with &lt;code&gt;force&lt;/code&gt; flag set to false.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl drain &amp;lt;Node name&amp;gt; --delete-emptydir-data --force=false --grace-period=-1 --ignore-daemonsets=true --timeout=&amp;lt;Desired timeout in secs&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Observe that the workloads move to another node. The volumes should first detach and attach to workloads once they move to another node.&lt;/li&gt;&#xA;&lt;li&gt;Observe the logs, one by one all the pods should get evicted.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;node/&amp;lt;node-name&amp;gt; already cordoned&#xA;WARNING: ignoring DaemonSet-managed Pods: ingress-nginx/nginx-ingress-controller-bpf2t, kube-system/canal-hwk6v, longhorn-system/engine-image-ei-605a0f3e-8gb8l, longhorn-system/longhorn-csi-plugin-flq84, longhorn-system/longhorn-manager-tps6v&#xA;evicting pod longhorn-system/instance-manager-r-1aebab59&#xA;evicting pod kube-system/coredns-849545576b-v54vn&#xA;evicting pod longhorn-system/instance-manager-e-e591dbce&#xA;pod/instance-manager-r-1aebab59 evicted&#xA;pod/instance-manager-e-e591dbce evicted&#xA;pod/coredns-849545576b-v54vn evicted&#xA;node/&amp;lt;node-name&amp;gt; evicted&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify the instance manager pods are gone and not recreated after the drain.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Note: &lt;code&gt;--ignore-daemonsets&lt;/code&gt; should be set to true to ignore some DaemonSets that exist on node such as Longhorn manager, Longhorn CSI plugin, engine image in a Longhorn deployed cluster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Physical node down</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/physical-node-down/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/physical-node-down/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;One physical node down should result in the state of that node change to &lt;code&gt;Down&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;When using with CSI driver, one node with controller (StatefulSet/Deployment) and pod down should result in Kubernetes migrate the pod to another node, and Longhorn volume should be able to be used on that node as well. Test scenarios for this are documented &lt;a href=&#34;../../../node/improve-node-failure-handling/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this case, RWX should be excluded.&lt;/p&gt;&#xA;&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/5900#issuecomment-1541360552&#34;&gt;https://github.com/longhorn/longhorn/issues/5900#issuecomment-1541360552&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Single replica node down</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/single-replica-node-down/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/single-replica-node-down/</guid>
      <description>&lt;h2 id=&#34;related-issues&#34;&gt;Related Issues&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2329&#34;&gt;https://github.com/longhorn/longhorn/issues/2329&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2309&#34;&gt;https://github.com/longhorn/longhorn/issues/2309&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/3957&#34;&gt;https://github.com/longhorn/longhorn/issues/3957&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;default-setting&#34;&gt;Default Setting&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;Automatic salvage&lt;/code&gt; is enabled.&lt;/p&gt;&#xA;&lt;h2 id=&#34;node-restartdown-scenario-with-pod-deletion-policy-when-node-is-down-set-to-default-value-do-nothing&#34;&gt;Node restart/down scenario with &lt;code&gt;Pod Deletion Policy When Node is Down&lt;/code&gt; set to default value &lt;code&gt;do-nothing&lt;/code&gt;.&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create RWO|RWX volume with replica count = 1 &amp;amp; data locality = enabled|disabled|strict-local.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For data locality = strict-local, use RWO volume to do test.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create deployment|statefulset for volume.&lt;/li&gt;&#xA;&lt;li&gt;Power down node of volume/replica.&lt;/li&gt;&#xA;&lt;li&gt;The workload pod will get stuck in the &lt;code&gt;terminating&lt;/code&gt; state.&lt;/li&gt;&#xA;&lt;li&gt;Volume will fail to attach since volume is not ready (i.e remains faulted, since single replica is on downed node).&lt;/li&gt;&#xA;&lt;li&gt;Power up node or delete the workload pod so that kubernetes will recreate pod on another node.&lt;/li&gt;&#xA;&lt;li&gt;Verify auto salvage finishes (i.e pod completes start).&lt;/li&gt;&#xA;&lt;li&gt;Verify volume attached &amp;amp; accessible by pod (i.e test data is available).&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For data locality = strict-local volume, volume wiil keep in detaching, attaching status for about 10 minutes, after volume attached to node which replica located, check volume healthy and pod status.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;node-restartdown-scenario-with-pod-deletion-policy-when-node-is-down-set-to-delete-both-statefulset-and-deployment-pod&#34;&gt;Node restart/down scenario with &lt;code&gt;Pod Deletion Policy When Node is Down&lt;/code&gt; set to &lt;code&gt;delete-both-statefulset-and-deployment-pod&lt;/code&gt;&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create RWO|RWX volume with replica count = 1 &amp;amp; data locality = enabled|disabled|strict-local.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For data locality = strict-local, use RWO volume to do test.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create deployment|statefulset for volume.&lt;/li&gt;&#xA;&lt;li&gt;Power down node of volume/replica.&lt;/li&gt;&#xA;&lt;li&gt;Volume will become faulted.&lt;/li&gt;&#xA;&lt;li&gt;Wait for pod deletion &amp;amp; recreation on another node.&#xA;The pod recreation will not happen immediately.&lt;/li&gt;&#xA;&lt;li&gt;The replacement workload pod will get stuck in the &lt;code&gt;ContainerCreating&lt;/code&gt; state.&lt;/li&gt;&#xA;&lt;li&gt;Power on node of volume/replica.&lt;/li&gt;&#xA;&lt;li&gt;Verify the auto salvage finishes for volumes.&lt;/li&gt;&#xA;&lt;li&gt;Verify volume attached &amp;amp; accessible by pod (i.e test data is available).&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test node deletion</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/node-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/node-deletion/</guid>
      <description>&lt;h3 id=&#34;case-1-delete-multiple-kinds-of-nodes&#34;&gt;Case 1: Delete multiple kinds of nodes:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Shut down the VM for one node and wait for the node &lt;code&gt;Down&lt;/code&gt;. Disable another node.&lt;/li&gt;&#xA;&lt;li&gt;Delete the above 2 nodes. Make sure the corresponding Kubernetes node object is deleted. &amp;ndash;&amp;gt; The related Longhorn node objects will be cleaned up immediately, too.&lt;/li&gt;&#xA;&lt;li&gt;Add new nodes with the same names for the cluster. &amp;ndash;&amp;gt; The new nodes are available.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-2-delete-nodes-when-there-are-running-volumes&#34;&gt;Case 2: Delete nodes when there are running volumes:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;For each node that will be deleted later:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;create and attach 4 volumes:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The 1st volume contains 1 replica only. Both the engines and the replicas are on the pre-delete node. (Attached to the pre-delete node.)&lt;/li&gt;&#xA;&lt;li&gt;The 2nd volume contains 1 replica only. The engine is on the pre-delete node and the replica is on another node. (Attached to the pre-delete node.)&lt;/li&gt;&#xA;&lt;li&gt;The 3rd and the 4th volume contain 3 replicas. Both volumes are attached to a node except for the pre-delete node.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Write some data to all volumes.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Delete multiple nodes in the cluster simultaneously. Make sure the corresponding Kubernetes node object is deleted. &amp;ndash;&amp;gt; The related Longhorn node objects will be cleaned up immediately, too.&lt;/li&gt;&#xA;&lt;li&gt;For each deleted node:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Verify the volume Health state  &amp;ndash;&amp;gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The 1st volume should become &lt;code&gt;Faulted&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;The 2nd volume should keep &lt;code&gt;Unknown&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;The 3rd and the 4th volume should become &lt;code&gt;Degraded&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Delete the 1st volume. &amp;ndash;&amp;gt; The volume can be deleted.&lt;/li&gt;&#xA;&lt;li&gt;Detach then reattach the 2nd volume. &amp;ndash;&amp;gt; The volume works fine and the data is correct.&lt;/li&gt;&#xA;&lt;li&gt;Crash all replicas of the 3rd volume and trigger the auto salvage. &amp;ndash;&amp;gt; The auto salvage should work. The volume works fine and the data is correct after the salvage.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;An example to crash every volume replica instance manager pods with kubectl:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;kubectl delete pods -l longhorn.io/instance-manager-type=replica -n longhorn-system --wait&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Disabled the auto salvage setting. Then crash all replicas of the 3rd volume again. &amp;ndash;&amp;gt; The replica on the deleted node cannot be salvaged manually, too. The salvage feature still works fine.&lt;/li&gt;&#xA;&lt;li&gt;Deleted the replica on the deleted node for the 4th volume. &amp;ndash;&amp;gt; The replica can be deleted.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
  </channel>
</rss>
