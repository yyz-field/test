<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Node on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node/</link>
    <description>Recent content in Node on Longhorn Manual Test Cases</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://longhorn.github.io/longhorn-tests/manual/pre-release/node/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Degraded availability with added nodes</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node/degraded-availability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node/degraded-availability/</guid>
      <description>&lt;h4 id=&#34;volume-creation-using-ui-with-degraded-availability-and-added-node&#34;&gt;Volume creation using UI with degraded availability and added node&lt;/h4&gt;&#xA;&lt;h5 id=&#34;related-issue&#34;&gt;Related Issue:&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1701&#34;&gt;https://github.com/longhorn/longhorn/issues/1701&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;prerequisites&#34;&gt;Prerequisites:&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Start with 1 node cluster.&lt;/li&gt;&#xA;&lt;li&gt;Double check if &amp;ldquo;Allow Volume Creation with Degraded Availability&amp;rdquo; is ticked or return &lt;strong&gt;true&lt;/strong&gt; with following command:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;kubectl get settings.longhorn.io/allow-volume-creation-with-degraded-availability -n longhorn-system&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;steps&#34;&gt;Steps:&lt;/h5&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a Deployment Pod with a volume and three replicas.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;After the volume is attached, on Volume page it should be displayed as &lt;code&gt;Degraded&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Hover the crusor to the red circle exclamation mark, the tooltip will says, &amp;ldquo;The volume cannot be scheduled&amp;rdquo;.&lt;/li&gt;&#xA;&lt;li&gt;Click into the volume detail page it will display &lt;code&gt;Scheduling Failure&lt;/code&gt; but the volume remain fuctional as expected.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Write data to the Pod.&lt;/li&gt;&#xA;&lt;li&gt;Scale down the deployment to 0 to detach the volume.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Volume return to &lt;code&gt;Detached&lt;/code&gt; state.&lt;/li&gt;&#xA;&lt;li&gt;Both &lt;code&gt;Degraded&lt;/code&gt; and &lt;code&gt;Scheduling Failure&lt;/code&gt; should be gone.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Scale up the deployment back to 1 verify the data.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;code&gt;Scheduling Failure&lt;/code&gt; should be seen again from the UI.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Add another node to the cluster.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Volume should start rebuilding on the second node soon.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Once the rebuild completed, scale down and back up the deployment to verify the data.&lt;/li&gt;&#xA;&lt;li&gt;And the third node to the cluster.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Volume should start rebuilding on the third node soon.&lt;/li&gt;&#xA;&lt;li&gt;Once the rebuilding starts, the &lt;code&gt;Scheduling Failure&lt;/code&gt; should be gone.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Scale down and back the deployment to verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Improve Node Failure Handling By Automatically Force Delete Terminating Pods of StatefulSet/Deployment On Downed Node</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node/improve-node-failure-handling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node/improve-node-failure-handling/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Setup a cluster of 3 worker nodes&lt;/li&gt;&#xA;&lt;li&gt;Install Longhorn and set &lt;code&gt;Default Replica Count = 2&lt;/code&gt; (because we will turn off one node)&lt;/li&gt;&#xA;&lt;li&gt;Create a StatefulSet with 2 pods using the command:&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl create -f https://raw.githubusercontent.com/longhorn/longhorn/master/examples/statefulset.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a volume + pv + pvc named &lt;code&gt;vol1&lt;/code&gt; and create a deployment(1 pod) of default ubuntu named &lt;code&gt;shell&lt;/code&gt; with the usage of pvc &lt;code&gt;vol1&lt;/code&gt; mounted under &lt;code&gt;/mnt/vol1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Find the node which contains one pod of the StatefulSet/Deployment. Power off the node&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;statefulset&#34;&gt;StatefulSet&lt;/h4&gt;&#xA;&lt;h5 id=&#34;if-nodedownpoddeletionpolicy--is-set-to-do-nothing---delete-deployment-pod&#34;&gt;if &lt;code&gt;NodeDownPodDeletionPolicy &lt;/code&gt; is set to &lt;code&gt;do-nothing &lt;/code&gt; | &lt;code&gt;delete-deployment-pod&lt;/code&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;wait till the &lt;code&gt;pod.deletionTimestamp&lt;/code&gt; has passed&lt;/li&gt;&#xA;&lt;li&gt;verify no replacement pod generated, the pod is stuck at terminating forever.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;if-nodedownpoddeletionpolicy--is-set-to-delete-statefulset-pod---delete-both-statefulset-and-deployment-pod&#34;&gt;if &lt;code&gt;NodeDownPodDeletionPolicy &lt;/code&gt; is set to &lt;code&gt;delete-statefulset-pod &lt;/code&gt; | &lt;code&gt;delete-both-statefulset-and-deployment-pod&lt;/code&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;wait till pod&amp;rsquo;s status becomes &lt;code&gt;terminating&lt;/code&gt; and the &lt;code&gt;pod.deletionTimestamp&lt;/code&gt; has passed (around 7 minutes)&lt;/li&gt;&#xA;&lt;li&gt;verify that the pod is deleted and there is a new running replacement pod.&lt;/li&gt;&#xA;&lt;li&gt;Verify that you can access/read/write the volume on the new pod&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;deployment&#34;&gt;Deployment&lt;/h4&gt;&#xA;&lt;h5 id=&#34;if-nodedownpoddeletionpolicy--is-set-to-do-nothing---delete-statefulset-pod&#34;&gt;if &lt;code&gt;NodeDownPodDeletionPolicy &lt;/code&gt; is set to &lt;code&gt;do-nothing &lt;/code&gt; | &lt;code&gt;delete-statefulset-pod&lt;/code&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;wait till the &lt;code&gt;pod.deletionTimestamp&lt;/code&gt; has passed&lt;/li&gt;&#xA;&lt;li&gt;replacement pod will be stuck in &lt;code&gt;Pending&lt;/code&gt; state forever&lt;/li&gt;&#xA;&lt;li&gt;force delete the terminating pod&lt;/li&gt;&#xA;&lt;li&gt;wait till replacement pod is running&lt;/li&gt;&#xA;&lt;li&gt;verify that you can access &lt;code&gt;vol1&lt;/code&gt; via the &lt;code&gt;shell&lt;/code&gt; replacement pod under &lt;code&gt;/mnt/vol1&lt;/code&gt; once it is in the running state&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;if-nodedownpoddeletionpolicy--is-set-to-delete-deployment-pod---delete-both-statefulset-and-deployment-pod&#34;&gt;if &lt;code&gt;NodeDownPodDeletionPolicy &lt;/code&gt; is set to &lt;code&gt;delete-deployment-pod &lt;/code&gt; | &lt;code&gt;delete-both-statefulset-and-deployment-pod&lt;/code&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;wait till the &lt;code&gt;pod.deletionTimestamp&lt;/code&gt; has passed&lt;/li&gt;&#xA;&lt;li&gt;verify that the pod is deleted and there is a new running replacement pod.&lt;/li&gt;&#xA;&lt;li&gt;verify that you can access &lt;code&gt;vol1&lt;/code&gt; via the &lt;code&gt;shell&lt;/code&gt; replacement pod under &lt;code&gt;/mnt/vol1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;other-kinds&#34;&gt;Other kinds&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Verify that Longhorn never deletes any other pod on the downed node.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;test-example&#34;&gt;Test example&lt;/h4&gt;&#xA;&lt;p&gt;One typical scenario when the enhancement has succeeded is as below. When a node (say &lt;code&gt;node-x&lt;/code&gt;) goes down (assume using Kubernetes&amp;rsquo; default settings and user allows Longhorn to force delete pods):&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
