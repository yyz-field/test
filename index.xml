<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Longhorn Test Cases on Longhorn Manual Test Cases</title>
    <link>https://longhorn.github.io/longhorn-tests/</link>
    <description>Recent content in Longhorn Test Cases on Longhorn Manual Test Cases</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://longhorn.github.io/longhorn-tests/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://longhorn.github.io/longhorn-tests/integration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/integration/</guid>
      <description>&lt;!doctype html&gt;&#xA;&lt;html lang=&#34;en&#34;&gt;&#xA;&lt;head&gt;&#xA;&lt;meta charset=&#34;utf-8&#34;&gt;&#xA;&lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1, minimum-scale=1&#34;&gt;&#xA;&lt;meta name=&#34;generator&#34; content=&#34;pdoc3 0.11.0&#34;&gt;&#xA;&lt;title&gt;tests API documentation&lt;/title&gt;&#xA;&lt;meta name=&#34;description&#34; content=&#34;&#34;&gt;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css&#34; integrity=&#34;sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==&#34; crossorigin&gt;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css&#34; integrity=&#34;sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==&#34; crossorigin&gt;&#xA;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css&#34; crossorigin&gt;&#xA;&lt;style&gt;:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar &gt; *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^=&#34;header-&#34;]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:&#34;DejaVu Sans Mono&#34;,monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl &gt; dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name &gt; span:first-child{white-space:nowrap}.name.class &gt; span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary &gt; *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:&#39;,\2002&#39;}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}&lt;/style&gt;&#xA;&lt;style media=&#34;screen and (min-width: 700px)&#34;&gt;@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc &gt; ul &gt; li{margin-top:.5em}}&lt;/style&gt;&#xA;&lt;style media=&#34;print&#34;&gt;@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:&#34; (&#34; attr(href) &#34;)&#34;;font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:&#34; (&#34; attr(title) &#34;)&#34;}.ir a:after,a[href^=&#34;javascript:&#34;]:after,a[href^=&#34;#&#34;]:after{content:&#34;&#34;}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}&lt;/style&gt;&#xA;&lt;script defer src=&#34;https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js&#34; integrity=&#34;sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==&#34; crossorigin&gt;&lt;/script&gt;&#xA;&lt;script&gt;window.addEventListener(&#39;DOMContentLoaded&#39;, () =&gt; {&#xA;hljs.configure({languages: [&#39;bash&#39;, &#39;css&#39;, &#39;diff&#39;, &#39;graphql&#39;, &#39;ini&#39;, &#39;javascript&#39;, &#39;json&#39;, &#39;plaintext&#39;, &#39;python&#39;, &#39;python-repl&#39;, &#39;rust&#39;, &#39;shell&#39;, &#39;sql&#39;, &#39;typescript&#39;, &#39;xml&#39;, &#39;yaml&#39;]});&#xA;hljs.highlightAll();&#xA;})&lt;/script&gt;&#xA;&lt;/head&gt;&#xA;&lt;body&gt;&#xA;&lt;main&gt;&#xA;&lt;article id=&#34;content&#34;&gt;&#xA;&lt;header&gt;&#xA;&lt;h1 class=&#34;title&#34;&gt;Namespace &lt;code&gt;tests&lt;/code&gt;&lt;/h1&gt;&#xA;&lt;/header&gt;&#xA;&lt;section id=&#34;section-intro&#34;&gt;&#xA;&lt;/section&gt;&#xA;&lt;section&gt;&#xA;&lt;/section&gt;&#xA;&lt;section&gt;&#xA;&lt;/section&gt;&#xA;&lt;section&gt;&#xA;&lt;/section&gt;&#xA;&lt;section&gt;&#xA;&lt;/section&gt;&#xA;&lt;/article&gt;&#xA;&lt;nav id=&#34;sidebar&#34;&gt;&#xA;&lt;div class=&#34;toc&#34;&gt;&#xA;&lt;ul&gt;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;ul id=&#34;index&#34;&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/nav&gt;&#xA;&lt;/main&gt;&#xA;&lt;footer id=&#34;footer&#34;&gt;&#xA;&lt;p&gt;Generated by &lt;a href=&#34;https://pdoc3.github.io/pdoc&#34; title=&#34;pdoc: Python API documentation generator&#34;&gt;&lt;cite&gt;pdoc&lt;/cite&gt; 0.11.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-upgrade-responder-collectiing-average-sizes-for-v1-volumes-only/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-upgrade-responder-collectiing-average-sizes-for-v1-volumes-only/</guid>
      <description>&lt;h2 id=&#34;related-issues&#34;&gt;Related issues&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/7380&#34;&gt;https://github.com/longhorn/longhorn/issues/7380&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;test-step&#34;&gt;Test step&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Patch build and deploy Longhorn.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;diff --git a/controller/setting_controller.go b/controller/setting_controller.go&#xA;index de77b7246..ac6263ac5 100644&#xA;--- a/controller/setting_controller.go&#xA;+++ b/controller/setting_controller.go&#xA;@@ -49,7 +49,7 @@ const (&#xA; var (&#xA; &#x9;upgradeCheckInterval          = time.Hour&#xA; &#x9;settingControllerResyncPeriod = time.Hour&#xA;-&#x9;checkUpgradeURL               = &amp;#34;https://longhorn-upgrade-responder.rancher.io/v1/checkupgrade&amp;#34;&#xA;+&#x9;checkUpgradeURL               = &amp;#34;http://longhorn-upgrade-responder.default.svc.cluster.local:8314/v1/checkupgrade&amp;#34;&#xA; )&#xA;&#xA; type SettingController struct {&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;&#xA;&lt;p&gt;Match the checkUpgradeURL with the application name: &lt;code&gt;http://&amp;lt;APP_NAME&amp;gt;-upgrade-responder.default.svc.cluster.local:8314/v1/checkupgrade&lt;/code&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;And&lt;/strong&gt; setting &lt;code&gt;v2-data-engine&lt;/code&gt; value is &lt;code&gt;true&lt;/code&gt;.&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; add a block disk to cluster nodes.&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; &lt;a href=&#34;https://github.com/longhorn/longhorn/tree/master/dev/upgrade-responder&#34;&gt;deploy upgrade responder stack&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[#1279](https://github.com/longhorn/longhorn/issues/1279) DR volume live upgrade and rebuild</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/backup-and-restore/dr-volume-live-upgrade-and-rebuild/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/backup-and-restore/dr-volume-live-upgrade-and-rebuild/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Launch Longhorn at the previous version.&lt;/li&gt;&#xA;&lt;li&gt;Launch a pod with Longhorn volume.&lt;/li&gt;&#xA;&lt;li&gt;Write data to the volume and take the 1st backup.&lt;/li&gt;&#xA;&lt;li&gt;Create 2 DR volumes from the 1st backup.&lt;/li&gt;&#xA;&lt;li&gt;Shutdown the pod and wait for the original volume detached.&lt;/li&gt;&#xA;&lt;li&gt;Expand the original volume and wait for the expansion complete.&lt;/li&gt;&#xA;&lt;li&gt;Write data to the original volume and take the 2nd backup. (Make sure the total data size is larger than the original volume size so that there is date written to the expanded part.)&lt;/li&gt;&#xA;&lt;li&gt;Trigger incremental restore for the DR volumes by listing the backup volumes, and wait for restore complete.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to the latest version.&lt;/li&gt;&#xA;&lt;li&gt;Crash one random replica for the 1st DR volume .&lt;/li&gt;&#xA;&lt;li&gt;Verify the 1st DR volume rebuild replicas and state change from &lt;code&gt;Degraded&lt;/code&gt; to &lt;code&gt;Healthy&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Write data to the original volume and take the 3rd backup.&lt;/li&gt;&#xA;&lt;li&gt;Trigger incremental restore for the DR volumes, and wait for restore complete.&lt;/li&gt;&#xA;&lt;li&gt;Do live upgrade for the 1st DR volume. This live upgrade call should success.&lt;/li&gt;&#xA;&lt;li&gt;Activate the 1st DR volume.&lt;/li&gt;&#xA;&lt;li&gt;Launch a pod for the 1st activated volume, and verify the restored data is correct.&lt;/li&gt;&#xA;&lt;li&gt;Do live upgrade for the original volume and the 2nd DR volumes.&lt;/li&gt;&#xA;&lt;li&gt;Crash one random replica for the 2nd DR volume.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the restore &amp;amp; rebuild complete.&lt;/li&gt;&#xA;&lt;li&gt;Delete one replica for the 2nd DR volume, then activate the DR volume before the rebuild complete.&lt;/li&gt;&#xA;&lt;li&gt;Verify the DR volume will be auto detached after the rebuild complete.&lt;/li&gt;&#xA;&lt;li&gt;Launch a pod for the 2nd activated volume, and verify the restored data is correct.&lt;/li&gt;&#xA;&lt;li&gt;Crash one replica for the 2nd activated volume.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the rebuild complete, then verify the volume still works fine by reading/writing more data.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>[#1341](https://github.com/longhorn/longhorn/issues/1341) concurrent backup test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/backup-and-restore/concurrent-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/backup-and-restore/concurrent-backup/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Take a manual backup of the volume &lt;code&gt;bak&lt;/code&gt; while a recurring backup is running&lt;/li&gt;&#xA;&lt;li&gt;verify that backup got created&lt;/li&gt;&#xA;&lt;li&gt;verify that backup sticks around even when recurring backups are cleaned up&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>[#1355](https://github.com/longhorn/longhorn/issues/1355) The node the restore volume attached to is down</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/restore-volume-node-down/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/restore-volume-node-down/</guid>
      <description>&lt;h3 id=&#34;case-1&#34;&gt;Case 1:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a backup.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Restore the above backup.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Power off the volume attached node during the restoring.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Wait for the Longhorn node down.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Wait for the restore volume being reattached and starting restoring volume with state &lt;code&gt;Degraded&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Wait for the restore complete.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&#xA;During the restoration process, if the engine process fails to communicate with a replica, all replicas will be marked as &lt;code&gt;ERR&lt;/code&gt;, and the volume&amp;rsquo;s &lt;code&gt;RestoreRequired&lt;/code&gt; status cannot be set to &lt;code&gt;false&lt;/code&gt;. Longhorn relies on the &lt;code&gt;RestoreRequired&lt;/code&gt; value to determine the completion of the restoration and whether the failed replicas can be automatically salvaged.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[#1366](https://github.com/longhorn/longhorn/issues/1366) &amp;&amp; [#1328](https://github.com/longhorn/longhorn/issues/1328) The node the DR volume attached to is rebooted</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-restart/dr-volume-node-rebooted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-restart/dr-volume-node-rebooted/</guid>
      <description>&lt;h4 id=&#34;scenario-1&#34;&gt;Scenario 1&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a pod with Longhorn volume.&lt;/li&gt;&#xA;&lt;li&gt;Write data to the volume and get the md5sum.&lt;/li&gt;&#xA;&lt;li&gt;Create the 1st backup for the volume.&lt;/li&gt;&#xA;&lt;li&gt;Create a DR volume from the backup.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the DR volume starting the initial restore. Then reboot the DR volume attached node immediately.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the DR volume detached then reattached.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the DR volume restore complete after the reattachment.&lt;/li&gt;&#xA;&lt;li&gt;Activate the DR volume and check the data md5sum.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;scenario-2&#34;&gt;Scenario 2&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a pod with Longhorn volume.&lt;/li&gt;&#xA;&lt;li&gt;Write data to the volume and get the md5sum.&lt;/li&gt;&#xA;&lt;li&gt;Create the 1st backup for the volume.&lt;/li&gt;&#xA;&lt;li&gt;Create a DR volume from the backup.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the DR volume to complete the initial restore.&lt;/li&gt;&#xA;&lt;li&gt;Write more data to the original volume and get the md5sum.&lt;/li&gt;&#xA;&lt;li&gt;Create the 2nd backup for the volume.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the DR volume incremental restore getting triggered. Then reboot the DR volume attached node immediately.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the DR volume detached then reattached.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the DR volume restore complete after the reattachment.&lt;/li&gt;&#xA;&lt;li&gt;Activate the DR volume and check the data md5sum.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>[#1404](https://github.com/longhorn/longhorn/issues/1404) test backup functionality on google cloud and other s3 interop providers.</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/backup-and-restore/google-cloud-s3-interop-backups/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/backup-and-restore/google-cloud-s3-interop-backups/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;create vol &lt;code&gt;s3-test&lt;/code&gt;and mount to a node on &lt;code&gt;/mnt/s3-test&lt;/code&gt; via pvc&lt;/li&gt;&#xA;&lt;li&gt;write some data on vol &lt;code&gt;s3-test&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;take backup(1)&lt;/li&gt;&#xA;&lt;li&gt;write new data on vol &lt;code&gt;s3-test&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;take backup(2)&lt;/li&gt;&#xA;&lt;li&gt;restore backup(1)&lt;/li&gt;&#xA;&lt;li&gt;verify data is consistent with backup(1)&lt;/li&gt;&#xA;&lt;li&gt;restore backup(2)&lt;/li&gt;&#xA;&lt;li&gt;verify data is consistent with backup(2)&lt;/li&gt;&#xA;&lt;li&gt;delete backup(1)&lt;/li&gt;&#xA;&lt;li&gt;delete backup(2)&lt;/li&gt;&#xA;&lt;li&gt;delete backup volume &lt;code&gt;s3-test&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;verify volume path is removed&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>[#2206](https://github.com/longhorn/longhorn/issues/2206) Fix the spinning disk on Longhorn</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/resiliency/simulated-slow-disk/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/resiliency/simulated-slow-disk/</guid>
      <description>&lt;p&gt;This case requires the creation of a slow virtual disk with &lt;code&gt;dmsetup&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Make a slow disk:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Make a disk image file: &lt;code&gt;truncate -s 10g slow.img&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a loopback device: &lt;code&gt;losetup --show -P -f slow.img&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Get the block size of the loopback device: &lt;code&gt;blockdev --getsize /dev/loopX&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create slow device: &lt;code&gt;echo &amp;quot;0 &amp;lt;blocksize&amp;gt; delay /dev/loopX 0 500&amp;quot; | dmsetup create dm-slow&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Format slow device: &lt;code&gt;mkfs.ext4 /dev/mapper/dm-slow&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Mount slow device: &lt;code&gt;mount /dev/mapper/dm-slow /mnt&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Build longhorn-engine and run it on the slow disk.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[#4637](https://github.com/longhorn/longhorn/issues/4637) pull backup created by another Longhorn system</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/backup-and-restore/pull-backup-created-by-another-longhorn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/backup-and-restore/pull-backup-created-by-another-longhorn/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Prepare 2 k8s clusters: cluster A and cluster B.&lt;/li&gt;&#xA;&lt;li&gt;Install previous version of Longhorn which doesn&amp;rsquo;t include this fix e.g v1.3.1, v1.2.5 on cluster A.&lt;/li&gt;&#xA;&lt;li&gt;Install the release version of Longhorn on cluster B.&lt;/li&gt;&#xA;&lt;li&gt;Set the same backup target on both cluster A and cluster B.&lt;/li&gt;&#xA;&lt;li&gt;Create volume, write some data, and take backup on cluster A.&lt;/li&gt;&#xA;&lt;li&gt;Wait for backup target polling update on cluster B.&lt;/li&gt;&#xA;&lt;li&gt;Make sure the backup created by cluster A can be pulled on cluster B.&lt;/li&gt;&#xA;&lt;li&gt;Restore the pulled backup and verify the data on cluster B.&lt;/li&gt;&#xA;&lt;li&gt;Repeat the test with both clusters installed the release version of Longhorn.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>[Add Extra Volume](https://longhorn.io/docs/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks/#create-additional-volume)</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/eks/add-extra-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/eks/add-extra-volume/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create EKS cluster with 3 nodes and install Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;a href=&#34;https://github.com/longhorn/longhorn/blob/master/examples/deployment.yaml&#34;&gt;deployment&lt;/a&gt; and write some data to it.&lt;/li&gt;&#xA;&lt;li&gt;In Longhorn, set &lt;code&gt;replica-replenishment-wait-interval&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab &lt;code&gt;Configuration/Compute/&amp;lt;node-group-name&amp;gt;&lt;/code&gt; and click the launch template.&lt;/li&gt;&#xA;&lt;li&gt;Click &lt;code&gt;Modify template (Create new version)&lt;/code&gt; in the &lt;code&gt;Actions&lt;/code&gt; drop-down menu.&lt;/li&gt;&#xA;&lt;li&gt;Choose the &lt;code&gt;Source template version&lt;/code&gt; in the &lt;code&gt;Launch template name and version description&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Click &lt;code&gt;Advanced&lt;/code&gt; then &lt;code&gt;Add new volume&lt;/code&gt; in &lt;code&gt;Configure storage&lt;/code&gt; and fill in the fields.&lt;/li&gt;&#xA;&lt;li&gt;Adjust the auto-mount script and add to &lt;code&gt;User data&lt;/code&gt; in &lt;code&gt;Advanced details&lt;/code&gt;. Make sure the &lt;code&gt;DEV_PATH&lt;/code&gt; matches the &lt;code&gt;Device name&lt;/code&gt; of the additional volume.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;MIME-Version: 1.0&#xA;Content-Type: multipart/mixed; boundary=&amp;#34;==MYBOUNDARY==&amp;#34;&#xA;--==MYBOUNDARY==&#xA;Content-Type: text/x-shellscript; charset=&amp;#34;us-ascii&amp;#34;&#xA;#!/bin/bash&#xA;# https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html#launch-template-user-data&#xA;echo &amp;#34;Running custom user data script&amp;#34;&#xA;DEV_PATH=&amp;#34;/dev/sdb&amp;#34;&#xA;mkfs -t ext4 ${DEV_PATH}&#xA;MOUNT_PATH=&amp;#34;/mnt/longhorn&amp;#34;&#xA;mkdir ${MOUNT_PATH}&#xA;mount ${DEV_PATH} ${MOUNT_PATH}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Click &lt;code&gt;Create template version&lt;/code&gt; to save changes.&lt;/li&gt;&#xA;&lt;li&gt;Go to the EKS cluster node-group and change &lt;code&gt;Launch template version&lt;/code&gt; in &lt;code&gt;Node Group configuration&lt;/code&gt;. Track the status in the &lt;code&gt;Update history&lt;/code&gt; tab.&lt;/li&gt;&#xA;&lt;li&gt;After update succeeded, check the deployment in step 2 still running and data exist, and check the extra volume can be added through Longhorn UI.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>[Expand Volume](https://longhorn.io/docs/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-aks/)</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/aks/expand-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/aks/expand-volume/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create AKS cluster with 3 nodes and install Longhorn.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create &lt;a href=&#34;https://github.com/longhorn/longhorn/blob/master/examples/deployment.yaml&#34;&gt;deployment&lt;/a&gt; and write some data to it.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;In Longhorn, set &lt;code&gt;replica-replenishment-wait-interval&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Add a new node-pool. Later Longhorn components will be automatically deployed on the nodes in this pool.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;AKS_NODEPOOL_NAME_NEW=&amp;lt;new-nodepool-name&amp;gt;&#xA;AKS_RESOURCE_GROUP=&amp;lt;aks-resource-group&amp;gt;&#xA;AKS_CLUSTER_NAME=&amp;lt;aks-cluster-name&amp;gt;&#xA;AKS_DISK_SIZE_NEW=&amp;lt;new-disk-size-in-gb&amp;gt;&#xA;AKS_NODE_NUM=&amp;lt;number-of-nodes&amp;gt;&#xA;AKS_K8S_VERSION=&amp;lt;kubernetes-version&amp;gt;&#xA;az aks nodepool add \&#xA;  --resource-group ${AKS_RESOURCE_GROUP} \&#xA;  --cluster-name ${AKS_CLUSTER_NAME} \&#xA;  --name ${AKS_NODEPOOL_NAME_NEW} \&#xA;  --node-count ${AKS_NODE_NUM} \&#xA;  --node-osdisk-size ${AKS_DISK_SIZE_NEW} \&#xA;  --kubernetes-version ${AKS_K8S_VERSION} \&#xA;  --mode System&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Using Longhorn UI to disable the disk scheduling and request eviction for nodes in the old node-pool.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Expand Volume](https://longhorn.io/docs/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-eks/#storage-expansion)</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/eks/expand-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/eks/expand-volume/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create EKS cluster with 3 nodes and install Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;a href=&#34;https://github.com/longhorn/longhorn/blob/master/examples/deployment.yaml&#34;&gt;deployment&lt;/a&gt; and write some data to it.&lt;/li&gt;&#xA;&lt;li&gt;In Longhorn, set &lt;code&gt;replica-replenishment-wait-interval&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Go to the launch template of the EKS cluster node-group. You can find in the EKS cluster tab &lt;code&gt;Configuration/Compute/&amp;lt;node-group-name&amp;gt;&lt;/code&gt; and click the launch template.&lt;/li&gt;&#xA;&lt;li&gt;Click &lt;code&gt;Modify template (Create new version)&lt;/code&gt; in the &lt;code&gt;Actions&lt;/code&gt; drop-down menu.&lt;/li&gt;&#xA;&lt;li&gt;Choose the &lt;code&gt;Source template version&lt;/code&gt; in the &lt;code&gt;Launch template name and version description&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Update the volume size in &lt;code&gt;Configure storage&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Click &lt;code&gt;Create template version&lt;/code&gt; to save changes.&lt;/li&gt;&#xA;&lt;li&gt;Go to the EKS cluster node-group and change &lt;code&gt;Launch template version&lt;/code&gt; in &lt;code&gt;Node Group configuration&lt;/code&gt;. Track the status in the &lt;code&gt;Update history&lt;/code&gt; tab.&lt;/li&gt;&#xA;&lt;li&gt;After update succeeded, check the deployment in step 2 still running and data exist, and check volume expanded as expected through Longhorn UI.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>[Expand Volume](https://longhorn.io/docs/1.3.0/advanced-resources/support-managed-k8s-service/manage-node-group-on-gke/)</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/gke/expand-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/gke/expand-volume/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create GKE cluster with 3 nodes and install Longhorn.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create &lt;a href=&#34;https://github.com/longhorn/longhorn/blob/master/examples/deployment.yaml&#34;&gt;deployment&lt;/a&gt; and write some data to it.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;In Longhorn, set &lt;code&gt;replica-replenishment-wait-interval&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Add a new node-pool. Later Longhorn components will be automatically deployed on the nodes in this pool.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;GKE_NODEPOOL_NAME_NEW=&amp;lt;new-nodepool-name&amp;gt;&#xA;GKE_REGION=&amp;lt;gke-region&amp;gt;&#xA;GKE_CLUSTER_NAME=&amp;lt;gke-cluster-name&amp;gt;&#xA;GKE_IMAGE_TYPE=Ubuntu&#xA;GKE_MACHINE_TYPE=&amp;lt;gcp-machine-type&amp;gt;&#xA;GKE_DISK_SIZE_NEW=&amp;lt;new-disk-size-in-gb&amp;gt;&#xA;GKE_NODE_NUM=&amp;lt;number-of-nodes&amp;gt;&#xA;gcloud container node-pools create ${GKE_NODEPOOL_NAME_NEW} \&#xA;  --region ${GKE_REGION} \&#xA;  --cluster ${GKE_CLUSTER_NAME} \&#xA;  --image-type ${GKE_IMAGE_TYPE} \&#xA;  --machine-type ${GKE_MACHINE_TYPE} \&#xA;  --disk-size ${GKE_DISK_SIZE_NEW} \&#xA;  --num-nodes ${GKE_NODE_NUM}&#xA;&#xA;gcloud container node-pools list \&#xA;  --zone ${GKE_REGION} \&#xA;  --cluster ${GKE_CLUSTER_NAME} &#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Using Longhorn UI to disable the disk scheduling and request eviction for nodes in the old node-pool.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Upgrade K8s](https://longhorn.io/docs/1.3.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-aks/)</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/aks/upgrade-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/aks/upgrade-k8s/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create AKS cluster with 3 nodes and install Longhorn.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create &lt;a href=&#34;https://github.com/longhorn/longhorn/blob/master/examples/deployment.yaml&#34;&gt;deployment&lt;/a&gt; and write some data to it.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;In Longhorn, set &lt;code&gt;replica-replenishment-wait-interval&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Upgrade AKS control plane.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;AKS_RESOURCE_GROUP=&amp;lt;aks-resource-group&amp;gt;&#xA;AKS_CLUSTER_NAME=&amp;lt;aks-cluster-name&amp;gt;&#xA;AKS_K8S_VERSION_UPGRADE=&amp;lt;aks-k8s-version&amp;gt;&#xA;az aks upgrade \&#xA;    --resource-group ${AKS_RESOURCE_GROUP} \&#xA;    --name ${AKS_CLUSTER_NAME} \&#xA;    --kubernetes-version ${AKS_K8S_VERSION_UPGRADE} \&#xA;    --control-plane-only&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Add a new node-pool.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;AKS_NODEPOOL_NAME_NEW=&amp;lt;new-nodepool-name&amp;gt;&#xA;AKS_DISK_SIZE=&amp;lt;disk-size-in-gb&amp;gt;&#xA;AKS_NODE_NUM=&amp;lt;number-of-nodes&amp;gt;&#xA;az aks nodepool add \&#xA;  --resource-group ${AKS_RESOURCE_GROUP} \&#xA;  --cluster-name ${AKS_CLUSTER_NAME} \&#xA;  --name ${AKS_NODEPOOL_NAME_NEW} \&#xA;  --node-count ${AKS_NODE_NUM} \&#xA;  --node-osdisk-size ${AKS_DISK_SIZE} \&#xA;  --kubernetes-version ${AKS_K8S_VERSION_UPGRADE} \&#xA;  --mode System&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Using Longhorn UI to disable the disk scheduling and request eviction for nodes in the old node-pool.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Upgrade K8s](https://longhorn.io/docs/1.3.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-eks/)</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/eks/upgrade-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/eks/upgrade-k8s/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create EKS cluster with 3 nodes and install Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;a href=&#34;https://github.com/longhorn/longhorn/blob/master/examples/deployment.yaml&#34;&gt;deployment&lt;/a&gt; and write some data to it.&lt;/li&gt;&#xA;&lt;li&gt;In Longhorn, set &lt;code&gt;replica-replenishment-wait-interval&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Following &lt;a href=&#34;https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html&#34;&gt;instructions&lt;/a&gt; to upgrade the cluster.&lt;/li&gt;&#xA;&lt;li&gt;Check the deployment in step 2 still running and data exist.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>[Upgrade K8s](https://longhorn.io/docs/1.3.0/advanced-resources/support-managed-k8s-service/upgrade-k8s-on-gke/)</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/gke/upgrade-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/managed-kubernetes-clusters/gke/upgrade-k8s/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create GKE cluster with 3 nodes and install Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Create &lt;a href=&#34;https://github.com/longhorn/longhorn/blob/master/examples/deployment.yaml&#34;&gt;deployment&lt;/a&gt; and write some data to it.&lt;/li&gt;&#xA;&lt;li&gt;In Longhorn, set &lt;code&gt;replica-replenishment-wait-interval&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;See &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrading_the_cluster&#34;&gt;Upgrading the cluster&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrading-nodes&#34;&gt;Upgrading node pools&lt;/a&gt; for instructions.&lt;/li&gt;&#xA;&lt;li&gt;Check the deployment in step 2 still running and data exist.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>1. Deployment of Longhorn</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/deployment/</guid>
      <description>&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;&#xA;&lt;p&gt;Longhorn v1.1.2 and above - Support Kubernetes 1.18+&lt;/p&gt;&#xA;&lt;p&gt;Longhorn v1.0.0 to v1.1.1 - Support Kubernetes 1.14+. Default 1.16+&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Install using Rancher Apps &amp;amp; MarketPlace App (Default)&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Install using Helm chart from &lt;a href=&#34;https://github.com/longhorn/longhorn/tree/master/chart&#34;&gt;https://github.com/longhorn/longhorn/tree/master/chart&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Install using YAML from &lt;a href=&#34;https://github.com/longhorn/longhorn/blob/master/deploy/longhorn.yaml&#34;&gt;https://github.com/longhorn/longhorn/blob/master/deploy/longhorn.yaml&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Note: Longhorn UI can scale to multiple instances for HA purposes.&lt;/p&gt;&#xA;&lt;h2 id=&#34;uninstallation&#34;&gt;Uninstallation&lt;/h2&gt;&#xA;&lt;p&gt;Make sure all the CRDs and other resources are cleaned up, following the uninstallation instruction.&#xA;&lt;a href=&#34;https://longhorn.io/docs/1.2.2/deploy/uninstall/&#34;&gt;https://longhorn.io/docs/1.2.2/deploy/uninstall/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;customizable-default-settings&#34;&gt;Customizable Default Settings&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://longhorn.io/docs/1.2.2/references/settings/&#34;&gt;https://longhorn.io/docs/1.2.2/references/settings/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Make sure the settings are updated if it’s the fresh installation of Longhorn.&lt;/p&gt;</description>
    </item>
    <item>
      <title>2. UI</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/ui/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/ui/</guid>
      <description>&lt;h2 id=&#34;accessibility-of-longhorn-ui&#34;&gt;Accessibility of Longhorn UI&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1.&lt;/td&gt;&#xA;          &lt;td&gt;Access Longhorn UI using rancher proxy&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a cluster (3 worker nodes and 1 etcd/control plane) in rancher, Go to the default project.&lt;br&gt;2.  Go to App, Click the launch app.&lt;br&gt;3.  Select longhorn.&lt;br&gt;4.  Select &lt;code&gt;Rancher-Proxy&lt;/code&gt; under the Longhorn UI service.&lt;br&gt;5.  Once the app is deployed successfully, click the &lt;a href=&#34;https://173.255.255.35/k8s/clusters/c-qnl4b/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/&#34;&gt;/index.html&lt;/a&gt; link appears in App page.&lt;br&gt;6.  The page should redirect to longhorn UI - &lt;a href=&#34;https://173.255.255.35/k8s/clusters/c-qnl4b/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/#/dashboard&#34;&gt;https://rancher/k8s/clusters/c-aaaa/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/#/dashboard&lt;/a&gt;&lt;br&gt;7.  Verify all the pages, refresh each page and verify. Create a volume and check the volume detail page also.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2.&lt;/td&gt;&#xA;          &lt;td&gt;Access Longhorn UI under Kubectl proxy&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a cluster (3 worker nodes and 1 etcd/control plane) using rke.&lt;br&gt;2.  Start kubectl proxy by command &lt;code&gt;kubectl proxy&lt;/code&gt;.&lt;br&gt;3.  It should start proxy locally on 8001 port.&lt;br&gt;4.  Navigate to &lt;a href=&#34;http://localhost:8001/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:http/proxy/&#34;&gt;http://localhost:8001/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:http/proxy/&lt;/a&gt;&lt;br&gt;5.  The above link should open the longhorn UI.&lt;br&gt;6.  Verify all the pages, refresh each page and verify. Create a volume and check the volume detail page also.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3.&lt;/td&gt;&#xA;          &lt;td&gt;Access Longhorn UI with node port&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a cluster (3 worker nodes and 1 etcd/control plane) in rancher, Go to the default project.&lt;br&gt;2.  Go to App, Click the launch app.&lt;br&gt;3.  Select longhorn.&lt;br&gt;4.  Select &lt;code&gt;NodePort&lt;/code&gt; under the Longhorn UI service.&lt;br&gt;5.  Once the app is deployed successfully, click the link like &lt;a href=&#34;http://104.131.80.163:32059/&#34;&gt;32059/tcp&lt;/a&gt; appears in App page.&lt;br&gt;6.  The page should redirect to longhorn UI - &lt;a href=&#34;http://104.131.80.163:32059/#/dashboard&#34;&gt;http://node-ip:32059/#/dashboard&lt;/a&gt;&lt;br&gt;7.  Verify all the pages, refresh each page and verify. Create a volume and check the volume detail page also.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4.&lt;/td&gt;&#xA;          &lt;td&gt;Access Longhorn UI with ingress controller&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a cluster(3 worker nodes and 1 etcd/control plane).&lt;br&gt;2.  Deploy longhorn.&lt;br&gt;3.  Create ingress controller. refer &lt;a href=&#34;https://longhorn.io/docs/1.0.1/deploy/accessing-the-ui/longhorn-ingress/&#34;&gt;https://longhorn.io/docs/1.0.1/deploy/accessing-the-ui/longhorn-ingress/&lt;/a&gt;&lt;br&gt;4.  If cluster is imported/created in rancher create ingress using rancher UI by selecting &lt;code&gt;Target Backend&lt;/code&gt; as &lt;code&gt;longhorn frontend&lt;/code&gt; and path &lt;code&gt;/&lt;/code&gt;&lt;br&gt;5.  Access the ingress. It should redirect to longhorn UI.&lt;br&gt;6.  Verify all the pages, refresh each page and verify. Create a volume and check the volume detail page also.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5.&lt;/td&gt;&#xA;          &lt;td&gt;Access Longhorn UI with a load balancer&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a cluster (3 worker nodes and 1 etcd/control plane) in rancher.&lt;br&gt;2.  Create a route 53 entry pointing to worker nodes of the cluster in AWS.&lt;br&gt;3.  Deploy longhorn from catalog library and mention the route 53 entry in the load balancer.&lt;br&gt;4.  Go to the link that appears on the app page for the longhorn app.&lt;br&gt;5.  The page to redirect to longhorn UI with URL as route 53 entry.&lt;br&gt;6.  Verify all the pages, refresh each page and verify. Create a volume and check the volume detail page also.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6.&lt;/td&gt;&#xA;          &lt;td&gt;Access Longhorn UI with reverse proxy&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a cluster (3 worker nodes and 1 etcd/control plane) in rancher, Go to the default project.&lt;br&gt;2.  Go to App, Click the launch app.&lt;br&gt;3.  Select longhorn.&lt;br&gt;4.  Select &lt;code&gt;NodePort&lt;/code&gt; under the Longhorn UI service.&lt;br&gt;5.  Install nginx in local system.&lt;br&gt;6.  Set the &lt;code&gt;proxy_pass&lt;/code&gt; of &lt;a href=&#34;http://104.131.80.163:32059/#/dashboard&#34;&gt;http://node-ip:32059&lt;/a&gt; in ngnix.conf file as per below example.&lt;br&gt;7.  Start nginx&lt;br&gt;8.  Access the port given in &lt;code&gt;listen&lt;/code&gt; parameter from nginx.conf. ex - //localhost:822&lt;br&gt;9.  The page should redirect to longhorn UI&lt;br&gt;10.  Verify all the pages, refresh each page and verify. Create a volume and check the volume detail page also.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;nginx.conf example&lt;/p&gt;</description>
    </item>
    <item>
      <title>3. Volume</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/volume/</guid>
      <description>&lt;h3 id=&#34;test-cases-for-volume&#34;&gt;Test cases for Volume&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Check volume Details&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Longhorn Nodes has node tags&lt;br&gt;*   Node Disks has disk tags&lt;br&gt;*   Backup target is set to NFS server, or S3 compatible target&lt;br&gt;&lt;br&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Check volume details page&lt;br&gt;3.  Create volume backup&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume Details&lt;br&gt;    *   &lt;code&gt;State&lt;/code&gt; should be &lt;code&gt;Attached&lt;/code&gt;&lt;br&gt;    *   &lt;code&gt;Health&lt;/code&gt; should be healthy&lt;br&gt;    *   &lt;code&gt;Frontend&lt;/code&gt; should be &lt;code&gt;Block Device&lt;/code&gt;&lt;br&gt;    *   &lt;code&gt;Attached Node &amp;amp; Endpoint&lt;/code&gt; should be node name that volume is attached to and PATH of the volume device file on that node.&lt;br&gt;    *   &lt;code&gt;Size&lt;/code&gt; should match volume size specified in Create Volume step&lt;br&gt;    *   &lt;code&gt;Actual Size&lt;/code&gt; should be &lt;code&gt;0Bi&lt;/code&gt; (No data has been written to the volume yet)&lt;br&gt;    *   &lt;code&gt;Engine Image&lt;/code&gt; should be &lt;code&gt;longhornio/longhorn-engine:&amp;lt;LONGHORN_VERSION&amp;gt;&lt;/code&gt;&lt;br&gt;    *   &lt;code&gt;Created&lt;/code&gt; should indicate time since volume is created.&lt;br&gt;    *   &lt;code&gt;Node Tags&lt;/code&gt; should be empty (no node tags has been specified during creation)&lt;br&gt;    *   &lt;code&gt;Disk Tags&lt;/code&gt; should be empty (no disk tags has been specified during creation)&lt;br&gt;    *   &lt;code&gt;Last Backup&lt;/code&gt; should be empty (no backup has been created)&lt;br&gt;    *   &lt;code&gt;Last Backup At&lt;/code&gt; should be empty (no backup has been created)&lt;br&gt;    *   &lt;code&gt;Instance Manager&lt;/code&gt; should contain instance manager image name&lt;br&gt;    *   &lt;code&gt;Namespace&lt;/code&gt; should match namespace specified in Volume Create step.&lt;br&gt;    *   &lt;code&gt;PVC Name&lt;/code&gt; should be empty (no PV has been created for that volume yet)&lt;br&gt;    *   &lt;code&gt;PV Name&lt;/code&gt; should be empty (no PV has been created for that volume yet)&lt;br&gt;    *   &lt;code&gt;PV Status&lt;/code&gt; should be empty (no PV/PVC has been created for that volume yet)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Filter Volumes&lt;/td&gt;&#xA;          &lt;td&gt;User should be able to filter volumes using the following filters&lt;br&gt;&lt;br&gt;*   Name&lt;br&gt;*   Node&lt;br&gt;*   Status (Healthy, In progress, Degraded, Faulted, detached)&lt;br&gt;*   Namespace&lt;br&gt;*   Node redundancy (Yes, Limited, No)&lt;br&gt;*   PV Name&lt;br&gt;*   PVC Name&lt;br&gt;*   Node tag&lt;br&gt;*   Disk tag&lt;br&gt;&lt;br&gt;Notes:&lt;br&gt;&lt;br&gt;*   Limited node redundancy: at least one healthy replica is running at the same node as another&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume list should match filtering criteria applied.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Delete multiple volumes&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;    *   Create multiple volumes&lt;br&gt;&lt;br&gt;1.  Select multiple volumes and delete&lt;/td&gt;&#xA;          &lt;td&gt;*   Volumes should be deleted&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Attach multiple volumes&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;    *   Create multiple volumes&lt;br&gt;&lt;br&gt;1.  Select multiple volumes and Attach them to a node&lt;/td&gt;&#xA;          &lt;td&gt;*   All Volumes should be attached to the same node specified in volume attach request.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Attach multiple volumes in maintenance mode&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;    *   Create multiple volumes&lt;br&gt;&lt;br&gt;1.  Select multiple volumes and Attach them to a node in maintenance mode&lt;/td&gt;&#xA;          &lt;td&gt;*   All Volumes should be attached in maintenance mode to the same node specified in volume attach request.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Detach multiple volumes&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;    *   Multiple attached volumes&lt;br&gt;*   Select multiple volumes and detach&lt;/td&gt;&#xA;          &lt;td&gt;*   Volumes should be detached&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Backup multiple Volumes&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;    *   Longhorn should be configured to point to a backupstore&lt;br&gt;    *   Multiple volumes existed and attached to node/used buy kubernetes workload&lt;br&gt;    *   Write some data to multiple volumes and compute it’s checksum&lt;br&gt;*   Select multiple volumes and Create a backup&lt;br&gt;*   restore volumes backups and check its data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume backups should be created&lt;br&gt;*   Restored volumes from backup should contain the same data when backup is created&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Create PV/PVC for multiple volumes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create multiple volumes&lt;br&gt;&lt;br&gt;1.  Select multiple volumes&lt;br&gt;2.  Create a PV, specify filesysem&lt;br&gt;3.  Check PV in Lonhgorn UI and in Kubernetes&lt;br&gt;4.  Create PVC&lt;br&gt;5.  Check PVC in Lonhgorn UI and in Kubernetes&lt;br&gt;6.  Delete PVC&lt;br&gt;7.  Check PV in Lonhgorn UI and in Kubernetes&lt;/td&gt;&#xA;          &lt;td&gt;*   For all selected volumes&lt;br&gt;    *   PV should created&lt;br&gt;    *   PV/PVC status in UI should be &lt;code&gt;Available&lt;/code&gt;&lt;br&gt;    *   PV &lt;code&gt;spec.csi.fsType&lt;/code&gt; should match filesystem specified in PV creation request&lt;br&gt;    *   PV &lt;code&gt;spec.storageClassName&lt;/code&gt; should match the setting in &lt;code&gt;Default Longhorn Static StorageClass Name&lt;/code&gt;&lt;br&gt;    *   PV &lt;code&gt;spec.csi.volumeHandle&lt;/code&gt; should be the volume name&lt;br&gt;    *   PV/PVC status in UI should be &lt;code&gt;Bound&lt;/code&gt; in Longhorn UI&lt;br&gt;    *   PVC namespace should match namespace specified in PVC creation request&lt;br&gt;    *   After Deleting PVC, PV/PVC status should be &lt;code&gt;Relased&lt;/code&gt; in Longhorn UI.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Volume expansion&lt;/td&gt;&#xA;          &lt;td&gt;Check Multiple Volume expansion test cases work for multiple volumes&lt;br&gt;&lt;br&gt;&lt;a href=&#34;https://rancher.atlassian.net/wiki/spaces/LON/pages/354453117/Volume+detail+page&#34;&gt;Test Cases in Volume Details page&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Volume expansion should work for multiple volumes.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Engine Offline Upgrade For Multiple Volumes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Volume is consumed by Kubernetes deployment workload&lt;br&gt;*   Volume use old Longhorn Engine&lt;br&gt;&lt;br&gt;1.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;2.  Scale down deployment , volume gets detached&lt;br&gt;3.  Upgrade Longhorn engine image to use new deployed engine image&lt;br&gt;4.  Scale up deployment, volume gets attached&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume read/write operations should work before and after engine upgrade.&lt;br&gt;*   Old Engine &lt;code&gt;Reference Count&lt;/code&gt; will be decreased by 1&lt;br&gt;*   New Engine &lt;code&gt;Reference Count&lt;/code&gt; will be increased by 1&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;Show System Hidden&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;:&lt;br&gt;&lt;br&gt;*   Volume is created and attached to a pod.&lt;br&gt;&lt;br&gt;1.  Click the volume appearing on volume list page, it takes user to volume.&lt;br&gt;2.  Take snapshot and upgrade the replicas.&lt;br&gt;3.  Under snapshot section, enable option &amp;lsquo;Show System Hidden&lt;/td&gt;&#xA;          &lt;td&gt;Enabling this option will show system created snapshots while rebuilding of replicas.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;Event log&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;:&lt;br&gt;&lt;br&gt;*   Volume is created and attached to a pod.&lt;br&gt;&lt;br&gt;1.  Click event log to expand&lt;/td&gt;&#xA;          &lt;td&gt;Verify details appearing in logs.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;replica&#34;&gt;Replica&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Automated ? / test name&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Replica list&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a volume and change the default number of replicas&lt;br&gt;2.  Attach volume to a node&lt;br&gt;3.  Check replica list&lt;/td&gt;&#xA;          &lt;td&gt;*   Number of replicas should match number specified in volume creation request&lt;br&gt;*   All replicas should be &lt;code&gt;Running&lt;/code&gt;, and &lt;code&gt;Healthy&lt;/code&gt;&lt;br&gt;*   Replica info also should contain, &lt;code&gt;Node Name&lt;/code&gt;, &lt;code&gt;Replica Instance Manager Name&lt;/code&gt;, &lt;code&gt;Replica Path&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;test_volume_update_replica_count&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Update volume replica count (increase)&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a volume&lt;br&gt;2.  Attach volume to a node&lt;br&gt;3.  Increase replica count +1&lt;/td&gt;&#xA;          &lt;td&gt;*   New system hidden snapshot should be created&lt;br&gt;*   A new replica should be created&lt;br&gt;*   New replicas should be &lt;code&gt;Running&lt;/code&gt; &amp;amp; &lt;code&gt;Rebuilding&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;test_volume_update_replica_count&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Update volume replica count (decrease)&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a volume&lt;br&gt;2.  Attach volume to a node&lt;br&gt;3.  decrease replica count by +1&lt;br&gt;4.  Delete a replica&lt;/td&gt;&#xA;          &lt;td&gt;*   After decreasing replica count, nothing should happen&lt;br&gt;*   Deleting a replica will not trigger replica rebuild&lt;/td&gt;&#xA;          &lt;td&gt;test_volume_update_replica_count&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;snapshot&#34;&gt;Snapshot&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Create Snapshot&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume head should have parent as snapshot#1&lt;br&gt;*   Snapshot should be created&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Revert Snapshot&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a deployment workload with &lt;code&gt;nReplicas = 1&lt;/code&gt; using Longhorn volume&lt;br&gt;2.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;3.  Write some other data, compute it’s checksum (checksum#2)&lt;br&gt;4.  Create a snapshot (snapshot#1)&lt;br&gt;5.  Scale down deployment &lt;code&gt;nReplicas = 0&lt;/code&gt;&lt;br&gt;6.  Attach volume in &lt;code&gt;maintenance mode&lt;/code&gt;&lt;br&gt;7.  Revert to (snapshot#1)&lt;br&gt;8.  Detach volume&lt;br&gt;9.  Scale back deployment &lt;code&gt;nReplicas = 1&lt;/code&gt;&lt;br&gt;10.  Compute data checksum (checksum#3)&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume head should have parent as snapshot#1&lt;br&gt;*   Volume state should be &lt;code&gt;Detached&lt;/code&gt; after scaling down deployment &lt;code&gt;nReplicas = 0&lt;/code&gt;&lt;br&gt;*   In Volume Details &lt;code&gt;Attached Node&lt;/code&gt; should be Node name which volume is attached to, without an &lt;code&gt;Endpoint&lt;/code&gt; (block device path)&lt;br&gt;*   Data checksum after revert should match data checksum when taking snapshot &lt;code&gt;checksum#3 == checksum#1&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Delete Snapshot&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)&lt;br&gt;5.  Write data to volume, compute it’s checksum (checksum#4) → live data&lt;br&gt;6.  Delete (snapshot#2)&lt;br&gt;7.  Revert to (snapshot#3)&lt;br&gt;8.  Revert to (snapshot#1)&lt;/td&gt;&#xA;          &lt;td&gt;*   Snapshot#2 will be deleted, verify in replica /var/lib/rancher/longhorn/replicas/&lt;br&gt;*   After reverting to snapshot#3 verify the data.&lt;br&gt;*   After reverting to snapshot#1 data checksum should match (checksum#1)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Delete Snapshot while rebuilding replicas&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume (1GB+), compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Delete a replica&lt;br&gt;5.  while replica is rebuilding, try to delete (snapshot#1)&lt;/td&gt;&#xA;          &lt;td&gt;*   New system snapshot should be created&lt;br&gt;*   Will &lt;strong&gt;NOT&lt;/strong&gt; be able to delete snapshot#1&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Snapshot Purge&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume (1GB+), compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Delete a replica&lt;br&gt;5.  After rebuild is complete, delete (snapshot#1)&lt;/td&gt;&#xA;          &lt;td&gt;*   New system snapshot should be created&lt;br&gt;*   Snapshot#1 will be delete&lt;br&gt;*   Snapshot purge process will be triggered&lt;br&gt;*   Only system snapshot should be present&lt;br&gt;*   You will not be able revert to the system snapshot&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Create recurring snapshots&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a deployment workload with &lt;code&gt;nReplicas = 1&lt;/code&gt; using Longhorn volume&lt;br&gt;2.  Write data to volume , compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a recurring snapshot &lt;code&gt;every 5 minutes&lt;/code&gt;. and set retain count to &lt;code&gt;5&lt;/code&gt;&lt;br&gt;4.  Wait for 2 recurring snapshots to triggered (snapshot#1, snapshot#2 )&lt;br&gt;5.  Scale down deployment &lt;code&gt;nReplicas = 0&lt;/code&gt;&lt;br&gt;6.  Attach volume in &lt;code&gt;maintenance mode&lt;/code&gt;&lt;br&gt;7.  Revert to (snapshot#1)&lt;br&gt;8.  Scale back deployment &lt;code&gt;nReplicas = 1&lt;/code&gt;&lt;br&gt;9.  Wait for another recurring snapshots to triggered (snapshot#3)&lt;br&gt;10.  Delete (snapshot#1)&lt;/td&gt;&#xA;          &lt;td&gt;*   Snapshots (snapshot#1, snapshot#2) should be created&lt;br&gt;*   Before deleting (snapshot#1), Parent snapshot of (snapshot#3) should be (snapshot#1)&lt;br&gt;*   After deleting (snapshot#1), Parent snapshot of (snapshot#3) should be starting point.&lt;br&gt;*   Only max of &lt;code&gt;5&lt;/code&gt; snapshots should be retained&lt;br&gt;*   Oldest snapshot will be removed when number of snapshots created by recurring job exceeds retain count&lt;br&gt;*   only snapshots generated by recurring job is affected by retain count, user can created manual snapshots and it will not be deleted automatically.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Disabling/Deleting recurring snapshots&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a deployment workload with &lt;code&gt;nReplicas = 1&lt;/code&gt; using Longhorn volume&lt;br&gt;2.  Write data to volume , compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a recurring snapshot &lt;code&gt;every 5 minutes&lt;/code&gt;. and set retain count to &lt;code&gt;5&lt;/code&gt;&lt;br&gt;4.  Wait for 2 recurring snapshots to triggered (snapshot#1,snapshot#2 )&lt;br&gt;5.  Delete the recurring snapshots&lt;/td&gt;&#xA;          &lt;td&gt;*   Recurring snapshots will stop after deletion of it.&lt;br&gt;*   Existing snapshots should retain.&lt;br&gt;*   User should be able to take snapshot manually.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Operation with volume created using rancher UI&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create pv/pvc in rancher UI.&lt;br&gt;2.  Deploy a workload with PVC created in rancher UI.&lt;br&gt;3.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;4.  In longhorn UI, create a snapshot.&lt;br&gt;5.  Write data to volume again.&lt;br&gt;6.  Revert to snapshot.&lt;br&gt;7.  Delete the snapshot.&lt;/td&gt;&#xA;          &lt;td&gt;*   User should be able to create snapshot.&lt;br&gt;*   User should to revert to snapshot created verify this by checksum.&lt;br&gt;*   User should be able to delete the snapshot. Verify this in replicas in the nodes.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Delete the last snapshot&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume(more than 4k), compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)&lt;br&gt;5.  Write data to volume, compute it’s checksum (checksum#4) → live data&lt;br&gt;6.  Delete (snapshot#3)&lt;/td&gt;&#xA;          &lt;td&gt;*   Data from last snapshot should not get lost.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;Multi branch snapshot&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume(more than 4k), compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)&lt;br&gt;5.  Write data to volume, compute it’s checksum (checksum#4) → live data&lt;br&gt;6.  Revert to snapshot#2&lt;br&gt;7.  Write data to volume, compute it’s checksum.&lt;br&gt;8.  Repeat steps 2,3 two more times to have (snapshot#4, snapshot#5) with different data files (checksum#5, checksum#6)&lt;br&gt;9.  Revert to snapshot#3&lt;/td&gt;&#xA;          &lt;td&gt;*   Verify the data in any of the replica&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;Backup from a snapshot&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Repeat steps 2,3 two more times to have (snapshot#2, snapshot#3) with different data files (checksum#2, checksum#3)&lt;br&gt;5.  Write data to volume, compute it’s checksum (checksum#4) → live data&lt;br&gt;6.  Take backup from a snapshot#2.&lt;br&gt;7.  Restore from the backup taken&lt;/td&gt;&#xA;          &lt;td&gt;Verify the data of the backup, it should match data from snapshot#2&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;volume-expansion&#34;&gt;Volume Expansion&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Volume &lt;strong&gt;Online expansion&lt;/strong&gt; for attached volume&lt;br&gt;&lt;br&gt;&lt;strong&gt;(Not Supported for now)&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create multiple 4 volumes, each of size 5 GB, Attach them to nodes&lt;br&gt;2.  Format each volume using one of the following formats (ext 2/3/4 &amp;amp; xfs)&lt;br&gt;3.  Mount volumes to directories on the nodes.&lt;br&gt;4.  Check volume size and used space using &lt;code&gt;df -h&lt;/code&gt; command&lt;br&gt;5.  Write 4 GB data file to each volume&lt;br&gt;6.  For each volume, from Operation menu, Click &lt;code&gt;Expand Volume&lt;/code&gt;, set size to &lt;code&gt;10 GB&lt;/code&gt;, and click &lt;code&gt;OK&lt;/code&gt;&lt;br&gt;7.  Check volume size and used space using &lt;code&gt;df -h&lt;/code&gt; command&lt;br&gt;8.  Add more 4 GB data file to each volume&lt;br&gt;9.  Check volume size and used space using &lt;code&gt;df -h&lt;/code&gt; command&lt;br&gt;10.  Check volume size expanded&lt;/td&gt;&#xA;          &lt;td&gt;*   Volumes should be expanded to the new size&lt;br&gt;*   Volume read/write operations should work after size expansion&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Volume &lt;strong&gt;Online expansion&lt;/strong&gt; for volume consumed by Kubernetes workload&lt;br&gt;&lt;br&gt;&lt;strong&gt;→ Kubernetes Version: 1.15&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a &lt;code&gt;2GB&lt;/code&gt; volume used by a Kubernetes workload&lt;br&gt;2.  Expand Volume size to &lt;code&gt;10 GB&lt;/code&gt;&lt;br&gt;3.  In Kubernetes, edit PV/PVC capacity to match new volume size.&lt;br&gt;4.  Check volume size using &lt;code&gt;df -h&lt;/code&gt; command from Kubernetes workload&lt;br&gt;5.  Write 8 GB data file to volume, and compute its checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   When resizing, A message indicate that &lt;code&gt;The capacity of related PV and PVC will not be updated&lt;/code&gt;&lt;br&gt;*   Volumes should be expanded to the new size&lt;br&gt;*   Volume read/write operations should work after size expansion&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Volume &lt;strong&gt;Online expansion&lt;/strong&gt; for volume consumed by Kubernetes workload&lt;br&gt;&lt;br&gt;&lt;strong&gt;→ Kubernetes Version: 1.16+&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   PVC is dynamically provisioned by the StorageClass.&lt;br&gt;*   The Kubernetes is version 1.16+ or the feature gate for volume expansion is enabled.&lt;br&gt;*   The StorageClass should support resize, &lt;code&gt;allowVolumeExpansion: true&lt;/code&gt; is set in the StorageClass&lt;br&gt;&lt;br&gt;1.  Create a &lt;code&gt;2GB&lt;/code&gt; volume used by a Kubernetes workload&lt;br&gt;2.  Expand Volume size to &lt;code&gt;10 GB&lt;/code&gt;&lt;br&gt;3.  Check volume size using &lt;code&gt;df -h&lt;/code&gt; command from Kubernetes workload&lt;br&gt;4.  Write 8 GB data file to volume, and compute its checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   When resizing, A message indicate that &lt;code&gt;The capacity of related PV and PVC will not be updated&lt;/code&gt;&lt;br&gt;*   Volumes should be expanded to the new size&lt;br&gt;*   Volume read/write operations should work after size expansion&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Volume Offline expansion&lt;br&gt;&lt;br&gt;&lt;strong&gt;Kubernetes Version: &amp;lt; 1.16&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create and attach a volume&lt;br&gt;2.  Format and mount the volume. Fill up the volume and get the checksum (checksum#1)&lt;br&gt;3.  Unmount and detach the volume.&lt;br&gt;4.  Expand the volume and wait for the expansion complete.&lt;br&gt;5.  Reattach and remount the volume. Check the checksum and if the filesystem is expanded.&lt;br&gt;6.  Fill up the expanded parts and get the checksum. (checksum#2)&lt;br&gt;7.  Unmount and detach the volume.&lt;br&gt;8.  Launch a workload for it on a different node. Check data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume size should be expanded&lt;br&gt;*   Volume read/write operations should work after size expansion&lt;br&gt;*   After expansion, data checksum should match checksum#1&lt;br&gt;*   Final data checksum should match checksum#2&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Volume expansion with revert and backup&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create and attach a volume.&lt;br&gt;2.  Format and mount the volume. Fill up the volume and get the checksum. (checksum#1)&lt;br&gt;3.  Create the 1st snapshot and backup. (snapshot#1 &amp;amp; backup#1)&lt;br&gt;4.  Expand the volume. Fill up the expanded part and get the checksum (checksum#2)&lt;br&gt;5.  Create the 2nd snapshot and backup. (snapshot#2 &amp;amp; backup#2)&lt;br&gt;6.  Check if the backup volume size is expanded.&lt;br&gt;7.  Restore backup#2 to a volume and check its data&lt;br&gt;8.  Clean up then refill the volume. Get the checksum. (checksum#3)&lt;br&gt;9.  Create the 3rd snapshot and backup. (snapshot#3 &amp;amp; backup#3)&lt;br&gt;10.  Revert to the 2nd snapshot. Check the checksum.&lt;br&gt;11.  Revert to the 1st snapshot. Check the checksum and if we can still use the expanded part.&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume should be expanded&lt;br&gt;*   backup#2 size should be expanded and match volume new expanded size&lt;br&gt;*   Restored volume data from backup#2 should match checksum#2&lt;br&gt;*   After reverting to snapshot#1, data checksum should match checksum#1&lt;br&gt;*   After reverting to snapshot#1 expanded size should be usable.&lt;br&gt;*   Volume read/write operations should work after expansion and revert.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;rwx-volume-native-support-starting-with-v110&#34;&gt;RWX Volume native support starting with v1.1.0&lt;/h2&gt;&#xA;&lt;h3 id=&#34;prerequisite&#34;&gt;Prerequisite:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Longhorn is deployed in a cluster having 4 nodes (1 etcd/control plane and 3 worker)&lt;/li&gt;&#xA;&lt;li&gt;NFS-Common is installed on the nodes.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Scenario&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Steps&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Create StatefulSet/Deployment with single pod with volume attached in RWX mode&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 1 pod.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Verify that a PVC, ShareManger pod, CRD and volume in Longhorn get created.&lt;br&gt;4.  Verify there is directory with the name of PVC exists in the ShareManager mount point i.e. &lt;code&gt;export&lt;/code&gt;&lt;br&gt;5.  Write some data in the pod and verify the same data reflects in the ShareManager.&lt;br&gt;6.  Verify the longhorn volume, it should reflect the correct size.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Create StatefulSet/Deployment with more than 1 pod with volume attached in RWX mode.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with multiple pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Verify that one volume per pod in Longhorn gets created.&lt;br&gt;4.  Verify there is directory with the name of PVC exists in the ShareManager mount point i.e. &lt;code&gt;export&lt;/code&gt;&lt;br&gt;5.  Verify that Longhorn UI shows all the pods name attached to the volume.&lt;br&gt;6.  Write some data in all the pod and verify all the data reflects in the ShareManager.&lt;br&gt;7.  Verify the longhorn volume, it should reflect the correct size.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Create StatefulSet/Deployment with the existing PVC of a RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 1 pod.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Verify that a PVC, ShareManger pod, CRD and volume in Longhorn get created.&lt;br&gt;4.  Write some data in the pod and verify the same data reflects in the ShareManager.&lt;br&gt;5.  Create another StatefulSet/Deployment using the above created PVC.&lt;br&gt;6.  Write some data in the new pod, the same should be reflected in the ShareManager pod.&lt;br&gt;7.  Verify the longhorn volume, it should reflect the correct size.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Scale up StatefulSet/Deployment with one pod attached with volume in RWX mode.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 1 pod.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod and verify the same data reflects in the ShareManager.&lt;br&gt;4.  Scale up the StatefulSet/Deployment.&lt;br&gt;5.  Verify a new volume gets created.&lt;br&gt;6.  Write some data in the new pod, the same should be reflected in the ShareManager pod.&lt;br&gt;7.  Verify the longhorn volume, it should reflect the correct size.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Scale down StatefulSet/Deployment attached with volume in RWX mode to zero.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 1 pod.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod and verify the same data reflects in the ShareManager.&lt;br&gt;4.  Scale down the StatefulSet/Deployment to zero&lt;br&gt;5.  Verify the ShareManager pod gets deleted.&lt;br&gt;6.  Verify the volume should be in detached state.&lt;br&gt;7.  Create a new StatefulSet/Deployment with the existing PVC with different mount point.&lt;br&gt;8.  Verify the ShareManager should get created and volume should become attached.&lt;br&gt;9.  Verify the data.&lt;br&gt;10.  Delete the newly created StatefulSet/Deployment.&lt;br&gt;11.  Verify the ShareManager pod gets deleted again.&lt;br&gt;12.  Scale up the first StatefulSet/Deployment.&lt;br&gt;13.  Verify the ShareManager should get created and volume should become attached.&lt;br&gt;14.  Verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Delete the Workload StatefulSet/Deployment attached with RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 1 pod.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod and verify the same data reflects in the ShareManager.&lt;br&gt;4.  Delete the workload.&lt;br&gt;5.  Verify the ShareManager pod gets deleted but the CRD should not be deleted.&lt;br&gt;6.  Verify the volume should be in detached state.&lt;br&gt;7.  Create another StatefulSet with existing PVC.&lt;br&gt;8.  Verify the ShareManager should get created and volume should become attached.&lt;br&gt;9.  Verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Take snapshot and backup of a RWX volume in Longhorn.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Take a snapshot and a backup.&lt;br&gt;5.  Write some more data into the pod.&lt;br&gt;6.  Revert to snapshot 1 and verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Restore a backup taken from a RWX volume in Longhorn.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Take a backup of a RWX volume.&lt;br&gt;5.  Restore from the backup and attach the volume to a pod.&lt;br&gt;6.  Verify the data and the volume should be read write once.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Create DR volume of a RWX volume in Longhorn.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Take a backup of the volume.&lt;br&gt;5.  Create a DR volume of the backup.&lt;br&gt;6.  Write more data in the pods and take more backups.&lt;br&gt;7.  Verify the DR volume is getting synced with latest backup.&lt;br&gt;8.  Activate the DR volume and verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Expand the RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Expand the volume.&lt;br&gt;5.  Verify that user is able to write data in the expanded volume.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;Recurring Backup/Snapshot with RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Schedule a recurring backup/Snapshot.&lt;br&gt;5.  Verify the recurring jobs are getting created and is taking backup/snapshot successfully.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;Deletion of the replica of a Longhorn RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Delete one of the replica and verify that the rebuild of replica is working fine.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;Parallel writing&lt;/td&gt;&#xA;          &lt;td&gt;1.  Write data in multiple pods attached to the same volume at the same time.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;14&lt;/td&gt;&#xA;          &lt;td&gt;Data locality with RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Enable &lt;code&gt;Data-locality&lt;/code&gt;&lt;br&gt;5.  Disable &lt;code&gt;Node soft anti-affinity&lt;/code&gt;.&lt;br&gt;6.  Disable the node where the volume is attached for some time.&lt;br&gt;7.  Wait for replica to be rebuilt on another node.&lt;br&gt;8.  Enable the node scheduling and verify a replica gets rebuilt on the attached node.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;15&lt;/td&gt;&#xA;          &lt;td&gt;Node eviction with RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Do a node eviction and verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;16&lt;/td&gt;&#xA;          &lt;td&gt;Auto salvage feature on an RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Crash all the replicas and verify the auto-salvage works fine.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;17&lt;/td&gt;&#xA;          &lt;td&gt;RWX volume with &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt; enabled.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Set a recurring backup and scale down all the pods.&lt;br&gt;5.  Verify the volume get attached at scheduled time and backup/snapshot get created.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;18&lt;/td&gt;&#xA;          &lt;td&gt;RWX volume with Toleration.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Set some Toleration.&lt;br&gt;5.  Verify the ShareManager pods have the toleration and annotation updated.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;19&lt;/td&gt;&#xA;          &lt;td&gt;Detach/Delete operation on an RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Detach action on the Longhorn UI should not work on RWX volume.&lt;br&gt;2.  On deletion of the RWX volume, the ShareManager CRDs should also get deleted.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;20&lt;/td&gt;&#xA;          &lt;td&gt;Crash instance e manager of the RWX volume&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Crash the instance manager.&lt;br&gt;5.  On crashing the IM, the ShareManager pods should be immediately redeployed.&lt;br&gt;6.  Based on the setting &lt;code&gt;Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly&lt;/code&gt;, the workload pods will get redeployed.&lt;br&gt;7.  On recreating on workload pods, the volume should get attached successfully.&lt;br&gt;8.  If &lt;code&gt;Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly&lt;/code&gt; is disabled, user should see I/O error on the mounted point.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;21&lt;/td&gt;&#xA;          &lt;td&gt;Reboot the ShareManager and workload node&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Reboot the ShareManager node.&lt;br&gt;5.  The ShareManager pod should move to another node.&lt;br&gt;6.  As the instance e manager is on the same node and based on setting &lt;code&gt;Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly&lt;/code&gt;, the workload should be redeployed and volume should be available to user.&lt;br&gt;7.  Reboot the workload node.&lt;br&gt;8.  On restart on the node, pods should get attached to the volume. Verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;22&lt;/td&gt;&#xA;          &lt;td&gt;Power down the ShareManager and workload node.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Power down the ShareManager node.&lt;br&gt;5.  The ShareManager pod should move to another node.&lt;br&gt;6.  As the instance manager is on the same node and based on the setting &lt;code&gt;Automatically Delete Workload Pod when The Volume Is Detached Unexpectedly&lt;/code&gt;, the workload should be redeployed and volume should be available to user.&lt;br&gt;7.  Power down the workload node.&lt;br&gt;8.  The workload pods should move to another node based on &lt;code&gt;Pod Deletion Policy When Node is Down&lt;/code&gt; setting.&lt;br&gt;9.  Once the pods are up, they should get attached to the volume. Verify the data.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;23&lt;/td&gt;&#xA;          &lt;td&gt;Kill the nfs process in the ShareManager&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Kill the NFS server in the ShareManager pod.&lt;br&gt;5.  The NFS server should retry to come up.&lt;br&gt;6.  Volume should continue to accessible.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;24&lt;/td&gt;&#xA;          &lt;td&gt;Delete the ShareManager CRD.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Delete the ShareManager CRD.&lt;br&gt;5.  A new ShareManager CRD should be created.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;25&lt;/td&gt;&#xA;          &lt;td&gt;Delete the ShareManager pod.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Delete the ShareManager pod.&lt;br&gt;5.  A new ShareManager pod should be immediately created.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;26&lt;/td&gt;&#xA;          &lt;td&gt;Drain the ShareManager node.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Drain the ShareManager pod node.&lt;br&gt;5.  The volume should get detached first, then the shareManager pod should move to another node and Volume should get reattached.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;27&lt;/td&gt;&#xA;          &lt;td&gt;Disk full on the ShareManager node.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod and make the disk almost full.&lt;br&gt;4.  Verify the RWX volume is not failed.&lt;br&gt;5.  Verify the creation of snapshot/backup.&lt;br&gt;6.  Try to write more data, and the it should error out &lt;code&gt;no space left&lt;/code&gt;.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;28&lt;/td&gt;&#xA;          &lt;td&gt;Scheduling failure with RWX volume.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Disable 1 node.&lt;br&gt;2.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;3.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;4.  Verify the RWX volume gets created with degraded state.&lt;br&gt;5.  Write some data in the pod.&lt;br&gt;6.  Enable the node and the volume should become healthy.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;29&lt;/td&gt;&#xA;          &lt;td&gt;Add a node in the cluster.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Add a node in the cluster.&lt;br&gt;2.  Create multiple statefulSet/deployment with RWX volume.&lt;br&gt;3.  Verify that the ShareManager pod is able to scheduled on the new node.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;30&lt;/td&gt;&#xA;          &lt;td&gt;Delete a node from the cluster.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a StatefulSet/Deployment with 2 pods.&lt;br&gt;2.  Attach a volume with RWX mode using longhorn class and selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;br&gt;3.  Write some data in the pod.&lt;br&gt;4.  Delete the ShareManager node from the cluster.&lt;br&gt;5.  Verify the ShareManager pod move to new node and volume continues to be accessible.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;31&lt;/td&gt;&#xA;          &lt;td&gt;RWX with Linux/SLES OS&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;32&lt;/td&gt;&#xA;          &lt;td&gt;RWX with K3s set up&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;33&lt;/td&gt;&#xA;          &lt;td&gt;RWX in Air gap set up.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;34&lt;/td&gt;&#xA;          &lt;td&gt;RWX in PSP enabled set up.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>5. Kubernetes</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/kubernetes/</guid>
      <description>&lt;h3 id=&#34;dynamic-provisioning-with-storageclass&#34;&gt;Dynamic provisioning with StorageClass&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Can create and use volume using StorageClass&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Can create a new StorageClass use new parameters and it will take effect on the volume created by the storage class.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;If the PV reclaim policy is delete, once PVC and PV are deleted, Longhorn volume should be deleted.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;static-provisioning-using-longhorn-created-pvpvc&#34;&gt;Static provisioning using Longhorn created PV/PVC&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;PVC can be used by the new workload&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Delete the PVC will not result in PV deletion&lt;/p&gt;</description>
    </item>
    <item>
      <title>6. Backup</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/backup/</guid>
      <description>&lt;h2 id=&#34;automation-tests&#34;&gt;Automation Tests&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test name&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;tag&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;test_backup&lt;/td&gt;&#xA;          &lt;td&gt;Test basic backup&lt;br&gt;&lt;br&gt;Setup:&lt;br&gt;&lt;br&gt;1.  Create a volume and attach to the current node&lt;br&gt;2.  Run the test for all the available backupstores.&lt;br&gt;&lt;br&gt;Steps:&lt;br&gt;&lt;br&gt;1.  Create a backup of volume&lt;br&gt;2.  Restore the backup to a new volume&lt;br&gt;3.  Attach the new volume and make sure the data is the same as the old one&lt;br&gt;4.  Detach the volume and delete the backup.&lt;br&gt;5.  Wait for the restored volume&amp;rsquo;s &lt;code&gt;lastBackup&lt;/code&gt; to be cleaned (due to remove the backup)&lt;br&gt;6.  Delete the volume&lt;/td&gt;&#xA;          &lt;td&gt;Backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;test_backup_labels&lt;/td&gt;&#xA;          &lt;td&gt;Test that the proper Labels are applied when creating a Backup manually.&lt;br&gt;&lt;br&gt;1.  Create a volume&lt;br&gt;2.  Run the following steps on all backupstores&lt;br&gt;3.  Create a backup with some random labels&lt;br&gt;4.  Get backup from backupstore, verify the labels are set on the backups&lt;/td&gt;&#xA;          &lt;td&gt;Backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;test_deleting_backup_volume&lt;/td&gt;&#xA;          &lt;td&gt;Test deleting backup volumes&lt;br&gt;&lt;br&gt;1.  Create volume and create backup&lt;br&gt;2.  Delete the backup and make sure it&amp;rsquo;s gone in the backupstore&lt;/td&gt;&#xA;          &lt;td&gt;Backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;test_listing_backup_volume&lt;/td&gt;&#xA;          &lt;td&gt;Test listing backup volumes&lt;br&gt;&lt;br&gt;1.  Create three volumes: &lt;code&gt;volume1/2/3&lt;/code&gt;&lt;br&gt;2.  Setup NFS backupstore since we can manipulate the content easily&lt;br&gt;3.  Create snapshots for all three volumes&lt;br&gt;4.  Rename &lt;code&gt;volume1&lt;/code&gt;&amp;lsquo;s &lt;code&gt;volume.cfg&lt;/code&gt; to &lt;code&gt;volume.cfg.tmp&lt;/code&gt; in backupstore&lt;br&gt;5.  List backup volumes. Make sure &lt;code&gt;volume1&lt;/code&gt; errors out but found other two&lt;br&gt;6.  Restore &lt;code&gt;volume1&lt;/code&gt;&amp;lsquo;s &lt;code&gt;volume.cfg&lt;/code&gt;.&lt;br&gt;7.  Make sure now backup volume &lt;code&gt;volume1&lt;/code&gt; can be found and deleted&lt;br&gt;8.  Delete backups for &lt;code&gt;volume2/3&lt;/code&gt;, make sure they cannot be found later&lt;/td&gt;&#xA;          &lt;td&gt;Backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;test_ha_backup_deletion_recovery&lt;/td&gt;&#xA;          &lt;td&gt;[HA] Test deleting the restored snapshot and rebuild&lt;br&gt;&lt;br&gt;Backupstore: all&lt;br&gt;&lt;br&gt;1.  Create volume and attach it to the current node.&lt;br&gt;2.  Write &lt;code&gt;data&lt;/code&gt; to the volume and create snapshot &lt;code&gt;snap2&lt;/code&gt;&lt;br&gt;3.  Backup &lt;code&gt;snap2&lt;/code&gt; to create a backup.&lt;br&gt;4.  Create volume &lt;code&gt;res_volume&lt;/code&gt; from the backup. Check volume &lt;code&gt;data&lt;/code&gt;.&lt;br&gt;5.  Check snapshot chain, make sure &lt;code&gt;backup_snapshot&lt;/code&gt; exists.&lt;br&gt;6.  Delete the &lt;code&gt;backup_snapshot&lt;/code&gt; and purge snapshots.&lt;br&gt;7.  After purge complete, delete one replica to verify rebuild works.&lt;/td&gt;&#xA;          &lt;td&gt;Backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;test_backup_kubernetes_status&lt;/td&gt;&#xA;          &lt;td&gt;Test that Backups have KubernetesStatus stored properly when there is an associated PersistentVolumeClaim and Pod.&lt;br&gt;&lt;br&gt;1.  Setup a random backupstore&lt;br&gt;2.  Set settings Longhorn Static StorageClass to &lt;code&gt;longhorn-static-test&lt;/code&gt;&lt;br&gt;3.  Create a volume and PV/PVC. Verify the StorageClass of PVC&lt;br&gt;4.  Create a Pod using the PVC.&lt;br&gt;5.  Check volume&amp;rsquo;s Kubernetes status to reflect PV/PVC/Pod correctly.&lt;br&gt;6.  Create a backup for the volume.&lt;br&gt;7.  Verify the labels of created backup reflect PV/PVC/Pod status.&lt;br&gt;8.  Restore the backup to a volume. Wait for restoration to complete.&lt;br&gt;9.  Check the volume&amp;rsquo;s Kubernetes Status&lt;br&gt;    1.  Make sure the &lt;code&gt;lastPodRefAt&lt;/code&gt; and &lt;code&gt;lastPVCRefAt&lt;/code&gt; is snapshot created time&lt;br&gt;        &lt;br&gt;10.  Delete the backup and restored volume.&lt;br&gt;11.  Delete PV/PVC/Pod.&lt;br&gt;12.  Verify volume&amp;rsquo;s Kubernetes Status updated to reflect history data.&lt;br&gt;13.  Attach the volume and create another backup. Verify the labels&lt;br&gt;14.  Verify the volume&amp;rsquo;s Kubernetes status.&lt;br&gt;15.  Restore the previous backup to a new volume. Wait for restoration.&lt;br&gt;16.  Verify the restored volume&amp;rsquo;s Kubernetes status.&lt;br&gt;    1.  Make sure &lt;code&gt;lastPodRefAt&lt;/code&gt; and &lt;code&gt;lastPVCRefAt&lt;/code&gt; matched volume on step 12&lt;/td&gt;&#xA;          &lt;td&gt;Backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;test_restore_inc&lt;/td&gt;&#xA;          &lt;td&gt;Test restore from disaster recovery volume (incremental restore)&lt;br&gt;&lt;br&gt;Run test against all the backupstores&lt;br&gt;&lt;br&gt;1.  Create a volume and attach to the current node&lt;br&gt;2.  Generate &lt;code&gt;data0&lt;/code&gt;, write to the volume, make a backup &lt;code&gt;backup0&lt;/code&gt;&lt;br&gt;3.  Create three DR(standby) volumes from the backup: &lt;code&gt;sb_volume0/1/2&lt;/code&gt;&lt;br&gt;4.  Wait for all three DR volumes to finish the initial restoration&lt;br&gt;5.  Verify DR volumes&amp;rsquo;s &lt;code&gt;lastBackup&lt;/code&gt; is &lt;code&gt;backup0&lt;/code&gt;&lt;br&gt;6.  Verify snapshot/pv/pvc/change backup target are not allowed as long as the DR volume exists&lt;br&gt;7.  Activate standby &lt;code&gt;sb_volume0&lt;/code&gt; and attach it to check the volume data&lt;br&gt;8.  Generate &lt;code&gt;data1&lt;/code&gt; and write to the original volume and create &lt;code&gt;backup1&lt;/code&gt;&lt;br&gt;9.  Make sure &lt;code&gt;sb_volume1&lt;/code&gt;&amp;lsquo;s &lt;code&gt;lastBackup&lt;/code&gt; field has been updated to &lt;code&gt;backup1&lt;/code&gt;&lt;br&gt;10.  Wait for &lt;code&gt;sb_volume1&lt;/code&gt; to finish incremental restoration then activate&lt;br&gt;11.  Attach and check &lt;code&gt;sb_volume1&lt;/code&gt;&amp;rsquo;s data&lt;br&gt;12.  Generate &lt;code&gt;data2&lt;/code&gt; and write to the original volume and create &lt;code&gt;backup2&lt;/code&gt;&lt;br&gt;13.  Make sure &lt;code&gt;sb_volume2&lt;/code&gt;&amp;lsquo;s &lt;code&gt;lastBackup&lt;/code&gt; field has been updated to &lt;code&gt;backup1&lt;/code&gt;&lt;br&gt;14.  Wait for &lt;code&gt;sb_volume2&lt;/code&gt; to finish incremental restoration then activate&lt;br&gt;15.  Attach and check &lt;code&gt;sb_volume2&lt;/code&gt;&amp;rsquo;s data&lt;br&gt;16.  Create PV, PVC and Pod to use &lt;code&gt;sb_volume2&lt;/code&gt;, check PV/PVC/POD are good&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Disaster Recovery&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;test_recurring_job&lt;/td&gt;&#xA;          &lt;td&gt;Test recurring job&lt;br&gt;&lt;br&gt;1.  Setup a random backupstore&lt;br&gt;2.  Create a volume.&lt;br&gt;3.  Create two jobs 1 job 1: snapshot every one minute, retain 2 1 job 2: backup every two minutes, retain 1&lt;br&gt;4.  Attach the volume.&lt;br&gt;5.  Sleep for 5 minutes&lt;br&gt;6.  Verify we have 4 snapshots total&lt;br&gt;    1.  2 snapshots, 1 backup, 1 volume-head&lt;br&gt;        &lt;br&gt;7.  Update jobs to replace the backup job&lt;br&gt;    1.  New backup job run every one minute, retain 2&lt;br&gt;        &lt;br&gt;8.  Sleep for 5 minutes.&lt;br&gt;9.  We should have 6 snapshots&lt;br&gt;    1.  2 from job_snap, 1 from job_backup, 2 from job_backup2, 1 volume-head&lt;br&gt;        &lt;br&gt;10.  Make sure we have no more than 5 backups.&lt;br&gt;    1.  old backup job may have at most 1 backups&lt;br&gt;        &lt;br&gt;    2.  new backup job may have at most 3 backups&lt;br&gt;        &lt;br&gt;11.  Make sure we have no more than 2 backups in progress&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Recurring Job&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;test_recurring_job_in_storageclass&lt;/td&gt;&#xA;          &lt;td&gt;Test create volume with StorageClass contains recurring jobs&lt;br&gt;&lt;br&gt;1.  Create a StorageClass with recurring jobs&lt;br&gt;2.  Create a StatefulSet with PVC template and StorageClass&lt;br&gt;3.  Verify the recurring jobs run correctly.&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Recurring Job&lt;br&gt;&lt;br&gt;Kubernetes&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;test_recurring_job_in_volume_creation&lt;/td&gt;&#xA;          &lt;td&gt;Test create volume with recurring jobs&lt;br&gt;&lt;br&gt;1.  Create volume with recurring jobs though Longhorn API&lt;br&gt;2.  Verify the recurring jobs run correctly&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Recurring Job&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;test_recurring_job_kubernetes_status&lt;/td&gt;&#xA;          &lt;td&gt;Test RecurringJob properly backs up the KubernetesStatus&lt;br&gt;&lt;br&gt;1.  Setup a random backupstore.&lt;br&gt;2.  Create a volume.&lt;br&gt;3.  Create a PV from the volume, and verify the PV status.&lt;br&gt;4.  Create a backup recurring job to run every 2 minutes.&lt;br&gt;5.  Verify the recurring job runs correctly.&lt;br&gt;6.  Verify the backup contains the Kubernetes Status labels&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Recurring Job&lt;br&gt;&lt;br&gt;Volume: Kubernetes Status&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;test_recurring_job_labels&lt;/td&gt;&#xA;          &lt;td&gt;Test a RecurringJob with labels&lt;br&gt;&lt;br&gt;1.  Set a random backupstore&lt;br&gt;2.  Create a backup recurring job with labels&lt;br&gt;3.  Verify the recurring jobs runs correctly.&lt;br&gt;4.  Verify the labels on the backup is correct&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Recurring Job&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;test_recurring_jobs_maximum_retain&lt;/td&gt;&#xA;          &lt;td&gt;Test recurring jobs&amp;rsquo; maximum retain&lt;br&gt;&lt;br&gt;1.  Create two jobs, with retain 30 and 21.&lt;br&gt;2.  Try to apply the jobs to a volume. It should fail.&lt;br&gt;3.  Reduce retain to 30 and 20.&lt;br&gt;4.  Now the jobs can be applied the volume&lt;/td&gt;&#xA;          &lt;td&gt;Backup: Recurring Job&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;backup-create-operations-test-cases&#34;&gt;Backup create operations test cases&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Create backup from existing snapshot&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Backup target is set to NFS server, or S3 compatible target.&lt;br&gt;&lt;br&gt;1.  Create a workload using Longhorn volume&lt;br&gt;2.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a snapshot (snapshot#1)&lt;br&gt;4.  Create a backup from (snapshot#1)&lt;br&gt;5.  Restore backup to a different volume&lt;br&gt;6.  Attach volume to a node and check it’s data, and compute it’s checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   Backup should be created&lt;br&gt;*   Restored volume data checksum should match (checksum#1)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Create volume backup for a volume attached to a node&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Backup target is set to NFS server, or S3 compatible target.&lt;br&gt;&lt;br&gt;1.  Create a volume, attach it to a node&lt;br&gt;2.  Format volume using ext4/xfs filesystem and mount it to a directory on the node&lt;br&gt;3.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;4.  Create a backup&lt;br&gt;5.  Restore backup to a different volume&lt;br&gt;6.  Attach volume to a node and check it’s data, and compute it’s checksum&lt;br&gt;7.  Check volume backup labels&lt;/td&gt;&#xA;          &lt;td&gt;*   Backup should be created&lt;br&gt;*   Restored volume data checksum should match (checksum#1)&lt;br&gt;*   backup should have no backup labels&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Create volume backup used by Kubernetes workload&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Backup target is set to NFS server, or S3 compatible target.&lt;br&gt;&lt;br&gt;1.  Create a deployment workload with &lt;code&gt;nReplicas = 1&lt;/code&gt; using Longhorn volume&lt;br&gt;2.  Write data to volume, compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a backup&lt;br&gt;4.  Check backup labels&lt;br&gt;5.  Scale down deployment &lt;code&gt;nReplicas = 0&lt;/code&gt;&lt;br&gt;6.  Delete Longhorn volume&lt;br&gt;7.  Restore backup to a volume with the same deleted volume name&lt;br&gt;8.  Scale back deployment &lt;code&gt;nReplicas = 1&lt;/code&gt;&lt;br&gt;9.  Check volume data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   Backup labels should contain the following informations about workload that was using the volume at time of backup.&lt;br&gt;    *   Namespace&lt;br&gt;        &lt;br&gt;    *   PV Name&lt;br&gt;        &lt;br&gt;    *   PVC Name&lt;br&gt;        &lt;br&gt;    *   PV Status&lt;br&gt;        &lt;br&gt;    *   Workloads Status&lt;br&gt;        &lt;br&gt;        *   Pod Name  &lt;br&gt;            Workload Name  &lt;br&gt;            Workload Type  &lt;br&gt;            Pod Status&lt;br&gt;            &lt;br&gt;*   After volume restore, data checksum should match (checksum#1)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Create volume backup with customized labels&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Backup target is set to NFS server, or S3 compatible target.&lt;br&gt;&lt;br&gt;1.  Create a volume, attach it to a node&lt;br&gt;2.  Create a backup, add customized labels  &lt;br&gt;    key: &lt;code&gt;K1&lt;/code&gt; value: &lt;code&gt;V1&lt;/code&gt;&lt;br&gt;3.  Check volume backup labels&lt;/td&gt;&#xA;          &lt;td&gt;*   Backup should be created with customized labels&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Create recurring backups&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a deployment workload with &lt;code&gt;nReplicas = 1&lt;/code&gt; using Longhorn volume&lt;br&gt;2.  Write data to volume , compute it’s checksum (checksum#1)&lt;br&gt;3.  Create a recurring backup &lt;code&gt;every 5 minutes&lt;/code&gt;. and set retain count to &lt;code&gt;5&lt;/code&gt;&lt;br&gt;4.  add customized labels key: &lt;code&gt;K1&lt;/code&gt; value: &lt;code&gt;V1&lt;/code&gt;&lt;br&gt;5.  Wait for recurring backup to triggered (backup#1, backup#2 )&lt;br&gt;6.  Scale down deployment &lt;code&gt;nReplicas = 0&lt;/code&gt;&lt;br&gt;7.  Delete the volume.&lt;br&gt;8.  Restore backup to a volume with the same deleted volume name&lt;br&gt;9.  Scale back deployment &lt;code&gt;nReplicas = 1&lt;/code&gt;&lt;br&gt;10.  Check volume data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   backups should be created with Kubernetes status labels and customized labels&lt;br&gt;*   After volume restore, data checksum should match (checksum#1)&lt;br&gt;*   after restoring the backup recurring backups should continue to be created&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Backup created using Longhorn behind proxy&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Setup a Proxy on an instance (Optional: use squid)&lt;br&gt;*   Create a single node cluster in EC2&lt;br&gt;*   Deploy Longhorn&lt;br&gt;&lt;br&gt;1.  Block outgoing traffic except for the proxy instance.&lt;br&gt;2.  Create AWS secret in longhorn.&lt;br&gt;3.  In UI Settings page, set backupstore target and backupstore credential secret&lt;br&gt;4.  Create a volume, attach it to a node, format the volume, and mount it to a directory.&lt;br&gt;5.  Write some data to the volume, and create a backup.&lt;/td&gt;&#xA;          &lt;td&gt;*   Ensure backup is created&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Backup created in a backup store supports Virtual Hosted Style&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create an OSS bucket in Alibaba Cloud(Aliyun)&lt;br&gt;2.  Create a secret without &lt;code&gt;VIRTUAL_HOSTED_STYLE&lt;/code&gt; for the OSS bucket.&lt;br&gt;3.  Set backup target and the secret in Longhorn UI.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Backup created in a backup store supports both Virtual Hosted style and traditional&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create an S3 bucket in AWS.&lt;br&gt;2.  Create a secret without &lt;code&gt;VIRTUAL_HOSTED_STYLE&lt;/code&gt; for the S3 bucket.&lt;br&gt;3.  Set backup target and the secret in Longhorn UI.&lt;br&gt;4.  Verify backup list/create/delete/restore work fine without the configuration.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;backup-restore-operations-test-cases&#34;&gt;Backup restore operations test cases&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Filter backup using backup name&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;&lt;br&gt;1.  Filter backups by volume name&lt;/td&gt;&#xA;          &lt;td&gt;*   volumes should be filtered using full/partial volume names&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Restore last backup with different name&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create a Volume, attach it to a node, write some data (300MB+), compute it’s checksum and create a backup (repeat for 3 times).&lt;br&gt;*   Volume now has multiple backups (backup#1, backup#2, backup#3) respectively.&lt;br&gt;&lt;br&gt;1.  Restore latest volume backup using &lt;strong&gt;different&lt;/strong&gt; name than it’s original&lt;br&gt;2.  After restore complete, attach the volume to a node, and check data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   New Volume should be created and attached to a node in maintenance mode&lt;br&gt;*   Restore process should be triggered restoring latest backup content to the volume&lt;br&gt;*   After restore is completed, volume is detached from the node&lt;br&gt;*   data checksum should match data checksum for (backup#3)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Restore specific with different name&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create a Volume, attach it to a node, write some data (300MB+), compute it’s checksum and create a backup (repeat for 3 times).&lt;br&gt;*   Volume now has multiple backups (backup#1, backup#2, backup#3) respectively.&lt;br&gt;&lt;br&gt;1.  Restore restore the second backup (backup#2) using &lt;strong&gt;different&lt;/strong&gt; name than it’s original&lt;br&gt;2.  After restore complete, attach the volume to a node, and check data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   New Volume should be created and attached to a node in maintenance mode&lt;br&gt;*   Restore process should be triggered restoring latest backup content to the volume&lt;br&gt;*   After restore is completed, volume is detached from the node&lt;br&gt;*   data checksum should match data checksum for (backup#2)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Volume backup URL&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;&lt;br&gt;1.  get backup URL&lt;/td&gt;&#xA;          &lt;td&gt;*   Backup URL should point to a link to backup on configured backupstore&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Restore backup with different number of replicas&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;&lt;br&gt;1.  Restore a backup and set different number of replicas&lt;/td&gt;&#xA;          &lt;td&gt;*   Restored volume replica count should match the number in restore backup request&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Restore backup with Different Node tags&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;*   Longhorn Nodes should have Node Tags&lt;br&gt;&lt;br&gt;1.  Restore a backup and set node tags&lt;/td&gt;&#xA;          &lt;td&gt;*   Restored volume replicas should scheduled only to nodes have Node Tags match Tags specified in restore backup request&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Restore backup with Different Disk Tags&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;*   Longhorn Nodes Disks should have Disk Tags&lt;br&gt;&lt;br&gt;1.  Restore a backup and set disk tags&lt;/td&gt;&#xA;          &lt;td&gt;*   Restored volume replicas should scheduled only to disks have Disk Tags match Tags specified in restore backup request&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Restore backup with both Node and Disk Tags&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;*   Longhorn Nodes should have Node Tags&lt;br&gt;*   Longhorn Nodes Disks should have Disk Tags&lt;br&gt;&lt;br&gt;1.  Restore a backup and set both Node and Disk tags&lt;/td&gt;&#xA;          &lt;td&gt;*   Restored volume replicas should scheduled only to nodes that have both Node and Disk tags specified in restore backup request.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Restore last backup with same previous name (Volume already exists)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create a Volume, attach it to a node, write some data (300MB+), compute it’s checksum and create a backup (repeat for 3 times).&lt;br&gt;*   Volume now has multiple backups (backup#1, backup#2, backup#3) respectively.&lt;br&gt;&lt;br&gt;1.  Restore latest volume backup using &lt;strong&gt;same&lt;/strong&gt; original volume name&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume can’t be restored&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Restore last backup with same previous name&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create a Volume, attach it to a node, write some data (300MB+), compute it’s checksum and create a backup (repeat for 3 times).&lt;br&gt;*   Volume now has multiple backups (backup#1, backup#2, backup#3) respectively.&lt;br&gt;*   Detach and delete volume&lt;br&gt;&lt;br&gt;1.  Restore latest volume backup using &lt;strong&gt;same&lt;/strong&gt; original volume name&lt;br&gt;2.  After restore complete, attach the volume to a node, and check data checksum&lt;/td&gt;&#xA;          &lt;td&gt;*   New Volume with same old name should be created and attached to a node in maintenance mode&lt;br&gt;*   Restore process should be triggered restoring latest backup content to the volume&lt;br&gt;*   After restore is completed, volume is detached from the node&lt;br&gt;*   data checksum should match data checksum for (backup#3)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;Restore volume used by Kubernetes workload with same previous name&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create a deployment workload using a Longhorn volume, write some data (300MB+), compute it’s checksum and create a backup (repeat for 3 times).&lt;br&gt;*   Volume now has multiple backups (backup#1, backup#2, backup#3) respectively.&lt;br&gt;*   Scale down the deployment to zero&lt;br&gt;*   Delete volume&lt;br&gt;&lt;br&gt;1.  Restore latest volume backup using &lt;strong&gt;same&lt;/strong&gt; original volume name&lt;br&gt;2.  After restore complete, scale up the deployment&lt;/td&gt;&#xA;          &lt;td&gt;*   New Volume with same old name should be created and attached to a node in maintenance mode&lt;br&gt;*   Restore process should be triggered restoring latest backup content to the volume&lt;br&gt;*   After restore is completed, volume is detached from the node&lt;br&gt;*   Old &lt;code&gt;PV/PVC , Namespace &amp;amp; Attached To&lt;/code&gt; information should be restored&lt;br&gt;*   Volume should be accessible from the deployment pod&lt;br&gt;*   Data checksum should match data checksum for (backup#3)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;Restore volume used by Kubernetes workload with different name&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create a deployment workload using a Longhorn volume, write some data (300MB+), compute it’s checksum and create a backup (repeat for 3 times).&lt;br&gt;*   Volume now has multiple backups (backup#1, backup#2, backup#3) respectively.&lt;br&gt;*   Scale down the deployment to zero&lt;br&gt;*   Delete volume&lt;br&gt;&lt;br&gt;1.  Restore latest volume backup using &lt;strong&gt;different&lt;/strong&gt; name than its original&lt;br&gt;2.  After restore complete&lt;br&gt;    1.  Delete old PVC&lt;br&gt;        &lt;br&gt;    2.  Create a new PV for volume&lt;br&gt;        &lt;br&gt;    3.  Create a new PVC with same old PVC name&lt;br&gt;        &lt;br&gt;3.  scale up the deployment&lt;/td&gt;&#xA;          &lt;td&gt;*   New Volume with same old name should be created and attached to a node in maintenance mode&lt;br&gt;*   Restore process should be triggered restoring latest backup content to the volume&lt;br&gt;*   After restore is completed, volume is detached from the node&lt;br&gt;*   Old &lt;code&gt;Namespace &amp;amp; Attached To&lt;/code&gt; information should be restored&lt;br&gt;*   &lt;code&gt;PV/PVC&lt;/code&gt; information should be empty after restore completed  &lt;br&gt;    old PV &lt;code&gt;spec.csi.volumeHandle&lt;/code&gt;will not match the new volume name&lt;br&gt;*   After New PV/PVC is created, deployment pod should be able to claim the new PVC and access volume with new name.&lt;br&gt;*   Data checksum should match data checksum for (backup#3)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;Restore last backup (batch operation)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;&lt;br&gt;1.  select multiple volumes, restore the latest backup for all of them&lt;/td&gt;&#xA;          &lt;td&gt;*   New volumes with same old volume names should be created, attached to nodes and restore process should be triggered&lt;br&gt;*   &lt;code&gt;PV/PVC&lt;/code&gt; information should be restored for volumes that had PV/PVC created&lt;br&gt;*   &lt;code&gt;Namespace &amp;amp; Attached To&lt;/code&gt; information should be restored for volumes that had been used by kubnernetes workload at the time of backup&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;14&lt;/td&gt;&#xA;          &lt;td&gt;Delete All Volume Backups&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   One or more backup is created for multiple volume.&lt;br&gt;&lt;br&gt;1.  Delete All backups for a volume&lt;br&gt;2.  Check backupstore, and confirm backups has been deleted&lt;/td&gt;&#xA;          &lt;td&gt;*   Backups should not be delete from Longhorn UI, and also from backupstore.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;15&lt;/td&gt;&#xA;          &lt;td&gt;Restore backup created using Longhorn behind proxy.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Setup a Proxy on an instance (Optional: use squid)&lt;br&gt;*   Create a single node cluster in EC2&lt;br&gt;*   Deploy Longhorn&lt;br&gt;&lt;br&gt;1.  Block outgoing traffic except for the proxy instance.&lt;br&gt;2.  Create AWS secret in longhorn as follows:&lt;br&gt;3.  In UI Settings page, set backupstore target and backupstore credential secret&lt;br&gt;4.  Create a volume, attach it to a node, format the volume, and mount it to a directory.&lt;br&gt;5.  Write some data to the volume, and create a backup.&lt;br&gt;6.  Wait for backup to complete, and the try to restore the backup to a volume with different name.&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume should get restored successfully.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;disaster-recovery-test-cases&#34;&gt;Disaster Recovery test cases&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Tests Prerequisite&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>7. Node</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/node/</guid>
      <description>&lt;h2 id=&#34;ui-specific-test-cases&#34;&gt;&lt;strong&gt;UI specific test cases&lt;/strong&gt;&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Storage details&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;    *   Longhorn Installed&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Verify the allocated/used storage show the right data in node details page.&lt;br&gt;2.  Create a volume of 20 GB and attach to a pod and verify the storage allocated/used is shown correctly.&lt;/td&gt;&#xA;          &lt;td&gt;Without any volume, allocated should be 0 and on creating new volume it should be updated as per volume present.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Filters applied to node list&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;    *   Longhorn Installed&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  In longhorn UI node tab- Change the filter based on name/status etc. Verify the nodes are appearing properly.&lt;/td&gt;&#xA;          &lt;td&gt;Nodes satisfying filter should only get displayed on page.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Sort the nodes view&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;    *   Longhorn Installed&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  In longhorn UI node tab- Click title to sort the nodes appearing based on status/name etc&lt;/td&gt;&#xA;          &lt;td&gt;Nodes list should get sorted ascending/descending based on status/name stc&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Expand All&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;    *   Longhorn Installed&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  In longhorn UI node tab- Click ‘expand all’ button.&lt;/td&gt;&#xA;          &lt;td&gt;All nodes should get expanded and show disks details&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;additional-tests-for-ui&#34;&gt;Additional Tests for UI&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Scenario&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Readiness column&lt;/td&gt;&#xA;          &lt;td&gt;1.  Click On a node’s readiness state say “Ready”&lt;br&gt;2.  Verify components window opens&lt;br&gt;3.  Verify Engine image and instance manager details are seen&lt;/td&gt;&#xA;          &lt;td&gt;Components window should open with details of Engine image and instance manager&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Replicas column&lt;/td&gt;&#xA;          &lt;td&gt;1.  Click on the count (number of replicas) for a node in the replica column&lt;br&gt;2.  Verify the list of replicas on the node is available&lt;br&gt;3.  Verify user is able to delete a replica on the node by selecting the replica and clicking on delete&lt;br&gt;4.  Verify the replica is deleted by navigating to the specific Volume → Volume details page → replicas&lt;/td&gt;&#xA;          &lt;td&gt;*   User should be able to view all the replicas on the node&lt;br&gt;*   User should be able to delete replica on the node&lt;br&gt;*   User should be able to see the replica deleted in the volume → Volume details page → replicas page&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;test-cases&#34;&gt;Test cases&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Instructions&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Node scheduling&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;    *   Longhorn Deployed with 3 nodes&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Disable Node Scheduling on a node&lt;br&gt;2.  Create a volume with 3 replicas, and attach it to a node&lt;br&gt;3.  Re-enabled node scheduling on the node&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume should be created and attached&lt;br&gt;*   Volume replicas should be scheduled to Schedulable nodes only&lt;br&gt;*   Re-enabling node scheduling will not affect existing scheduled replicas, it will only affect new replicas being created, or rebuilt.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Disk Scheduling&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;    *   Longhorn Deployed with 3 nodes&lt;br&gt;        &lt;br&gt;    *   Add additional disk (Disk#1) ,attach it and mounted to Node-01.&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Create a New Disk, Keep Disk Scheduling disabled&lt;br&gt;2.  Create a volume (vol#1), set replica count to &lt;code&gt;4&lt;/code&gt; and attach it to a node&lt;br&gt;3.  Check (vol#1) replica paths&lt;br&gt;4.  Enable Scheduling on (disk#1)&lt;br&gt;5.  Create a volume (vol#2), set replica count to &lt;code&gt;4&lt;/code&gt; and attach it to a node&lt;br&gt;6.  Check (vol#2) replica paths&lt;/td&gt;&#xA;          &lt;td&gt;*   (vol#1) replicas should be scheduled only to Disks withe Scheduling enabled, no replicas should be scheduled to (disk#1)&lt;br&gt;*   One of (vol#2) replica paths will be scheduled to (disk#1)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Volume Created with Node Tags&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;    *   Longhorn Deployed with 3 nodes&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Create Node tags as follows:&lt;br&gt;    1.  Node-01: fast&lt;br&gt;        &lt;br&gt;    2.  Node-02: slow&lt;br&gt;        &lt;br&gt;    3.  Node-02: fast&lt;br&gt;        &lt;br&gt;2.  Create a volume (vol#1), set Node tags to slow&lt;br&gt;3.  Create a volume (vol#2), set Node tags to fast&lt;br&gt;4.  Check Volumes replicas paths&lt;br&gt;5.  Check Volume detail &lt;code&gt;Node Tags&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;*   vol#1 replicas should only be scheduled to Node-02&lt;br&gt;*   vol#2 replicas should only be scheduled to Node-01 and Node-03&lt;br&gt;*   Node Tag volume detail should contain Node tag specified in volume creation request.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Volumes created with Disk Tags&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;    *   Longhorn Deployed with 3 nodes, with default disks (disk#01-1, disk#02-1, disk#03-1)&lt;br&gt;        &lt;br&gt;    *   &lt;code&gt;disk#0X-Y&lt;/code&gt; indicate that disk is attached to &lt;code&gt;Node-0X&lt;/code&gt; , and it is disk number &lt;code&gt;Y&lt;/code&gt; on that node.&lt;br&gt;        &lt;br&gt;*   Create 3 additional disks (disk#01-2, disk#02-2, disk#03-2), attach each one to a different node, and mount it to a directory on that node.&lt;br&gt;&lt;br&gt;1.  Create Disk tags as follows:&lt;br&gt;    1.  disk#01-1: fast&lt;br&gt;        &lt;br&gt;    2.  disk#01-2: fast&lt;br&gt;        &lt;br&gt;    3.  disk#02-1: slow&lt;br&gt;        &lt;br&gt;    4.  disk#02-2: slow&lt;br&gt;        &lt;br&gt;    5.  disk#03-1: fast&lt;br&gt;        &lt;br&gt;    6.  disk#01-2: fast&lt;br&gt;        &lt;br&gt;2.  Create a volume (vol#1), set Disk tags to slow&lt;br&gt;3.  Create a volume (vol#2), set Disk tags to fast&lt;br&gt;4.  Check Volumes replicas paths&lt;br&gt;5.  Check Volume detail &lt;code&gt;Disk Tags&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;*   vol#1 replicas should only be scheduled to disks have slow tag (disk#02-1 and disk#02-2)&lt;br&gt;*   vol#2 replicas should can be scheduled to disks have fast Tag  &lt;br&gt;    (disk#01-1, disk#01-2, disk#03-1, disk#03-2)&lt;br&gt;*   Disk Tag volume detail should contain Disk tag specified in volume creation request.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Volumes created with both DIsk and Node Tags&lt;/td&gt;&#xA;          &lt;td&gt;*   Create a volume, set Disk and node tags, and attach it to a node&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume replicas should be scheduled only to node that have Node tags, and only on disks that have Disk tags specified on volume creation request&lt;br&gt;*   If No Node match both Node and Disk tags, volume replicas will not be created.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Remove Disk From Node&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;    *   Longhorn Deployed with 3 nodes&lt;br&gt;        &lt;br&gt;    *   Add additional disk (Disk#1) ,attach it and mounted to Node-01.&lt;br&gt;        &lt;br&gt;    *   Some replicas should be scheduled to Disk#1&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Disable Scheduling on disk#1&lt;br&gt;2.  Delete all replicas scheduled to disk#1, replicas should start to rebuild on other disks&lt;br&gt;3.  Delete disk from node&lt;/td&gt;&#xA;          &lt;td&gt;*   Stopping Disk scheduling will prevent replicas to be scheduled on it&lt;br&gt;*   Disk can’t be deleted if at least one replicas is still scheduled to it.&lt;br&gt;*   Disk can be delete only after all replica have been rescheduled to other disks.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Power off a node&lt;/td&gt;&#xA;          &lt;td&gt;1.  Power off a node&lt;/td&gt;&#xA;          &lt;td&gt;*   Node should report down on Node page&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Delete Longhorn Node&lt;/td&gt;&#xA;          &lt;td&gt;1.  Disable Scheduling on the node&lt;br&gt;2.  Delete all replicas on the node to be rescheduled to another nodes&lt;br&gt;3.  Detach all volume attached to the node, re-attach them on other nodes&lt;br&gt;4.  Delete Node from Kubernetes&lt;br&gt;5.  Delete Node From Longhorn&lt;/td&gt;&#xA;          &lt;td&gt;*   Node can’t be deleted if Node Scheduling is enabled on that node&lt;br&gt;*   Node can’t be deleted unless it all replicas are deleted from that node&lt;br&gt;*   Node can’t be deleted unless it all attached volumes get detached from that node&lt;br&gt;*   Node can’t be deleted unless it has been deleted from Kubernetes first&lt;br&gt;*   After node is deleted from Kubernetes, node should report down on Longhorn&lt;br&gt;*   Node should be deleted from Longhorn&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Default Disk on Labeled Nodes&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;    *   Create 3 node k8s cluster&lt;br&gt;        &lt;br&gt;    *   Create &lt;code&gt;/home/longhorn&lt;/code&gt; directory on all 3 nodes&lt;br&gt;        &lt;br&gt;    *   Add new disk to each node, format it with &lt;code&gt;ext4&lt;/code&gt;, and mount it to &lt;code&gt;/mnt/disk&lt;/code&gt;&lt;br&gt;        &lt;br&gt;*   Use the following label and annotations for nodes&lt;br&gt;&lt;br&gt;Node-01 &amp;amp; Node-03&lt;br&gt;&lt;br&gt;&lt;code&gt;labels:&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;node.longhorn.io/create-``default``-disk:&lt;/code&gt; &lt;code&gt;&amp;quot;config&amp;quot;&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;annotations:&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;node.longhorn.io/``default``-disks-config:&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;&#39;[{``&amp;quot;path&amp;quot;``:``&amp;quot;/home/longhorn&amp;quot;``,``&amp;quot;allowScheduling&amp;quot;``:``true``,&lt;/code&gt; &lt;code&gt;&amp;quot;tags&amp;quot;``:[``&amp;quot;ssd&amp;quot;``,&lt;/code&gt; &lt;code&gt;&amp;quot;fast&amp;quot;``]},&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;{``&amp;quot;path&amp;quot;``:``&amp;quot;/mnt/disk&amp;quot;``,``&amp;quot;allowScheduling&amp;quot;``:``true``,``&amp;quot;storageReserved&amp;quot;``:``1024``,``&amp;quot;tags&amp;quot;``:[``&amp;quot;ssd&amp;quot;``,``&amp;quot;fast&amp;quot;``]}]&#39;&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;node.longhorn.io/``default``-node-tags:&lt;/code&gt; &lt;code&gt;&#39;[&amp;quot;fast&amp;quot;, &amp;quot;storage&amp;quot;]&#39;&lt;/code&gt;&lt;br&gt;&lt;br&gt;Node-02&lt;br&gt;&lt;br&gt; &lt;code&gt;labels:&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;node.longhorn.io/create-``default``-disk:&lt;/code&gt; &lt;code&gt;&amp;quot;config&amp;quot;&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;annotations:&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;node.longhorn.io/``default``-disks-config:&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;&#39;[{``&amp;quot;path&amp;quot;``:``&amp;quot;/home/longhorn&amp;quot;``,``&amp;quot;allowScheduling&amp;quot;``:``true``,&lt;/code&gt; &lt;code&gt;&amp;quot;tags&amp;quot;``:[``&amp;quot;hdd&amp;quot;``,&lt;/code&gt; &lt;code&gt;&amp;quot;slow&amp;quot;``]},&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;{``&amp;quot;path&amp;quot;``:``&amp;quot;/mnt/disk&amp;quot;``,``&amp;quot;allowScheduling&amp;quot;``:``true``,``&amp;quot;storageReserved&amp;quot;``:``1024``,``&amp;quot;tags&amp;quot;``:[``&amp;quot;hdd&amp;quot;``,``&amp;quot;slow&amp;quot;``]}]&#39;&lt;/code&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;node.longhorn.io/``default``-node-tags:&lt;/code&gt; &lt;code&gt;&#39;[&amp;quot;slow&amp;quot;, &amp;quot;storage&amp;quot;]&#39;&lt;/code&gt; &lt;br&gt;&lt;br&gt;1.  Set &lt;code&gt;create-default-disk-labeled-nodes: True&lt;/code&gt; in &lt;code&gt;longhorn-default-setting&lt;/code&gt; config map&lt;br&gt;2.  Deploy Longhorn&lt;/td&gt;&#xA;          &lt;td&gt;*   Longhorn Should be deployed successfully&lt;br&gt;*   Node-01 &amp;amp; Node-03&lt;br&gt;    *   should be tagged with &lt;code&gt;fast&lt;/code&gt; and &lt;code&gt;storage&lt;/code&gt; tags&lt;br&gt;        &lt;br&gt;    *   Disk scheduling should be allowed on both disks&lt;br&gt;        &lt;br&gt;    *   Disks should be tagged with &lt;code&gt;ssd&lt;/code&gt; and &lt;code&gt;fast&lt;/code&gt; tags&lt;br&gt;        &lt;br&gt;    *   1024 MB is reserved storage on &lt;code&gt;/mnt/disk&lt;/code&gt;&lt;br&gt;        &lt;br&gt;*   Node-02&lt;br&gt;    *   should be tagged with &lt;code&gt;Slow&lt;/code&gt; and &lt;code&gt;storage&lt;/code&gt; tags&lt;br&gt;        &lt;br&gt;    *   should be tagged with &lt;code&gt;slow&lt;/code&gt; and &lt;code&gt;storage&lt;/code&gt; tags&lt;br&gt;        &lt;br&gt;    *   Disk scheduling should be allowed on both disks&lt;br&gt;        &lt;br&gt;    *   Disks should be tagged with &lt;code&gt;hdd&lt;/code&gt; and &lt;code&gt;slow&lt;/code&gt; tags&lt;br&gt;        &lt;br&gt;    *   1024 MB is reserved storage on &lt;code&gt;/mnt/disk&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Default Data Path&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create 3 node k8s cluster&lt;br&gt;*   Create &lt;code&gt;/home/longhorn&lt;/code&gt; directory on all 3 nodes&lt;br&gt;&lt;br&gt;  &lt;br&gt;&lt;br&gt;1.  Set &lt;code&gt;defaultDataPath&lt;/code&gt; to &lt;code&gt;/home/longhorn/&lt;/code&gt; in &lt;code&gt;longhorn-default-setting&lt;/code&gt; ConfigMap&lt;br&gt;2.  Deploy Longhorn&lt;br&gt;3.  Create a volume, attach it to a node&lt;/td&gt;&#xA;          &lt;td&gt;*   In Longhorn Setting, &lt;code&gt;Default Data Path&lt;/code&gt; should be &lt;code&gt;/home/longhorn&lt;/code&gt;&lt;br&gt;*   All volumes replicas paths should begin with &lt;code&gt;/home/longhorn&lt;/code&gt; prefix&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;Update Taint Toleration Setting&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   All Longhorn volumes should be detached then Longhorn components will be restarted to apply new tolerations.&lt;br&gt;*   Notice that &amp;ldquo;&lt;a href=&#34;http://kubernetes.io&#34;&gt;kubernetes.io&lt;/a&gt;&amp;rdquo; is used as the key of all Kubernetes default tolerations, please do not contain this substring in your toleration setting.&lt;br&gt;*   In Longhorn taint tolerations are column separated&lt;br&gt;&lt;br&gt;1.  Using Kubernetes, taint &lt;strong&gt;Some&lt;/strong&gt; nodes  &lt;br&gt;    For example, &lt;code&gt;key1=value1:NoSchedule&lt;/code&gt; &lt;code&gt;key2=value2:NoExecute&lt;/code&gt;&lt;br&gt;2.  Update Taint Toleration Setting with &lt;code&gt;key1=value1:NoSchedule;key2:NoExecute&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;*   Longhorn Components will be restarted&lt;br&gt;*   Longhorn Components should be rescheduled to tainted nodes nodes only.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;Default Taint Toleration Setting&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;&lt;br&gt;*   Create 3 node k8s cluster&lt;br&gt;*   Using Kubernetes, taint &lt;strong&gt;Some&lt;/strong&gt; nodes  &lt;br&gt;    For example, &lt;code&gt;key1=value1:NoSchedule&lt;/code&gt; &lt;code&gt;key2=value2:NoExecute&lt;/code&gt;&lt;br&gt;&lt;br&gt;1.  Set &lt;code&gt;taint-toleration&lt;/code&gt; to &lt;code&gt;key1=value1:NoSchedule;key2:NoExecute&lt;/code&gt; in &lt;code&gt;longhorn-default-setting&lt;/code&gt; ConfigMap&lt;br&gt;2.  Deploy Longhorn&lt;/td&gt;&#xA;          &lt;td&gt;*   Longhorn components should be only deployed to tainted nodes&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;Node Readiness&lt;/td&gt;&#xA;          &lt;td&gt;*   Delete the following Longhorn components pods&lt;br&gt;    *   Engine image&lt;br&gt;        &lt;br&gt;    *   Instance Manager (engine)&lt;br&gt;        &lt;br&gt;    *   Instance Manager (replica)&lt;/td&gt;&#xA;          &lt;td&gt;*   Deleting any components should be reflected on node Readiness&lt;br&gt;*   Deleted component must be redeployed&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;14&lt;/td&gt;&#xA;          &lt;td&gt;Storage Minimal Available Percentage Setting&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;    *   Longhorn Installed&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Change Storage Minimal Available Percentage to &lt;code&gt;50%&lt;/code&gt;&lt;br&gt;2.  Fill up node disk up to 55% of it’s capacity&lt;/td&gt;&#xA;          &lt;td&gt;*   Storage Minimal Available Percentage &lt;strong&gt;default value&lt;/strong&gt; is &lt;code&gt;25%&lt;/code&gt;&lt;br&gt;*   Filled Disk Should be &lt;code&gt;Unschedulable&lt;/code&gt;&lt;br&gt;*   If Node has only one disk, Node also should be &lt;code&gt;Unschedulable&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;15&lt;/td&gt;&#xA;          &lt;td&gt;Storage Over Provisioning Percentage&lt;/td&gt;&#xA;          &lt;td&gt;*   &lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;br&gt;    *   Longhorn Installed&lt;br&gt;        &lt;br&gt;    *   Assume Nodes has disks with 100GB size&lt;br&gt;        &lt;br&gt;&lt;br&gt;1.  Change &lt;code&gt;Storage Over Provisioning Percentage&lt;/code&gt; to &lt;code&gt;300&lt;/code&gt;&lt;br&gt;2.  Check Node Disks available size for allocation&lt;/td&gt;&#xA;          &lt;td&gt;*   Storage Over Provisioning Percentage default value is &lt;code&gt;200&lt;/code&gt;&lt;br&gt;*   Disk storage that can be allocated relative to the hard drive&amp;rsquo;s capacity should be 3x disk size == &lt;code&gt;300 GB&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;additional-tests&#34;&gt;Additional Tests&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Scenario&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Create Default Disk on Labeled Nodes - True&lt;/td&gt;&#xA;          &lt;td&gt;1.  In Longhorn Setting - set Create Default Disk on Labeled Nodes - True&lt;br&gt;2.  Scale up the number of worker nodes in Rancher&lt;br&gt;3.  The node is displayed in Longhorn UI when it comes up “Active” in rancher.&lt;br&gt;4.  Verify the node’s status is “Disabled”&lt;br&gt;5.  Verify the default disk is NOT created in → Node → Edit Node and Disk&lt;br&gt;6.  Add label &lt;code&gt;node.longhorn.io/create-default-disk=true&lt;/code&gt; on the node&lt;br&gt;7.  Verify the node is seen as “Schedulable” on longhorn UI. Verify Node → Edit Node and Disk, the default disk is created&lt;br&gt;8.  Add a node tag to this node n1&lt;br&gt;9.  Create a volume - volume-1, add node tag n1&lt;br&gt;10.  Attach it to the same node&lt;br&gt;11.  Verify the replica is running successfully.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create Default Disk on Labeled Nodes should be set to True&lt;br&gt;2.  When a new node is added, the node should show up as disabled on Longhorn UI&lt;br&gt;3.  The node should NOT have any default disk&lt;br&gt;4.  Label should be added on the node.&lt;br&gt;5.  The node status changes to “Schedulable” and default disk is created on the node.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;nodedisk-eviction-test-cases&#34;&gt;Node/Disk Eviction Test cases&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Scenario&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Steps&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Attached volume replica evict from node&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;1.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Enable soft anti-affinity to True.&lt;br&gt;4.  Evict replicas from one node&lt;br&gt;5.  Verify the data after the replica is evited.&lt;/td&gt;&#xA;          &lt;td&gt;1.  A replica should be rebuilt in any other node except the evicted node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Interrupt the rebuild after the eviction of replica from node&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;1.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Enable soft anti-affinity to True.&lt;br&gt;4.  Evict replicas from one node&lt;br&gt;5.  When the replica is rebuilding, delete it.&lt;/td&gt;&#xA;          &lt;td&gt;1.  A replica should be rebuilt in any other node except the evicted node.&lt;br&gt;2.  On the deletion of the replica, the system should start to rebuild a new replica.&lt;br&gt;3.  The replica from the evicted node will be removed.&lt;br&gt;4.  Data should be intact.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Node evicted while restore rebuilding is in progress&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;1.  Restore a volume (3 replicas).&lt;br&gt;2.  While it is restoring, evict replicas from one node.&lt;br&gt;3.  When the replica is rebuilding, delete it.&lt;br&gt;1.  The rebuilding replica should be completed and then a new replica should get created on another node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Evict node with multiple replicas of the same volume&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;1.  Enable soft anti-affinity to True.&lt;br&gt;2.  Create a volume (3 replicas) and make sure two replicas exist on the same node.&lt;br&gt;3.  Write data to it and compute md5sum.&lt;br&gt;4.  Evict replicas from the node where the two replicas exist.&lt;br&gt;5.  Verify the data after the replicas are evicted.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Two replicas should be rebuilt in any other node except the evicted node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Evict node with soft anti-affinity as false&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;1.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Evict replicas from one node&lt;br&gt;4.  Verify the data after the replica is evited.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Eviction should stuck.&lt;br&gt;2.  Volume scheduling should fail.&lt;br&gt;3.  There should be logs like &lt;pre&gt;&lt;code&gt;[longhorn-manager-dkn9c] time=``&amp;quot;2020-09-09T00:23:56Z&amp;quot;&lt;/code&gt; &lt;code&gt;level=debug msg=``&amp;quot;Creating one more replica for eviction&amp;quot;&lt;/code&gt;&lt;br&gt;&lt;code&gt;[longhorn-manager-dkn9c] time=``&amp;quot;2020-09-09T00:23:56Z&amp;quot;&lt;/code&gt; &lt;code&gt;level=error msg=``&amp;quot;There&#39;s no available disk for replica vol-1-r-6268393a, size 2147483648&amp;quot;&lt;/code&gt;&lt;br&gt;&lt;code&gt;[longhorn-manager-dkn9c] time=``&amp;quot;2020-09-09T00:23:56Z&amp;quot;&lt;/code&gt; &lt;code&gt;level=error msg=``&amp;quot;unable to schedule replica vol-1-r-6268393a of volume vol-1&amp;quot;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Add node after evicting node with soft anti-affinity as false&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;1.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Evict replicas from one node&lt;br&gt;4.  Add a worker node to the cluster.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Eviction should stuck but recover after the additional node is available for scheduling.&lt;br&gt;2.  Volume scheduling should fail initially but should be successful once the additional node is available.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Multi operation&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Enable soft anti-affinity to True.&lt;br&gt;4.  Select two nodes and evict replicas from them.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Replica eviction should happen one by one from nodes&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Replica eviction from disk with soft anti-affinity as True.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Add an additional disk to a node.&lt;br&gt;2.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;3.  Write data to it and compute md5sum.&lt;br&gt;4.  Enable soft anti-affinity to True.&lt;br&gt;5.  Evict from the additional disk.&lt;/td&gt;&#xA;          &lt;td&gt;1.  A replica should be rebuilt in any other node except the evicted node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Replica eviction from disk with soft anti-affinity as False.&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Add an additional disk to a node.&lt;br&gt;2.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;3.  Write data to it and compute md5sum.&lt;br&gt;4.  Enable soft anti-affinity to True.&lt;br&gt;5.  Evict from the additional disk.&lt;/td&gt;&#xA;          &lt;td&gt;1.  A replica should be rebuilt in another disk on the same node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Interrupt eviction&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a volume (3 replicas), attach it to a pod.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Enable soft anti-affinity to True.&lt;br&gt;4.  Evict replicas from one node&lt;br&gt;5.  Stop eviction.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The replicas should evict one by one and once the eviction&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;Evict replica of volume with DR volume&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a DR volume (3 replicas)&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Enable soft anti-affinity to True.&lt;br&gt;4.  Evict replicas from one node&lt;br&gt;5.  Verify the data after the replica is evited.&lt;/td&gt;&#xA;          &lt;td&gt;1.  A replica should be rebuilt in any other node except the evicted node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;Evict replica of volume with the restored volume&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Restore a volume (3 replicas) from backup, attach it to a pod.&lt;br&gt;2.  Enable soft anti-affinity to True.&lt;br&gt;3.  Evict replicas from one node&lt;br&gt;4.  Verify the data after the replica is evited.&lt;/td&gt;&#xA;          &lt;td&gt;1.  A replica should be rebuilt in any other node except the evicted node.&lt;br&gt;2.  The replica from the evicted node should be removed.&lt;br&gt;3.  Data should be intact.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;Evict replica of volume with the detached volume&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn installed in set up of 3 worker nodes and 1 etc/control plane&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Restore a volume (3 replicas) from backup.&lt;br&gt;2.  Write data to it and compute md5sum.&lt;br&gt;3.  Enable soft anti-affinity to True.&lt;br&gt;4.  Evict replicas from one node&lt;br&gt;5.  Verify the data after the replica is evited.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The volume should get attached to a node.&lt;br&gt;2.  A replica should be rebuilt in any other node except the evicted node.&lt;br&gt;3.  The replica from the evicted node should be removed.&lt;br&gt;4.  The volume should get detached.&lt;br&gt;5.  Data should be intact.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;14&lt;/td&gt;&#xA;          &lt;td&gt;On upgraded setup&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Longhorn v1.0.2 installed in set up of 4 worker nodes and 1 etc/control plane&lt;br&gt;2.  Create a volume, restored volume and DR volume&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Upgrade the longhorn to master.&lt;br&gt;2.  Create a volume and attach it to a pod.&lt;br&gt;3.  Write data to it and compute md5sum.&lt;br&gt;4.  Evict replicas from one node&lt;br&gt;5.  Verify the data after the replica is evited.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The replicas of volumes with v1.0.2 engine should also get evicted except the DR volume.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>8. Scheduling</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/scheduling/</guid>
      <description>&lt;h2 id=&#34;manual-test&#34;&gt;Manual Test&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test name&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expectation&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;EKS across zone scheduling&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;:&lt;br&gt;&lt;br&gt;*   EKS Cluster with 3 nodes across two AWS zones (zone#1, zone#2)&lt;br&gt;&lt;br&gt;1.  Create a volume with 2 replicas, and attach it to a node.&lt;br&gt;2.  Delete a replica scheduled to each zone, repeat it few times&lt;br&gt;3.  Scale volume replicas = 3&lt;br&gt;4.  Scale volume replicas to 4&lt;/td&gt;&#xA;          &lt;td&gt;*   Volume replicas should be scheduled one per AWS zone&lt;br&gt;*   Deleting a replica in a zone should trigger a replica rebuild&lt;br&gt;*   new rebuilding replica should be scheduled to the same zone as the deleted replica&lt;br&gt;*   Scaling volume replicas to 3 will distribute replicas across all nodes&lt;br&gt;*   Scaling volume replicas to 4 will be governed by soft anti-affinity rule, so no guarantee on which node the new replica should be scheduled.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;anti-affinity-test&#34;&gt;Anti-affinity test&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test case&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expectation&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Automation test case&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Replica scheduling (soft anti-affinity enabled)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;*   &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt; setting is &lt;strong&gt;Enabled&lt;/strong&gt;&lt;br&gt;1.  Create a volume&lt;br&gt;2.  Attach volume to a node&lt;br&gt;3.  Increase replica count to exceed the number of Longhorn node count&lt;/td&gt;&#xA;          &lt;td&gt;*   New replicas will be scheduled to node&lt;br&gt;*   Volume Status will be &lt;code&gt;Healthy&lt;/code&gt;, with limited node redundancy hint icon&lt;br&gt;&lt;code&gt;Limited node redundancy: at least one healthy replica is running at the same node as another&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;test_soft_anti_affinity_scheduling&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Replica scheduling (soft anti-affinity disabled)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt;&lt;br&gt;*   &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt; setting is &lt;strong&gt;Enabled&lt;/strong&gt;&lt;br&gt;1.  Create a volume&lt;br&gt;2.  Attach volume to a node&lt;br&gt;3.  Increase replica count to exceed the number of Longhorn node count&lt;br&gt;4.  Disable &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt; setting&lt;br&gt;5.  Delete a replica&lt;br&gt;6.  Re-Enable &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt; setting&lt;/td&gt;&#xA;          &lt;td&gt;*   Replicas won’t be removed after disabling &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt;&lt;br&gt;*   when &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt; setting is disabled New Replicas will not be scheduled to nodes.&lt;br&gt;*   when &lt;strong&gt;Replica Soft Anti-Affinity&lt;/strong&gt; setting is re-enabled, New Replicas can be scheduled to nodes.&lt;/td&gt;&#xA;          &lt;td&gt;test_hard_anti_affinity_scheduling&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 id=&#34;additional-tests&#34;&gt;Additional Tests&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Scenario&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected Results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Add Disk disk1, Disable scheduling for default disk -1&lt;/td&gt;&#xA;          &lt;td&gt;1.  By default the disk on a node is 0 default disk in in path - &lt;code&gt;/var/lib/longhorn/&lt;/code&gt;&lt;br&gt;2.  Add disk1 on the node&lt;br&gt;3.  Disable scheduling for the default disk&lt;br&gt;4.  Create a volume in Longhorn&lt;br&gt;5.  Verify the replicas are scheduled on disk1&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Add Disk disk1, Disable scheduling for default disk -2&lt;/td&gt;&#xA;          &lt;td&gt;Cluster spec - 3 worker nodes&lt;br&gt;&lt;br&gt;1.  Create a volume - 3 replicas in &lt;code&gt;/var/lib/longhorn/&lt;/code&gt; - default disk&lt;br&gt;2.  Add disk 1 on &lt;code&gt;/mnt/vol2&lt;/code&gt;on node 1&lt;br&gt;3.  Disable scheduling for the default disk&lt;br&gt;4.  enable scheduling for disk1&lt;br&gt;5.  Update the replicas to count = 4&lt;br&gt;6.  Say a replica is built on Node 2&lt;br&gt;7.  Delete the replica on node 1&lt;br&gt;8.  a new replica is rebuilt on node 1&lt;br&gt;9.  Verify replica is now available in &lt;code&gt;/mnt/vol2&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Replica when rebuilt on node 1 should be available on disk 1 - &lt;code&gt;/mnt/vol2&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Disable Scheduling On Cordoned Node&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;True&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;New volume&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Cordon a node W1&lt;br&gt;3.  Create a new volume with 4 replicas&lt;br&gt;4.  Verify the volume &lt;code&gt;vol-1&lt;/code&gt; is in detached state with error &lt;code&gt;Scheduling Failure Replica Scheduling Failure&lt;/code&gt;with the 4th replica in N/A state&lt;br&gt;5.  Add a new worker node W5 to the cluster&lt;br&gt;6.  &lt;code&gt;vol-1&lt;/code&gt; should become healthy.&lt;br&gt;7.  Attach it to a workload and verify data can be written into the volume&lt;/td&gt;&#xA;          &lt;td&gt;1.  &lt;code&gt;vol-1&lt;/code&gt; should be in detached state with error &lt;code&gt;Scheduling Failure Replica Scheduling Failure&lt;/code&gt;&lt;br&gt;2.  vol-1 should become healthy and should be used in a workload to write data into the volume&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;True&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;Existing volume&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Create a new volume with 4 replicas&lt;br&gt;3.  Volume vol-1 should be in a healthy detached state&lt;br&gt;4.  Attach it to a workload and verify data can be written into the volume&lt;br&gt;5.  cordon a worker node&lt;br&gt;6.  Use the. volume to a workload&lt;br&gt;7.  All the three replicas will be in running healthy state&lt;br&gt;8.  Delete replica on cordoned worker node&lt;br&gt;9.  Verify the volume &lt;code&gt;vol-1&lt;/code&gt; is in degraded state with error &lt;code&gt;Scheduling Failure Replica Scheduling Failure&lt;/code&gt;with the 4th replica in N/A state&lt;br&gt;10.  Add a new worker node W5 to the cluster&lt;br&gt;11.  Verify the repliica failed will be in rebuilding state now&lt;br&gt;12.  &lt;code&gt;vol-1&lt;/code&gt; should become healthy.&lt;br&gt;13.  Verify the data is consistent&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;False&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;New volume&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Cordon a node W1&lt;br&gt;3.  Create a new volume with 4 replicas&lt;br&gt;4.  &lt;code&gt;vol-1&lt;/code&gt; should be in healthy.&lt;br&gt;5.  Verify a replica is created on the cordoned worker node&lt;br&gt;6.  Attach it to a workload and verify data can be written into the volume&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;False&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;Existing volume&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;True&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Backup restore&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Cordon a node W1&lt;br&gt;3.  Create a backup restore volume from an existing backup.&lt;br&gt;4.  Give in the number of replicas - 4, volume name: &lt;code&gt;vol-2&lt;/code&gt;&lt;br&gt;5.  Verify the volume &lt;code&gt;vol-2&lt;/code&gt; is in detached state with error &lt;code&gt;Scheduling Failure Replica Scheduling Failure&lt;/code&gt;with the 4th replica in N/A state&lt;br&gt;6.  Verify no restoring should happen on the replicas.&lt;br&gt;7.  Add a new worker node W5 to the cluster&lt;br&gt;8.  &lt;code&gt;vol-2&lt;/code&gt; should start restoring now&lt;br&gt;9.  &lt;code&gt;vol-2&lt;/code&gt; should be in detached healthy state.&lt;br&gt;10.  attach to a workload and verify the checksum of data with that of the original one&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;False&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Backup restore&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Cordon a node W1&lt;br&gt;3.  Create a backup restore volume from an existing backup.&lt;br&gt;4.  Give in the number of replicas - 4, volume name: &lt;code&gt;vol-2&lt;/code&gt;&lt;br&gt;5.  Verify volume is in attached state and restoring should happen on the replicas&lt;br&gt;6.  &lt;code&gt;vol-2&lt;/code&gt; should be in detached healthy state. after restoration is complete&lt;br&gt;7.  attach to a workload and verify the checksum of data with that of the original one&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;True&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Create DR volume&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Cordon a node W1&lt;br&gt;3.  Create a &lt;code&gt;DRV&lt;/code&gt; from an existing backup.&lt;br&gt;4.  Give in the number of replicas - 4&lt;br&gt;5.  Verify the &lt;code&gt;DRV&lt;/code&gt; is in detached state with error &lt;code&gt;Scheduling Failure Replica Scheduling Failure&lt;/code&gt;with the 4th replica in N/A state&lt;br&gt;6.  Verify no restoring should happen on the replicas.&lt;br&gt;7.  Add a new worker node W5 to the cluster&lt;br&gt;8.  &lt;code&gt;DRV&lt;/code&gt; should start restoring now&lt;br&gt;9.  &lt;code&gt;DRV&lt;/code&gt; should be in healthy state.&lt;br&gt;10.  Activate the &lt;code&gt;DRV&lt;/code&gt; and verify it is in detached state&lt;br&gt;11.  attach to a workload and verify the checksum of data with that of the original one&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;Disable Scheduling On Cordoned Node: &lt;strong&gt;False&lt;/strong&gt;&lt;br&gt;&lt;br&gt;Create DR volume&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 4 worker nodes - custom cluster&lt;br&gt;2.  Cordon a node W1&lt;br&gt;3.  Create a &lt;code&gt;DRV&lt;/code&gt; from an existing backup.&lt;br&gt;4.  Give in the number of replicas - 4&lt;br&gt;5.  &lt;code&gt;DRV&lt;/code&gt; should start restoring now&lt;br&gt;6.  &lt;code&gt;DRV&lt;/code&gt; should be in healthy state.&lt;br&gt;7.  Activate the &lt;code&gt;DRV&lt;/code&gt; and verify it is in detached state&lt;br&gt;8.  attach to a workload and verify the checksum of data with that of the original one&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;Replica node level soft anti affinity: &lt;strong&gt;False&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;New volume&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 3 worker nodes - custom cluster&lt;br&gt;2.  Create a volume with replicas - 4&lt;br&gt;3.  Volume should be in detached state with error - &lt;code&gt;Scheduling Failure Replica Scheduling Failure&lt;/code&gt;with the 4th replica in N/A state&lt;br&gt;4.  Add a worker node&lt;br&gt;5.  the volume should be in healthy state&lt;br&gt;6.  User should be able to use the volume on the workload&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;Replica node level soft anti affinity: &lt;strong&gt;True&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;strong&gt;New volume&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;1.  There are 3 worker nodes - custom cluster&lt;br&gt;2.  Create a volume with replicas - 4&lt;br&gt;3.  the volume should be in healthy state. two replicas should be on the same host&lt;br&gt;4.  User should be able to use the volume on the workload&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>9. Upgrade</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/upgrade/</guid>
      <description>&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;#&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test name&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Higher version of Longhorn engine and lower version of volume&lt;/td&gt;&#xA;          &lt;td&gt;Test Longhorn upgrade&lt;br&gt;&lt;br&gt;1.  Create a volume, generate and write &lt;code&gt;data&lt;/code&gt; into the volume.&lt;br&gt;2.  Keep the volume attached, then upgrade Longhorn system.&lt;br&gt;3.  Write data in volume.&lt;br&gt;4.  Take snapshot#1. Compute the checksum#1&lt;br&gt;5.  Write data to volume. Compute the checksum#2&lt;br&gt;6.  Take backup&lt;br&gt;7.  Revert to snapshot#1&lt;br&gt;8.  Restore the backup.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Restore the backup taken with older engine version&lt;/td&gt;&#xA;          &lt;td&gt;1.  Create a volume, attach to a pod and write data into the volume. Compute md5sum of data&lt;br&gt;2.  Take a backup.&lt;br&gt;3.  Upgrade engine.&lt;br&gt;4.  Make the upgraded engine as default.&lt;br&gt;5.  Restore the backup, verify the checksum&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>Access Longhorn GUI using Rancher proxy</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/rancher-integration/access-lh-gui-using-rancher-ui/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/rancher-integration/access-lh-gui-using-rancher-ui/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Downstream (RKE2/RKE1/K3s) cluster in Rancher&lt;/p&gt;&#xA;&lt;p&gt;AND Deploy Longhorn using either of Kubectl/helm/marketplace app&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Click the &lt;code&gt;Longhorn&lt;/code&gt; app on Rancher UI&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Navigates to Longhorn UI&lt;/p&gt;&#xA;&lt;p&gt;AND User should be to do all the operations available on the Longhorn GUI&lt;/p&gt;&#xA;&lt;p&gt;AND URL should be a suffix to the Rancher URL&lt;/p&gt;&#xA;&lt;p&gt;AND NO error in the console logs&lt;/p&gt;</description>
    </item>
    <item>
      <title>Automatically Upgrading Longhorn Engine Test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/auto-upgrade-engine/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/auto-upgrade-engine/</guid>
      <description>&lt;h4&gt;Longhorn version &gt;= 1.1.1 &lt;/h4&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2152&#34;&gt;Reference ticket 2152&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;test-basic-upgrade&#34;&gt;Test basic upgrade&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install old Longhorn version. E.g., &amp;lt;= &lt;code&gt;v1.0.2&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a volume, attach it to a pod, write some data. Create a DR volume and leave it in the detached state.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade to Longhorn master&lt;/li&gt;&#xA;&lt;li&gt;Set setting &lt;code&gt;concurrent automatic engine upgrade per node limit&lt;/code&gt; to 3&lt;/li&gt;&#xA;&lt;li&gt;Verify that volumes&amp;rsquo; engines are upgraded automatically.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;test-concurrent-upgrade&#34;&gt;Test concurrent upgrade&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a StatefulSet of scale 10 using 10 Longhorn volume. Set node selector so that all pods land on the same node.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to use a newer default engine image&lt;/li&gt;&#xA;&lt;li&gt;In Longhorn UI and Longhorn manager logs, Verify that Longhorn doesn&amp;rsquo;t upgrade all volumes at the same time. Only 3 at a time.&lt;/li&gt;&#xA;&lt;li&gt;In Longhorn UI, while the concurrent value is set to limit 1, change the value to higher number and verify if the upgrade is happening as per the new value.&lt;/li&gt;&#xA;&lt;li&gt;Create stateful set pods with volume claim template, set the scale to 10 and in the configure options, select RWO and RWX. Verify the volumes created have RWX set as RWX takes precedence.&lt;/li&gt;&#xA;&lt;li&gt;While the auto upgrade is set, try to manually upgrade the image engine to the latest version. The manual upgrade should work and the image should be upgraded to the default version.&lt;/li&gt;&#xA;&lt;li&gt;Take backup for a volume when upgrade hasn&amp;rsquo;t started.&lt;/li&gt;&#xA;&lt;li&gt;Create 10 volumes and set concurrent upgrade limit to 2. When the upgrade starts, delete all the volumes. [Volumes are deleted and Longhorn UI is not corrupted.&lt;/li&gt;&#xA;&lt;li&gt;Deploy a workload with rwx scale 10 with persistent volume claim template and verify the volume created can be used for all of 10 pods.&lt;/li&gt;&#xA;&lt;li&gt;While upgrade is in progress, detach the volume from node and verify the detached volume is upgraded.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;test-degraded-volume&#34;&gt;Test degraded volume&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Verify that Longhorn doesn&amp;rsquo;t upgrade engine image for degraded volume.&lt;/li&gt;&#xA;&lt;li&gt;While the upgrade is happening, make a volume degraded and verify if the upgrade still happen.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;test-dr-volume&#34;&gt;Test DR volume&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Verify that Longhorn doesn&amp;rsquo;t upgrade engine image for DR volume.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;test-expanding-volume&#34;&gt;Test expanding volume&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Verify that Longhorn doesn&amp;rsquo;t upgrade engine image for volume which is expanding.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Backing Image Error Reporting and Retry</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/backing-image-error-reporting-and-retry/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/backing-image-error-reporting-and-retry/</guid>
      <description>&lt;h2 id=&#34;backing-image-with-an-invalid-url-schema&#34;&gt;Backing image with an invalid URL schema&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a backing image via a invalid download URL. e.g., &lt;code&gt;httpsinvalid://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/code&gt;, &lt;code&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.invalid.com/parrot.raw&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the download start. The backing image data source pod, which is used to download the file from the URL, should become &lt;code&gt;Failed&lt;/code&gt; then be cleaned up immediately.&lt;/li&gt;&#xA;&lt;li&gt;The corresponding and only entry in the disk file status should be &lt;code&gt;failed&lt;/code&gt;. The error message in this entry should explain why the downloading or the pod becomes failed.&lt;/li&gt;&#xA;&lt;li&gt;Check if there is a backoff window for the downloading retry. The initial duration is 1 minute. The max interval is 5 minute. This can be verified by checking the timestamp of the error message or the logs in the longhorn manager pods.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;backing-image-with-sync-failure&#34;&gt;Backing image with sync failure&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a backing image. Then create and attach a volume using this backing image.&lt;/li&gt;&#xA;&lt;li&gt;Exec into one of the worker node, remove the files in that backing image directory and set the directory as immutable.&#xA;The removal of the files will trigger the sync process to sync backing image file from another worker node. Setting immutable to the directory will make the sync process and the following retry failed.&#xA;Option 1. Run the following command to remove the content of the backing image work directory and set the directory as immutable, which should cause the sync to fail.&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rm /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;/backing*&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chattr +i /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;Option 2: Run the following command to remove the content of the backing image work directory and wait for the backing.tmp file to be generated, indicating that the sync has started and the file is transferring. Set the directory as immutable during the transfer should still fail the sync since it can&amp;rsquo;t rename the tmp file to remove the .tmp extent in the end.&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rm /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;/backing* &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; true; &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; ls -l /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;/backing.tmp &amp;gt;/dev/null 2&amp;gt;&amp;amp;1; &lt;span style=&#34;color:#66d9ef&#34;&gt;then&lt;/span&gt; ls -l /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;; break; &lt;span style=&#34;color:#66d9ef&#34;&gt;fi&lt;/span&gt;; &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chattr +i /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;Monitor the backing-image-manager pod log. Verify the backoff works for the sync retry as well.&lt;/li&gt;&#xA;&lt;li&gt;Unset the immutable flag for the backing image directory. Then the retry should succeed, and the volume should become &lt;code&gt;healthy&lt;/code&gt; again after the backing image re-sync complete.&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chattr -i /var/lib/longhorn/backing-images/&amp;lt;backing image name&amp;gt;-&amp;lt;backing image UUID&amp;gt;/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Backing Image on a down node</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/backing-image-on-a-down-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/backing-image-on-a-down-node/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Update the settings:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Disable &lt;code&gt;Node Soft Anti-affinity&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Set &lt;code&gt;Replica Replenishment Wait Interval&lt;/code&gt; to a relatively long value.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create a backing image. Wait for the backing image being ready in the 1st disk.&lt;/li&gt;&#xA;&lt;li&gt;Create 2 volumes with the backing image and attach them on different nodes. Verify:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the disk state map of the backing image contains the disks of all replicas, and the state is running for all disks.&lt;/li&gt;&#xA;&lt;li&gt;the backing image content is correct.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Write random data to the volumes.&lt;/li&gt;&#xA;&lt;li&gt;Power off 2 nodes. One node should contain one volume engine. Verify that&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the related disk file state in the backing image will become &lt;code&gt;Unknown&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;the volume on the running node still works fine but is state &lt;code&gt;Degraded&lt;/code&gt;, and the content is correct.&lt;/li&gt;&#xA;&lt;li&gt;the volume on the down node become &lt;code&gt;Unknown&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Power on the node that contains one volume engine. Verify&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the failed replica of the &lt;code&gt;Degraded&lt;/code&gt; volume can be reused.&lt;/li&gt;&#xA;&lt;li&gt;the volume on the down node will be recovered automatically. And the data is correct.&lt;/li&gt;&#xA;&lt;li&gt;the backing image will be recovered automatically.&lt;/li&gt;&#xA;&lt;li&gt;the backing image file on this node will be reused when the related backing image manager pod is recovered (by check the pod log).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Delete all volumes and the backing image. Verify the backing image manager can be deleted once forcing removing the related terminating pod.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;available-test-backing-image-urls&#34;&gt;Available test backing image URLs:&lt;/h4&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&#xA;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.raw&#xA;https://cloud-images.ubuntu.com/minimal/releases/focal/release-20200729/ubuntu-20.04-minimal-cloudimg-amd64.img&#xA;https://github.com/rancher/k3os/releases/download/v0.11.0/k3os-amd64.iso &#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>BestEffort Recurring Job Cleanup</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/besteffort-recurring-job/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/besteffort-recurring-job/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Set up a &lt;code&gt;BackupStore&lt;/code&gt; anywhere (since the cleanup fails at the &lt;code&gt;Engine&lt;/code&gt; level, any &lt;code&gt;BackupStore&lt;/code&gt; can be used.&lt;/li&gt;&#xA;&lt;li&gt;Add both of the &lt;code&gt;Engine Images&lt;/code&gt; listed here:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;quay.io/ttpcodes/longhorn-engine:no-cleanup&lt;/code&gt; - &lt;code&gt;Snapshot&lt;/code&gt; and &lt;code&gt;Backup&lt;/code&gt; deletion are both set to return an error. If the &lt;code&gt;Snapshot&lt;/code&gt; part of a &lt;code&gt;Backup&lt;/code&gt; fails, that will error out first and &lt;code&gt;Backup&lt;/code&gt; deletion will not be reached.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;quay.io/ttpcodes/longhorn-engine:no-cleanup-backup&lt;/code&gt; - Only &lt;code&gt;Backup&lt;/code&gt; deletion is set to return an error. The &lt;code&gt;Snapshot&lt;/code&gt; part of a &lt;code&gt;Backup&lt;/code&gt; should succeed, and the &lt;code&gt;Backup&lt;/code&gt; deletion will fail.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The next steps need to be repeated for each &lt;code&gt;Engine Image&lt;/code&gt; (this is to test the code for &lt;code&gt;Snapshots&lt;/code&gt; and &lt;code&gt;Backups&lt;/code&gt; separately).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Change imagePullPolicy to IfNotPresent Test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/change-imagepullpolicy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/change-imagepullpolicy/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn using Helm chart with the new &lt;code&gt;longhorn master&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that Engine Image daemonset, Manager daemonset, UI deployment, Driver Deployer deployment has the field &lt;code&gt;spec.template.spec.containers.imagePullPolicy&lt;/code&gt; set to &lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;run the bash script &lt;code&gt;dev/scripts/update-image-pull-policy.sh&lt;/code&gt; inside &lt;code&gt;longhorn&lt;/code&gt; repo&lt;/li&gt;&#xA;&lt;li&gt;Verify that Engine Image daemonset, Manager daemonset, UI deployment, Driver Deployer deployment has the field &lt;code&gt;spec.template.spec.containers.imagePullPolicy&lt;/code&gt; set back to &lt;code&gt;Always&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Checksum enabled large volume with multiple rebuilding</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/stability/checksum-enabled-large-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/stability/checksum-enabled-large-volume/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create a 50 Gi volume. write around 30 Gi data into it.&lt;/li&gt;&#xA;&lt;li&gt;Enable the setting &lt;code&gt;Snapshot Data Integrity&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Keep writing in the volume continuously using dd command like &lt;code&gt;while true; do dd if=/dev/urandom of=t1 bs=512 count=1000 conv=fsync status=progress &amp;amp;&amp;amp; rm t1; done&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Create a recurring job of backup for every 15 min.&lt;/li&gt;&#xA;&lt;li&gt;Delete a replica and wait for the replica rebuilding.&lt;/li&gt;&#xA;&lt;li&gt;Compare the performance of replica rebuilding from previous Longhorn version without the setting &lt;code&gt;Snapshot Data Integrity&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Verify the Longhorn manager logs, no abnormal logs should be present.&lt;/li&gt;&#xA;&lt;li&gt;Repeat the steps of deletion of the replica and verify Longhorn doesn&amp;rsquo;t take more time than the first iteration.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Cluster using customize kubelet root directory</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/environment/cluster-using-customized-kubelet-root-directory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/environment/cluster-using-customized-kubelet-root-directory/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Set up a cluster using a customized kubelet root directory.&#xA;e.g., launching k3s &lt;code&gt;k3s server --kubelet-arg &amp;quot;root-dir=/var/lib/longhorn-test&amp;quot; &amp;amp;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Install &lt;code&gt;Longhorn&lt;/code&gt; with env &lt;code&gt;KUBELET_ROOT_DIR&lt;/code&gt; in &lt;code&gt;longhorn-driver-deployer&lt;/code&gt; being set to the corresponding value.&lt;/li&gt;&#xA;&lt;li&gt;Launch a pod using Longhorn volumes via StorageClass. Everything should work fine.&lt;/li&gt;&#xA;&lt;li&gt;Delete the pod and the PVC. Everything should be cleaned up.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Compatibility with k3s and SELinux</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/environment/k3s-selinux-compatibility/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/environment/k3s-selinux-compatibility/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Set up a node with &lt;code&gt;CentOS&lt;/code&gt; and make sure that the output of &lt;code&gt;sestatus&lt;/code&gt; indicates that &lt;code&gt;SELinux&lt;/code&gt; is enabled and set to &lt;code&gt;Enforcing&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Run the &lt;code&gt;k3s&lt;/code&gt; installation script.&lt;/li&gt;&#xA;&lt;li&gt;Install &lt;code&gt;Longhorn&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;The system should come up successfully. The logs of the &lt;code&gt;Engine Image&lt;/code&gt; pod should only say &lt;code&gt;installed&lt;/code&gt;, and the system should be able to deploy a &lt;code&gt;Volume&lt;/code&gt; successfully from the UI.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Note: There appears to be some problems with running &lt;code&gt;k3s&lt;/code&gt; on &lt;code&gt;CentOS&lt;/code&gt;, presumably due to the &lt;code&gt;firewalld&lt;/code&gt; rules. This seems to be reported in rancher/k3s#977. I ended up disabling &lt;code&gt;firewalld&lt;/code&gt; with &lt;code&gt;systemctl stop firewalld&lt;/code&gt; in order to get &lt;code&gt;k3s&lt;/code&gt; working.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CSI Sanity Check</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/csi-sanity-check/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/csi-sanity-check/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2076&#34;&gt;https://github.com/longhorn/longhorn/issues/2076&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;run-csi-sanity&#34;&gt;Run csi-sanity&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Prepare Longhorn cluster and setup backup target.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Make csi-sanity binary from &lt;a href=&#34;https://github.com/kubernetes-csi/csi-test&#34;&gt;csi-test&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;On one of the cluster node, run csi-sanity binary.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;csi-sanity -csi.endpoint /var/lib/kubelet/obsoleted-longhorn-plugins/driver.longhorn.io/csi.sock -ginkgo.skip&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;should create volume from an existing source snapshot|should return appropriate values|should succeed when creating snapshot with maximum-length name|should succeed when requesting to create a snapshot with already existing name and same source volume ID|should fail when requesting to create a snapshot with already existing name and different source volume ID&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;&#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Degraded availability with added nodes</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node/degraded-availability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node/degraded-availability/</guid>
      <description>&lt;h4 id=&#34;volume-creation-using-ui-with-degraded-availability-and-added-node&#34;&gt;Volume creation using UI with degraded availability and added node&lt;/h4&gt;&#xA;&lt;h5 id=&#34;related-issue&#34;&gt;Related Issue:&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1701&#34;&gt;https://github.com/longhorn/longhorn/issues/1701&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;prerequisites&#34;&gt;Prerequisites:&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Start with 1 node cluster.&lt;/li&gt;&#xA;&lt;li&gt;Double check if &amp;ldquo;Allow Volume Creation with Degraded Availability&amp;rdquo; is ticked or return &lt;strong&gt;true&lt;/strong&gt; with following command:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;kubectl get settings.longhorn.io/allow-volume-creation-with-degraded-availability -n longhorn-system&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;steps&#34;&gt;Steps:&lt;/h5&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a Deployment Pod with a volume and three replicas.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;After the volume is attached, on Volume page it should be displayed as &lt;code&gt;Degraded&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Hover the crusor to the red circle exclamation mark, the tooltip will says, &amp;ldquo;The volume cannot be scheduled&amp;rdquo;.&lt;/li&gt;&#xA;&lt;li&gt;Click into the volume detail page it will display &lt;code&gt;Scheduling Failure&lt;/code&gt; but the volume remain fuctional as expected.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Write data to the Pod.&lt;/li&gt;&#xA;&lt;li&gt;Scale down the deployment to 0 to detach the volume.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Volume return to &lt;code&gt;Detached&lt;/code&gt; state.&lt;/li&gt;&#xA;&lt;li&gt;Both &lt;code&gt;Degraded&lt;/code&gt; and &lt;code&gt;Scheduling Failure&lt;/code&gt; should be gone.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Scale up the deployment back to 1 verify the data.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;code&gt;Scheduling Failure&lt;/code&gt; should be seen again from the UI.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Add another node to the cluster.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Volume should start rebuilding on the second node soon.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Once the rebuild completed, scale down and back up the deployment to verify the data.&lt;/li&gt;&#xA;&lt;li&gt;And the third node to the cluster.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Volume should start rebuilding on the third node soon.&lt;/li&gt;&#xA;&lt;li&gt;Once the rebuilding starts, the &lt;code&gt;Scheduling Failure&lt;/code&gt; should be gone.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Scale down and back the deployment to verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Disk migration in AWS ASG</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/disk-migration-in-aws-asg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/disk-migration-in-aws-asg/</guid>
      <description>&lt;h2 id=&#34;some-longhorn-worker-nodes-in-aws-auto-scaling-group-is-in-replacement&#34;&gt;Some Longhorn worker nodes in AWS Auto Scaling group is in replacement&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Launch a Kubernetes cluster with the nodes in AWS Auto Scaling group. Make sure there is an additional EBS attached to instance with setting &lt;code&gt;Delete on Termination&lt;/code&gt; disabled.&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn v1.1.0 on the cluster and Set &lt;code&gt;ReplicaReplenishmentWaitInterval&lt;/code&gt;. Make sure it&amp;rsquo;s longer than the time needs for node replacement.&lt;/li&gt;&#xA;&lt;li&gt;Deploy some workloads using Longhorn volumes.&lt;/li&gt;&#xA;&lt;li&gt;Trigger the ASG instance refresh in AWS.&lt;/li&gt;&#xA;&lt;li&gt;Manually attach EBS to new instance and mount the disk.&lt;/li&gt;&#xA;&lt;li&gt;Add the disk in longhorn to make the existing replica available to be identified by Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Verify new replicas won&amp;rsquo;t be created before reaching &lt;code&gt;ReplicaReplenishmentWaitInterval&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Verify the failed replicas are reused after the node recovery.&lt;/li&gt;&#xA;&lt;li&gt;Verify if workloads still work fine with the volumes after the recovery.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;ebs-migration-in-aws-in-a-asg-set-up-using-script&#34;&gt;EBS migration in AWS in a ASG set up using script&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Launch a Kubernetes cluster with 1 node in AWS Auto Scaling group. Make sure there is an additional EBS attached to instance with setting &lt;code&gt;Delete on Termination&lt;/code&gt; disabled.&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn v1.1.0 on the cluster and Set &lt;code&gt;ReplicaReplenishmentWaitInterval&lt;/code&gt;. Make sure it&amp;rsquo;s longer than the time needs for node replacement.&lt;/li&gt;&#xA;&lt;li&gt;Deploy some workloads using Longhorn volumes.&lt;/li&gt;&#xA;&lt;li&gt;Modify launch template of ASG and provide mount command for EBS.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo mkdir -p /data&#xA;sudo mount /dev/xvdh /data&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Trigger ASG instance refresh with below script using AWS Cli, which will attach the existing EBS volume to the new instance.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;AWS_REGION=&amp;lt;Your aws region&amp;gt;&#xA;INSTANCE_NAME=&amp;lt;Instance name&amp;gt;&#xA;VOLUME_STATUS=&amp;#39;&amp;#39;&#xA;INSTANCE_ID=$(aws --region=$AWS_REGION ec2 describe-instances --filters &amp;#39;Name=tag:Name,Values=$INSTANCE_NAME&amp;#39; | jq -r &amp;#39;.Reservations[].Instances[].InstanceId&amp;#39;)&#xA;VOLUME_ID= $(aws --region=$AWS_REGION ec2 describe-volumes --filters &amp;#39;Name=&amp;#39;attachment.instance-id&amp;#39;,Values=&amp;#39;$INSTANCE_ID&amp;#39;&amp;#39; | jq -r &amp;#39;.Volumes[1].VolumeId&amp;#39;)&#xA;&#xA;aws autoscaling start-instance-refresh --auto-scaling-group-name my-asg --preferences &amp;#39;{&amp;#34;InstanceWarmup&amp;#34;: 300, &amp;#34;MinHealthyPercentage&amp;#34;: 90}&amp;#39;&#xA;&#xA;until [ &amp;#34;x$VOLUME_STATUS&amp;#34; == &amp;#34;xattached&amp;#34; ]; do&#xA;    VOL_STATUS=$(aws ec2 describe-volumes --volume-ids $VOL_ID --query &amp;#39;Volumes[0].State&amp;#39;)&#xA;    sleep 5&#xA;done&#xA;INSTANCE_ID=$(aws --region=$AWS_REGION ec2 describe-instances --filters &amp;#39;Name=tag:Name,Values=$INSTANCE_NAME&amp;#39; | jq -r &amp;#39;.Reservations[].Instances[].InstanceId&amp;#39;)&#xA;aws ec2 attach-volume --volume-id $VOLUME_ID --instance-id $INSTANCE_ID --device /dev/sdh&amp;#39;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Add the disk in longhorn to make the existing replica available to be identified by Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Verify new replicas won&amp;rsquo;t be created before reaching &lt;code&gt;ReplicaReplenishmentWaitInterval&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Verify the failed replicas are reused after the node recovery.&lt;/li&gt;&#xA;&lt;li&gt;Verify if workloads still work fine with the volumes after the recovery.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Note: 1 node cluster is taken only for testing purpose, in real scenario more complex script would be needed for cluster having multiple nodes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DR volume related latest backup deletion test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/dr-volume-latest-backup-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/dr-volume-latest-backup-deletion/</guid>
      <description>&lt;p&gt;DR volume keeps getting the latest update from the related backups. Edge cases where the latest backup is deleted can be test as below.&lt;/p&gt;&#xA;&lt;h2 id=&#34;case-1&#34;&gt;Case 1:&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume and take multiple backups for the same.&lt;/li&gt;&#xA;&lt;li&gt;Delete the latest backup.&lt;/li&gt;&#xA;&lt;li&gt;Create another cluster and set the same backup store to access the backups created in step 1.&lt;/li&gt;&#xA;&lt;li&gt;Go to backup page and click on the backup. Verify the &lt;code&gt;Create Disaster Recovery&lt;/code&gt; option is enabled for it.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;case-2&#34;&gt;Case 2:&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume V1 and take multiple backups for the same.&lt;/li&gt;&#xA;&lt;li&gt;Create another cluster and set the same backup store to access the backups created in step 1.&lt;/li&gt;&#xA;&lt;li&gt;Go to backup page and Create a Disaster Recovery Volume for the backups created in step 1.&lt;/li&gt;&#xA;&lt;li&gt;Create more backup(s) for volume V1 from step 1.&lt;/li&gt;&#xA;&lt;li&gt;Delete the latest backup before the DR volume starts the incremental restore process.&lt;/li&gt;&#xA;&lt;li&gt;Verify the DR Volume still remains healthy.&lt;/li&gt;&#xA;&lt;li&gt;Activate the DR Volume to verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Drain using Rancher UI</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/rancher-integration/drain-using-rancher-ui/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/rancher-integration/drain-using-rancher-ui/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Enabling &lt;code&gt;Delete Empty Dir Data&lt;/code&gt; is mandatory to drain a node if a pod is associated with any storage.&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-with-longhorn-default-setting-of-node-drain-policy-block-if-contains-last-replica&#34;&gt;Test with Longhorn default setting of &amp;lsquo;Node Drain Policy&amp;rsquo;: &lt;code&gt;block-if-contains-last-replica&lt;/code&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-drain-operation-on-single-node-using-rancher-ui&#34;&gt;1. Drain operation on single node using Rancher UI&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt;  Single node (1 Worker) cluster with Longhorn installed&lt;/p&gt;&#xA;&lt;p&gt;AND few RWO and RWX volumes attached with node/pod exists&lt;/p&gt;&#xA;&lt;p&gt;AND 1 RWO and 1 RWX volumes unattached&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Drain the node with default values of Rancher UI&lt;/p&gt;</description>
    </item>
    <item>
      <title>Extended CSI snapshot support to Longhorn snapshot</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/extend_csi_snapshot_support/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/extend_csi_snapshot_support/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2534&#34;&gt;https://github.com/longhorn/longhorn/issues/2534&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-setup&#34;&gt;Test Setup&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy the CSI snapshot CRDs, Controller as instructed at &lt;a href=&#34;https://longhorn.io/docs/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support/&#34;&gt;https://longhorn.io/docs/1.2.3/snapshots-and-backups/csi-snapshot-support/enable-csi-snapshot-support/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Deploy 4 VolumeSnapshotClass:&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshotClass&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-backup-1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;driver.longhorn.io&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;deletionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshotClass&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-backup-2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;driver.longhorn.io&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;deletionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bak&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshotClass&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-snapshot&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;driver.longhorn.io&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;deletionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snap&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshotClass&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;invalid-class&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;driver.longhorn.io&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;deletionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;invalid&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create Longhorn volume &lt;code&gt;test-vol&lt;/code&gt; of 5GB. Create PV/PVC for the Longhorn volume.&lt;/li&gt;&#xA;&lt;li&gt;Create a workload that uses the volume. Write some data to the volume.&#xA;Make sure data persist to the volume by running &lt;code&gt;sync&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Set up a backup target for Longhorn&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;scenarios-1-createsnapshot&#34;&gt;Scenarios 1: CreateSnapshot&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;code&gt;type&lt;/code&gt; is &lt;code&gt;bak&lt;/code&gt; or &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>HA Volume Migration</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/ha-volume-migration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/ha-volume-migration/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Create a migratable volume:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy a migratable StorageClass. e.g., &lt;a href=&#34;https://github.com/longhorn/longhorn/blob/master/examples/rwx/storageclass-migratable.yaml&#34;&gt;https://github.com/longhorn/longhorn/blob/master/examples/rwx/storageclass-migratable.yaml&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a PVC with access mode &lt;code&gt;ReadWriteMany&lt;/code&gt; via this StorageClass.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Attach a volume to a node and wait for volume running. Then write some data into the volume. Here I would recommend directly restoring a volume (set &lt;code&gt;fromBackup&lt;/code&gt; in the StorageClass) and attach it instead.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Start the migration by request attaching to another node for the volume.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Trigger the following scenarios then confirm or rollback the migration:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Improve Node Failure Handling By Automatically Force Delete Terminating Pods of StatefulSet/Deployment On Downed Node</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node/improve-node-failure-handling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node/improve-node-failure-handling/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Setup a cluster of 3 worker nodes&lt;/li&gt;&#xA;&lt;li&gt;Install Longhorn and set &lt;code&gt;Default Replica Count = 2&lt;/code&gt; (because we will turn off one node)&lt;/li&gt;&#xA;&lt;li&gt;Create a StatefulSet with 2 pods using the command:&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl create -f https://raw.githubusercontent.com/longhorn/longhorn/master/examples/statefulset.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a volume + pv + pvc named &lt;code&gt;vol1&lt;/code&gt; and create a deployment(1 pod) of default ubuntu named &lt;code&gt;shell&lt;/code&gt; with the usage of pvc &lt;code&gt;vol1&lt;/code&gt; mounted under &lt;code&gt;/mnt/vol1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Find the node which contains one pod of the StatefulSet/Deployment. Power off the node&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;statefulset&#34;&gt;StatefulSet&lt;/h4&gt;&#xA;&lt;h5 id=&#34;if-nodedownpoddeletionpolicy--is-set-to-do-nothing---delete-deployment-pod&#34;&gt;if &lt;code&gt;NodeDownPodDeletionPolicy &lt;/code&gt; is set to &lt;code&gt;do-nothing &lt;/code&gt; | &lt;code&gt;delete-deployment-pod&lt;/code&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;wait till the &lt;code&gt;pod.deletionTimestamp&lt;/code&gt; has passed&lt;/li&gt;&#xA;&lt;li&gt;verify no replacement pod generated, the pod is stuck at terminating forever.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;if-nodedownpoddeletionpolicy--is-set-to-delete-statefulset-pod---delete-both-statefulset-and-deployment-pod&#34;&gt;if &lt;code&gt;NodeDownPodDeletionPolicy &lt;/code&gt; is set to &lt;code&gt;delete-statefulset-pod &lt;/code&gt; | &lt;code&gt;delete-both-statefulset-and-deployment-pod&lt;/code&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;wait till pod&amp;rsquo;s status becomes &lt;code&gt;terminating&lt;/code&gt; and the &lt;code&gt;pod.deletionTimestamp&lt;/code&gt; has passed (around 7 minutes)&lt;/li&gt;&#xA;&lt;li&gt;verify that the pod is deleted and there is a new running replacement pod.&lt;/li&gt;&#xA;&lt;li&gt;Verify that you can access/read/write the volume on the new pod&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;deployment&#34;&gt;Deployment&lt;/h4&gt;&#xA;&lt;h5 id=&#34;if-nodedownpoddeletionpolicy--is-set-to-do-nothing---delete-statefulset-pod&#34;&gt;if &lt;code&gt;NodeDownPodDeletionPolicy &lt;/code&gt; is set to &lt;code&gt;do-nothing &lt;/code&gt; | &lt;code&gt;delete-statefulset-pod&lt;/code&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;wait till the &lt;code&gt;pod.deletionTimestamp&lt;/code&gt; has passed&lt;/li&gt;&#xA;&lt;li&gt;replacement pod will be stuck in &lt;code&gt;Pending&lt;/code&gt; state forever&lt;/li&gt;&#xA;&lt;li&gt;force delete the terminating pod&lt;/li&gt;&#xA;&lt;li&gt;wait till replacement pod is running&lt;/li&gt;&#xA;&lt;li&gt;verify that you can access &lt;code&gt;vol1&lt;/code&gt; via the &lt;code&gt;shell&lt;/code&gt; replacement pod under &lt;code&gt;/mnt/vol1&lt;/code&gt; once it is in the running state&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;if-nodedownpoddeletionpolicy--is-set-to-delete-deployment-pod---delete-both-statefulset-and-deployment-pod&#34;&gt;if &lt;code&gt;NodeDownPodDeletionPolicy &lt;/code&gt; is set to &lt;code&gt;delete-deployment-pod &lt;/code&gt; | &lt;code&gt;delete-both-statefulset-and-deployment-pod&lt;/code&gt;&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;wait till the &lt;code&gt;pod.deletionTimestamp&lt;/code&gt; has passed&lt;/li&gt;&#xA;&lt;li&gt;verify that the pod is deleted and there is a new running replacement pod.&lt;/li&gt;&#xA;&lt;li&gt;verify that you can access &lt;code&gt;vol1&lt;/code&gt; via the &lt;code&gt;shell&lt;/code&gt; replacement pod under &lt;code&gt;/mnt/vol1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;other-kinds&#34;&gt;Other kinds&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Verify that Longhorn never deletes any other pod on the downed node.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;test-example&#34;&gt;Test example&lt;/h4&gt;&#xA;&lt;p&gt;One typical scenario when the enhancement has succeeded is as below. When a node (say &lt;code&gt;node-x&lt;/code&gt;) goes down (assume using Kubernetes&amp;rsquo; default settings and user allows Longhorn to force delete pods):&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kubernetes upgrade test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/kubernetes-upgrade-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/kubernetes-upgrade-test/</guid>
      <description>&lt;p&gt;We also need to cover the Kubernetes upgrade process for supported Kubernetes version, make sure pod and volumes works after a major version upgrade.&lt;/p&gt;&#xA;&lt;h2 id=&#34;related-issue&#34;&gt;Related Issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2566&#34;&gt;https://github.com/longhorn/longhorn/issues/2566&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-with-k8s-upgrade&#34;&gt;Test with K8s upgrade&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a K8s (Immediate prior version) cluster with 3 worker nodes and 1 control plane.&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn version (Immediate prior version) on the cluster.&lt;/li&gt;&#xA;&lt;li&gt;Create a volume and attach to a pod.&lt;/li&gt;&#xA;&lt;li&gt;Write data to the volume and compute the checksum.&lt;/li&gt;&#xA;&lt;li&gt;Create 2nd volume and keep it detached.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade K8s to the latest version.&lt;/li&gt;&#xA;&lt;li&gt;Observe the volume replicas get rebuilt as the instance manager goes down temporarily for the upgrade.&lt;/li&gt;&#xA;&lt;li&gt;Verify all the volume should come up healthy after all the nodes are upgraded.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to the latest version and verify all the volumes become healthy eventually.&lt;/li&gt;&#xA;&lt;li&gt;Verify the data in the volume.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;Known Issue&lt;/strong&gt;: If the volumes are still attaching and the instance managers got killed due to node went down temporarily for the upgrade, the replicas on those instance managers will be out of sync and will become error.&#xA;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/494&#34;&gt;https://github.com/longhorn/longhorn/issues/494&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Longhorn in an hardened cluster</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/rancher-integration/lh-hardend-rancher/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/rancher-integration/lh-hardend-rancher/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Hardened Downstream (RKE2/RKE1/K3s) cluster in Rancher v2.6.x with CIS 1.6 as per &lt;a href=&#34;https://ranchermanager.docs.rancher.com/v2.6/pages-for-subheaders/rancher-v2.6-hardening-guides&#34;&gt;Hardening guide&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Deploy Longhorn using Marketplace app&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Longhorn should be deployed properly&lt;/p&gt;&#xA;&lt;p&gt;AND Volume creation and other operations should work fine&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Hardened Downstream (RKE2/RKE1/K3s) cluster in Rancher v2.7.x with CIS 1.6 or 1.20 or 1.23 as per &lt;a href=&#34;https://ranchermanager.docs.rancher.com/pages-for-subheaders/rancher-v2.7-hardening-guides&#34;&gt;Hardending guide for Rancher 2.7&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Deploy Longhorn using Marketplace app&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Longhorn should be deployed properly&lt;/p&gt;&#xA;&lt;p&gt;AND Volume creation and other operations should work fine&lt;/p&gt;</description>
    </item>
    <item>
      <title>Longhorn installation multiple times</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/stability/multiple-installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/stability/multiple-installation/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create a cluster(3 worker nodes and 1 etc/control plane).&lt;/li&gt;&#xA;&lt;li&gt;Deploy the longhorn app.&lt;/li&gt;&#xA;&lt;li&gt;Once longhorn deployed successfully, uninstall longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Repeat the steps 2 and 3 multiple times.&lt;/li&gt;&#xA;&lt;li&gt;Run the below script to install and uninstall longhorn continuously for some time.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;installcount=0&#xA;while true;&#xA; echo `date`&#xA; do&#xA;  kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml&#xA;&#xA;  pod=`kubectl get pods -n longhorn-system | grep -i &amp;#39;longhorn-manager&amp;#39; | grep -i &amp;#39;running&amp;#39; | awk -F &amp;#39; &amp;#39; &amp;#39;{print $2}&amp;#39; | grep &amp;#39;1/1&amp;#39; | wc -l`&#xA;  count=0&#xA;  while [ $pod != 3 ];&#xA;   do&#xA;    sleep 5&#xA;    pod=`kubectl get pods -n longhorn-system | grep -i &amp;#39;longhorn-manager&amp;#39; | grep -i &amp;#39;running&amp;#39; | awk -F &amp;#39; &amp;#39; &amp;#39;{print $2}&amp;#39; | grep &amp;#39;1/1&amp;#39; | wc -l`&#xA;    echo `kubectl get pods -n longhorn-system | grep -i &amp;#39;longhorn-manager&amp;#39;`&#xA;    count=$((count+1))&#xA;    if [ $count -gt 59 ]&#xA;     then&#xA;      echo &amp;#39;longhorn installation failed&amp;#39;&#xA;      exit&#xA;    fi&#xA;  done&#xA;&#xA;  sleep 30&#xA; &#xA;  kubectl create -f https://raw.githubusercontent.com/longhorn/longhorn/master/uninstall/uninstall.yaml&#xA;&#xA;  poduninstall=`kubectl get job/longhorn-uninstall -n default | grep &amp;#39;1/1&amp;#39; | wc -l`&#xA;  uninstall=0&#xA; while [ $poduninstall = 0 ];&#xA;  do&#xA;   sleep 10&#xA;   poduninstall=`kubectl get job/longhorn-uninstall -n default | grep &amp;#39;1/1&amp;#39; | wc -l`&#xA;   echo `kubectl get job/longhorn-uninstall`&#xA;   uninstall=$((uninstall+1))&#xA;   if [ $uninstall -gt 24 ]  &#xA;    then&#xA;     echo &amp;#39;Problem in unistall&amp;#39;&#xA;     exit&#xA;   fi&#xA; done&#xA; &#xA; kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml&#xA; sleep 3&#xA;&#xA; kubectl delete -f https://raw.githubusercontent.com/longhorn/longhorn/master/uninstall/uninstall.yaml&#xA;&#xA; nscount=0&#xA; longhornns=`kubectl get namespace | grep -i &amp;#39;longhorn-system&amp;#39; | wc -l`&#xA; while [ $longhornns != 0 ];&#xA;  do&#xA;   sleep 10&#xA;   longhornns=`kubectl get namespace | grep -i &amp;#39;longhorn-system&amp;#39; | wc -l`&#xA;   nscount=$((nscount+1))&#xA;   if [ $nscount -gt 18 ]&#xA;    then&#xA;     echo &amp;#39;longhorn-system termination stuck&amp;#39;&#xA;     exit&#xA;   fi&#xA; done&#xA;&#xA; installcount=$((installcount+1))&#xA; echo &amp;#39;Installation count = &amp;#39;&#xA; echo $installcount&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Longhorn Upgrade test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/longhorn-upgrade-test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/longhorn-upgrade-test/</guid>
      <description>&lt;h3 id=&#34;setup&#34;&gt;Setup&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;2 attached volumes with data. 2 detached volumes with data. 2 new volumes without data.&lt;/li&gt;&#xA;&lt;li&gt;2 deployments of one pod. 1 statefulset of 10 pods.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;Auto Salvage&lt;/code&gt; set to disable.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;test&#34;&gt;Test&lt;/h3&gt;&#xA;&lt;p&gt;After upgrade:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Make sure the existing instance managers didn&amp;rsquo;t restart.&lt;/li&gt;&#xA;&lt;li&gt;Make sure pods didn&amp;rsquo;t restart.&lt;/li&gt;&#xA;&lt;li&gt;Check the contents of the volumes.&lt;/li&gt;&#xA;&lt;li&gt;If the Engine API version is incompatible, manager cannot do anything about the attached volumes except detaching it.&lt;/li&gt;&#xA;&lt;li&gt;If the Engine API version is incompatible, manager cannot live-upgrade the attached volumes.&lt;/li&gt;&#xA;&lt;li&gt;If the Engine API version is incompatible, manager cannot reattach an existing volume until the user has upgraded the engine image to a manager supported version.&lt;/li&gt;&#xA;&lt;li&gt;After offline or online (live) engine upgrade, check the contents of the volumes are valid.&lt;/li&gt;&#xA;&lt;li&gt;For the volume never been attached in the old version, check it&amp;rsquo;s attachable after the upgrade.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Longhorn using fleet on multiple downstream clusters</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/rancher-integration/fleet-deploy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/rancher-integration/fleet-deploy/</guid>
      <description>&lt;p&gt;reference: &lt;a href=&#34;https://github.com/rancher/fleet&#34;&gt;https://github.com/rancher/fleet&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;test-longhorn-deployment-using-fleet&#34;&gt;Test Longhorn deployment using fleet:&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Downstream multiple (RKE2/RKE1/K3s) clusters in Rancher&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Use fleet to deploy Longhorn&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Longhorn should be deployed to all the cluster&lt;/p&gt;&#xA;&lt;p&gt;AND Longhorn UI should be accessible using Rancher proxy&lt;/p&gt;&#xA;&lt;h3 id=&#34;test-longhorn-uninstall-using-fleet&#34;&gt;Test Longhorn uninstall using fleet:&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Downstream multiple (RKE2/RKE1/K3s) clusters in Rancher&lt;/p&gt;&#xA;&lt;p&gt;AND Longhorn is deployed on all the clusters using fleet&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Use fleet to uninstall Longhorn&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Longhorn should be uninstalled from all the cluster&lt;/p&gt;</description>
    </item>
    <item>
      <title>Longhorn with engine is not deployed on all the nodes</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/partial-engine-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/partial-engine-deployment/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related Issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2081&#34;&gt;https://github.com/longhorn/longhorn/issues/2081&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;scenarios&#34;&gt;Scenarios:&lt;/h2&gt;&#xA;&lt;h3 id=&#34;case-1-test-volume-operations-when-some-of-the-engine-image-daemonset-pods-are-miss-scheduled&#34;&gt;Case 1: Test volume operations when some of the engine image DaemonSet pods are miss scheduled&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn in a 3-node cluster: &lt;code&gt;node-1&lt;/code&gt;, &lt;code&gt;node-2&lt;/code&gt;, &lt;code&gt;node-3&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a volume, &lt;code&gt;vol-1&lt;/code&gt;, of 3 replicas&lt;/li&gt;&#xA;&lt;li&gt;Create another volume, &lt;code&gt;vol-2&lt;/code&gt;, of 3 replicas&lt;/li&gt;&#xA;&lt;li&gt;Taint &lt;code&gt;node-1&lt;/code&gt; with the taint: &lt;code&gt;key=value:NoSchedule&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check that all functions (attach, detach, snapshot, backup, expand, restore, creating DR volume, &amp;hellip; ) are working ok for &lt;code&gt;vol-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-2-test-volume-operations-when-some-of-engine-image-daemonset-pods-are-not-fully-deployed&#34;&gt;Case 2: Test volume operations when some of engine image DaemonSet pods are not fully deployed&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Continue from case 1&lt;/li&gt;&#xA;&lt;li&gt;Attach &lt;code&gt;vol-1&lt;/code&gt; to &lt;code&gt;node-1&lt;/code&gt;. Change the number of replicas of &lt;code&gt;vol-1&lt;/code&gt; to 2. Delete the replica on &lt;code&gt;node-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Delete the pod on &lt;code&gt;node-1&lt;/code&gt; of the engine image DaemonSet. Or delete the engine image DaemonSet and wait for Longhorn to automatically recreates it.&lt;/li&gt;&#xA;&lt;li&gt;Notice that the engine image CR state become deploying&lt;/li&gt;&#xA;&lt;li&gt;Verify that functions (detach, snapshot, backup) are working ok for &lt;code&gt;vol-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Detach &lt;code&gt;vol-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn cannot attach &lt;code&gt;vol-1&lt;/code&gt; to &lt;code&gt;node-1&lt;/code&gt; since there is no engine image on &lt;code&gt;node-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check that all functions (attach to other nodes, detach, snapshot, backup, expand, restore, creating DR volume, &amp;hellip; ) are working ok for &lt;code&gt;vol-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;vol-2&lt;/code&gt; cannot be attached to any nodes because one of its replica is sitting on the &lt;code&gt;node-1&lt;/code&gt; which doesn&amp;rsquo;t have the engine image&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-3-test-engine-upgrade-when-some-of-the-engine-image-daemonset-pods-are-not-fully-deployed&#34;&gt;Case 3: Test engine upgrade when some of the engine image DaemonSet pods are not fully deployed&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Continue from case 2&lt;/li&gt;&#xA;&lt;li&gt;Deploy a new engine image, &lt;code&gt;newEI&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that you can upgrade &lt;code&gt;vol-1&lt;/code&gt; to &lt;code&gt;newEI&lt;/code&gt; (both live and offline upgrade)&lt;/li&gt;&#xA;&lt;li&gt;Verify that you can not upgrade &lt;code&gt;vol-2&lt;/code&gt; to &lt;code&gt;newEI&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-4-test-replicas-scheduling-when-some-of-the-engine-image-daemonset-pods-are-not-fully-deployed&#34;&gt;Case 4: Test replicas scheduling when some of the engine image DaemonSet pods are not fully deployed&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Continue from case 2&lt;/li&gt;&#xA;&lt;li&gt;Create a new volume, &lt;code&gt;vol-3&lt;/code&gt;, with 2 replicas&lt;/li&gt;&#xA;&lt;li&gt;Verify that replicas of &lt;code&gt;vol-3&lt;/code&gt; are on &lt;code&gt;node-2&lt;/code&gt; and &lt;code&gt;node-3&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check that all functions (attach, detach, snapshot, backup, expand, restore, creating DR volume,&amp;hellip; ) are working ok for &lt;code&gt;vol-3&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-5-test-longhorn-upgrade-when-some-of-the-engine-image-daemonset-pods-are-not-fully-deployed&#34;&gt;Case 5: Test Longhorn upgrade when some of the engine image DaemonSet pods are not fully deployed&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Continue from case 3&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to a new version&lt;/li&gt;&#xA;&lt;li&gt;Verify that the upgrade is not blocked. Longhorn successfully upgrades to the new version using the same default engine image even though the default engine image is not fully deployed.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-6-test-auto-upgrade-engine-feature-when-some-of-the-engine-image-daemonset-pods-are-not-fully-deployed&#34;&gt;Case 6: Test &lt;code&gt;auto upgrade engine&lt;/code&gt; feature when some of the engine image DaemonSet pods are not fully deployed&lt;/h3&gt;&#xA;&lt;p&gt;With the engine image is missing on &lt;code&gt;node-1&lt;/code&gt;, we need  to re-verify the manual test for feature &lt;code&gt;auto upgrade engine&lt;/code&gt; &lt;a href=&#34;https://github.com/longhorn/longhorn-tests/blob/master/docs/content/manual/pre-release/upgrade/auto-upgrade-engine.md&#34;&gt;https://github.com/longhorn/longhorn-tests/blob/master/docs/content/manual/pre-release/upgrade/auto-upgrade-engine.md&lt;/a&gt;&#xA;Make sure that Longhorn automatically upgrades engine image for volumes that are either:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Monitoring</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/functional-test-cases/monitoring/</guid>
      <description>&lt;h2 id=&#34;prometheus-support-test-cases&#34;&gt;Prometheus Support test cases&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install the Prometheus Operator (include a role and service account for it). For example:&lt;pre&gt;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: ClusterRoleBinding&#xA;metadata:&#xA;name: prometheus-operator&#xA;namespace: default&#xA;roleRef:&#xA;apiGroup: rbac.authorization.k8s.io&#xA;kind: ClusterRole&#xA;name: prometheus-operator&#xA;subjects:&lt;br&gt;  - kind: ServiceAccount&#xA;name: prometheus-operator&#xA;namespace: default&lt;br&gt;&amp;ndash;&#xA;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: ClusterRole&#xA;metadata:&#xA;name: prometheus-operator&#xA;namespace: default&#xA;rules:&lt;br&gt;  - apiGroups:&lt;br&gt;      - extensions&#xA;resources:&lt;br&gt;      - thirdpartyresources&#xA;verbs: [&amp;quot;&lt;em&gt;&amp;quot;]&lt;br&gt;  - apiGroups:&lt;br&gt;      - apiextensions.k8s.io&#xA;resources:&lt;br&gt;      - customresourcedefinitions&#xA;verbs: [&amp;quot;&lt;/em&gt;&amp;quot;]&lt;br&gt;  - apiGroups:&lt;br&gt;      - monitoring.coreos.com&#xA;resources:&lt;br&gt;      - alertmanagers&lt;br&gt;      - prometheuses&lt;br&gt;      - prometheuses/finalizers&lt;br&gt;      - servicemonitors&lt;br&gt;      - prometheusrules&lt;br&gt;      - podmonitors&#xA;verbs: [&amp;quot;&lt;em&gt;&amp;quot;]&lt;br&gt;  - apiGroups:&lt;br&gt;      - apps&#xA;resources:&lt;br&gt;      - statefulsets&#xA;verbs: [&amp;quot;&lt;/em&gt;&amp;quot;]&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - configmaps&lt;br&gt;      - secrets&#xA;verbs: [&amp;quot;*&amp;quot;]&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - pods&#xA;verbs: [&amp;ldquo;list&amp;rdquo;, &amp;ldquo;delete&amp;rdquo;]&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - services&lt;br&gt;      - endpoints&#xA;verbs: [&amp;ldquo;get&amp;rdquo;, &amp;ldquo;create&amp;rdquo;, &amp;ldquo;update&amp;rdquo;]&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - nodes&#xA;verbs: [&amp;ldquo;list&amp;rdquo;, &amp;ldquo;watch&amp;rdquo;]&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - namespaces&#xA;verbs: [&amp;ldquo;list&amp;rdquo;, &amp;ldquo;watch&amp;rdquo;]&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre&gt;&#xA;apiVersion: v1&#xA;kind: ServiceAccount&#xA;metadata:&#xA;  name: prometheus-operator&#xA;  namespace: default&#xA;---&#xA;apiVersion: apps/v1&#xA;kind: Deployment&#xA;metadata:&#xA;  labels:&#xA;    app: prometheus-operator&#xA;  name: prometheus-operator&#xA;  namespace: default&#xA;spec:&#xA;  replicas: 1&#xA;  selector:&#xA;    matchLabels:&#xA;      app: prometheus-operator&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: prometheus-operator&#xA;    spec:&#xA;      containers:&#xA;        - args:&#xA;            - --kubelet-service=kube-system/kubelet&#xA;            - --config-reloader-image=quay.io/coreos/configmap-reload:v0.0.1&#xA;          image: quay.io/coreos/prometheus-operator:v0.36.0&#xA;          name: prometheus-operator&#xA;          ports:&#xA;            - containerPort: 8080&#xA;              name: http&#xA;          resources:&#xA;            limits:&#xA;              cpu: 200m&#xA;              memory: 100Mi&#xA;            requests:&#xA;              cpu: 100m&#xA;              memory: 50Mi&#xA;      securityContext:&#xA;        runAsNonRoot: true&#xA;        runAsUser: 65534&#xA;      serviceAccountName: prometheus-operator&lt;/pre&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;Install a Service Monitor pointing to &lt;code&gt;longhon-backend&lt;/code&gt; service by selecting &lt;code&gt;app: longhorn-manager&lt;/code&gt; label. For example:&lt;pre&gt;apiVersion: monitoring.coreos.com/v1&#xA;kind: ServiceMonitor&#xA;metadata:&#xA;name: longhorn-backend&#xA;labels:&#xA;team: backend&#xA;spec:&#xA;selector:&#xA;matchLabels:&#xA;app: longhorn-manager&#xA;namespaceSelector:&#xA;matchNames:&lt;br&gt;    - longhorn-system&#xA;endpoints:&lt;br&gt;  - port: manager&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Install Prometheus (include a role and service account for it). Include the above &lt;code&gt;service monitor&lt;/code&gt; in the Prometheus&amp;rsquo;s config. Expose to the Prometheus instance to outside using a service of type NodePort. For example:&lt;pre&gt;apiVersion: v1&#xA;kind: ServiceAccount&#xA;metadata:&#xA;name: prometheus&lt;br&gt;&amp;mdash;&#xA;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: ClusterRole&#xA;metadata:&#xA;name: prometheus&#xA;rules:&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - nodes&lt;br&gt;      - services&lt;br&gt;      - endpoints&lt;br&gt;      - pods&lt;br&gt;    verbs: [&amp;ldquo;get&amp;rdquo;, &amp;ldquo;list&amp;rdquo;, &amp;ldquo;watch&amp;rdquo;]&lt;br&gt;  - apiGroups: [&amp;quot;&amp;quot;]&#xA;resources:&lt;br&gt;      - configmaps&#xA;verbs: [&amp;ldquo;get&amp;rdquo;]&lt;br&gt;  - nonResourceURLs: [&amp;quot;/metrics&amp;quot;, &amp;ldquo;/federate&amp;rdquo;]&#xA;verbs: [&amp;ldquo;get&amp;rdquo;]&lt;br&gt;&amp;mdash;&#xA;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: ClusterRoleBinding&#xA;metadata:&#xA;name: prometheus&#xA;roleRef:&#xA;apiGroup: rbac.authorization.k8s.io&#xA;kind: ClusterRole&#xA;name: prometheus&#xA;subjects:&lt;br&gt;  - kind: ServiceAccount&#xA;name: prometheus&#xA;namespace: default&lt;br&gt;&amp;mdash;&#xA;apiVersion: monitoring.coreos.com/v1&#xA;kind: Prometheus&#xA;metadata:&#xA;name: prometheus&#xA;spec:&#xA;serviceAccountName: prometheus&#xA;serviceMonitorSelector:&#xA;matchLabels:&#xA;team: backend&lt;br&gt;&amp;mdash;&#xA;apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;name: prometheus&#xA;spec:&#xA;type: NodePort&#xA;ports:&lt;br&gt;  - name: web&#xA;port: 9090&#xA;protocol: TCP&#xA;targetPort: web&#xA;ports:&lt;br&gt;    - port: 9090&#xA;selector:&#xA;prometheus: prometheus&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Find the &lt;code&gt;prometheus&lt;/code&gt; service and access Prometheus web UI using the nodeIP and the port&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Scenario&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Test Steps&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Expected results&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;All the Metrics are present&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;2.  Verify the metrics are available.&lt;/td&gt;&#xA;          &lt;td&gt;The below metrics should be available:&lt;br&gt;&lt;br&gt;1.  longhorn_volume_capacity_bytes&amp;lt;/pre&lt;br&gt;2.  longhorn_volume_usage_bytes&amp;lt;/pre&lt;br&gt;3.  longhorn_node_status&amp;lt;/pre&lt;br&gt;4.  onghorn_instance_manager_cpu_requests_millicpu&amp;lt;/pre&lt;br&gt;5.  longhorn_instance_manager_cpu_usage_millicpu&amp;lt;/pre&lt;br&gt;6.  longhorn_instance_manager_memory_requests_bytes&amp;lt;/pre&lt;br&gt;7.  longhorn_instance_manager_memory_usage_bytes&amp;lt;/pre&lt;br&gt;8.  longhorn_manager_cpu_usage_millicpu&amp;lt;/pre&lt;br&gt;9.  longhorn_manager_memory_usage_bytes&amp;lt;/pre&lt;br&gt;10.  longhorn_disk_capacity_bytes&amp;lt;/pre&lt;br&gt;11.  longhorn_disk_usage_bytes&amp;lt;/pre&lt;br&gt;12.  longhorn_node_capacity_bytes&amp;lt;/pre&lt;br&gt;13.  longhorn_node_usage_bytes&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_volume_capacity_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create 4 volumes of different sizes. (2, 3, 4, 5 Gi)&amp;lt;/pre&lt;br&gt;2.  Attach 1st volume to a pod and write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Attach 2nd volume to a pod and don’t write into.&amp;lt;/pre&lt;br&gt;4.  Leave the 3rd volume to the detached state.&amp;lt;/pre&lt;br&gt;5.  Attach the 4th volume to pod and write 1.5 Gi data into it. Detach the volume.&amp;lt;/pre&lt;br&gt;6.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;7.  Select &lt;code&gt;longhorn_volume_capacity_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the volumes should be identified by Prometheus&amp;lt;/pre&lt;br&gt;2.  All the volumes should show the capacity as 2 Gi&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_volume_usage_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create 4 volumes of different sizes. (2, 3, 4, 5 Gi)&amp;lt;/pre&lt;br&gt;2.  Attach 1st volume to a pod and write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Attach 2nd volume to a pod and don’t write into.&amp;lt;/pre&lt;br&gt;4.  Leave the 3rd volume to the detached state.&amp;lt;/pre&lt;br&gt;5.  Attach the 4th volume to pod and write 1.5 Gi data into it. Detach the volume.&amp;lt;/pre&lt;br&gt;6.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;7.  Select &lt;code&gt;longhorn_volume_usage_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the volumes should be identified by Prometheus&amp;lt;/pre&lt;br&gt;2.  Volume-1 should show 1 Gi&amp;lt;/pre&lt;br&gt;3.  Volume-2 should show 0 Gi&amp;lt;/pre&lt;br&gt;4.  Volume-3 should show 0 Gi&amp;lt;/pre&lt;br&gt;5.  Volume-4 should show 1.5 Gi&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;4&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_node_status&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Power down a node.&amp;lt;/pre&lt;br&gt;2.  Disable a node.&amp;lt;/pre&lt;br&gt;3.  Add a new node in the cluster.&amp;lt;/pre&lt;br&gt;4.  Delete a node from the cluster.&amp;lt;/pre&lt;br&gt;5.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;6.  Select &lt;code&gt;longhorn_node_status&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the nodes should be identified by Prometheus and one node should be shown in 3 rows based on the condition - &lt;code&gt;mountpropagation, ready, schedulable&lt;/code&gt;&amp;lt;/pre&lt;br&gt;2.  The correct status should be shown on Prometheus UI.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;5&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_instance_manager_cpu_requests_millicpu&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a volume and attach it to a pod.&amp;lt;/pre&lt;br&gt;2.  Write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Set multiple recurring backup on the volume.&amp;lt;/pre&lt;br&gt;4.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;5.  Select &lt;code&gt;longhorn_instance_manager_cpu_requests_millicpu&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The reading of cpu_requests should go up for the attached instance manager.&amp;lt;/pre&lt;br&gt;2.  The reading of other instance managers should not get impacted.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;6&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_instance_manager_cpu_usage_millicpu&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a volume and attach it to a pod.&amp;lt;/pre&lt;br&gt;2.  Write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Set multiple recurring backup on the volume.&amp;lt;/pre&lt;br&gt;4.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;5.  Select &lt;code&gt;longhorn_instance_manager_cpu_usage_millicpu&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The reading of cpu_usage should be shown correctly&amp;lt;/pre&lt;br&gt;2.  The reading of other instance managers should not get impacted.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;7&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_instance_manager_memory_requests_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a volume and attach it to a pod.&amp;lt;/pre&lt;br&gt;2.  Write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Set multiple recurring backup on the volume.&amp;lt;/pre&lt;br&gt;4.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;5.  Select &lt;code&gt;longhorn_instance_manager_memory_requests_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The reading of memory_requests should go up for the attached instance manager.&amp;lt;/pre&lt;br&gt;2.  The reading of other instance managers should not get impacted.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;8&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_instance_manager_memory_usage_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create a volume and attach it to a pod.&amp;lt;/pre&lt;br&gt;2.  Write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Set multiple recurring backup on the volume.&amp;lt;/pre&lt;br&gt;4.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;5.  Select &lt;code&gt;longhorn_instance_manager_memory_usage_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  The reading of memory_usage should go up for the attached instance manager.&amp;lt;/pre&lt;br&gt;2.  The reading of other instance managers should not get impacted.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;9&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_manager_cpu_usage_millicpu&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create 3 volumes of different sizes.&amp;lt;/pre&lt;br&gt;2.  Attach 1st volume to a pod and write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Leave the 2rd volume to the detached state.&amp;lt;/pre&lt;br&gt;4.  Attach the 3th volume to pod and write 1.5 Gi data into it. Attach the volume in maintenance mode.&amp;lt;/pre&lt;br&gt;5.  Set a recurring backup on volume 1st.&amp;lt;/pre&lt;br&gt;6.  Perform revert to snapshot with 3rd volume.&amp;lt;/pre&lt;br&gt;7.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;8.  Select &lt;code&gt;longhorn_manager_cpu_usage_millicpu&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Monitor the graph and the console on the Prometheus server, the cpu_usage should go up.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;10&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_manager_memory_usage_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create 3 volumes of different sizes.&amp;lt;/pre&lt;br&gt;2.  Attach 1st volume to a pod and write 1 Gi data into it.&amp;lt;/pre&lt;br&gt;3.  Leave the 2rd volume to the detached state.&amp;lt;/pre&lt;br&gt;4.  Attach the 3th volume to pod and write 1.5 Gi data into it. Attach the volume in maintenance mode.&amp;lt;/pre&lt;br&gt;5.  Set a recurring backup on volume 1st.&amp;lt;/pre&lt;br&gt;6.  Perform revert to snapshot with 3rd volume.&amp;lt;/pre&lt;br&gt;7.  Try to make disk full of a node where &lt;code&gt;longhorn-manager&lt;/code&gt; is running.&amp;lt;/pre&lt;br&gt;8.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;9.  Select &lt;code&gt;longhorn_manager_memory_usage_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  Monitor the graph and the console on the Prometheus server, the memory_usage should go up.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;11&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_disk_capacity_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create volumes and attach them to each node.&amp;lt;/pre&lt;br&gt;2.  Add an additional disk to all the nodes. (Different size)&amp;lt;/pre&lt;br&gt;3.  Write into the volumes.&amp;lt;/pre&lt;br&gt;4.  Power down a node.&amp;lt;/pre&lt;br&gt;5.  Disable a node.&amp;lt;/pre&lt;br&gt;6.  Add a new node in the cluster.&amp;lt;/pre&lt;br&gt;7.  Delete a node from the cluster.&amp;lt;/pre&lt;br&gt;8.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;9.  Select &lt;code&gt;longhorn_disk_capacity_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the disks should be identified by Prometheus.&amp;lt;/pre&lt;br&gt;2.  All the disks should show the correct total size of the disks.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;12&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_disk_usage_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create volumes and attach them to each node.&amp;lt;/pre&lt;br&gt;2.  Add an additional disk to all the nodes. (Different size)&amp;lt;/pre&lt;br&gt;3.  Write into the volumes.&amp;lt;/pre&lt;br&gt;4.  Power down a node.&amp;lt;/pre&lt;br&gt;5.  Disable a node.&amp;lt;/pre&lt;br&gt;6.  Add a new node in the cluster.&amp;lt;/pre&lt;br&gt;7.  Delete a node from the cluster.&amp;lt;/pre&lt;br&gt;8.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;9.  Select &lt;code&gt;longhorn_disk_usage_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the disks should be identified by Prometheus.&amp;lt;/pre&lt;br&gt;2.  All the disks should show the occupied size of the disks.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;13&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_node_capacity_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create volumes and attach them to each node.&amp;lt;/pre&lt;br&gt;2.  Add an additional disk to all the nodes. (Different size)&amp;lt;/pre&lt;br&gt;3.  Write into the volumes.&amp;lt;/pre&lt;br&gt;4.  Power down a node.&amp;lt;/pre&lt;br&gt;5.  Disable a node.&amp;lt;/pre&lt;br&gt;6.  Add a new node in the cluster.&amp;lt;/pre&lt;br&gt;7.  Delete a node from the cluster.&amp;lt;/pre&lt;br&gt;8.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;9.  Select &lt;code&gt;longhorn_node_capacity_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the nodes should be identified by Prometheus.&amp;lt;/pre&lt;br&gt;2.  All the nodes should show the total capacity available of disks available.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;14&lt;/td&gt;&#xA;          &lt;td&gt;longhorn_node_usage_bytes&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Pre-requisite:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Prometheus is setup is done and Prometheus web UI is accessible.&amp;lt;/pre&lt;br&gt;&lt;br&gt;&lt;strong&gt;Test Steps:&lt;/strong&gt;&lt;br&gt;&lt;br&gt;1.  Create volumes and attach them to each node.&amp;lt;/pre&lt;br&gt;2.  Add an additional disk to all the nodes. (Different size)&amp;lt;/pre&lt;br&gt;3.  Write into the volumes.&amp;lt;/pre&lt;br&gt;4.  Power down a node.&amp;lt;/pre&lt;br&gt;5.  Disable a node.&amp;lt;/pre&lt;br&gt;6.  Add a new node in the cluster.&amp;lt;/pre&lt;br&gt;7.  Delete a node from the cluster.&amp;lt;/pre&lt;br&gt;8.  Go to Prometheus web UI.&amp;lt;/pre&lt;br&gt;9.  Select &lt;code&gt;longhorn_node_usage_bytes&lt;/code&gt; and execute.&lt;/td&gt;&#xA;          &lt;td&gt;1.  All the nodes should be identified by Prometheus&amp;lt;/pre&lt;br&gt;2.  All the nodes should show the occupied space on all disks attached to the node.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;Note: More details can be found on &lt;a href=&#34;https://longhorn.io/docs/1.2.2/monitoring/&#34;&gt;https://longhorn.io/docs/1.2.2/monitoring/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>New Node with Custom Data Directory</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.0/new-node-custom-data-directory/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.0/new-node-custom-data-directory/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Make sure that the default &lt;code&gt;Longhorn&lt;/code&gt; setup has all nodes with&#xA;&lt;code&gt;/var/lib/rancher/longhorn/&lt;/code&gt; as the default Longhorn disk under the &lt;code&gt;Node&lt;/code&gt;&#xA;page. Additionally, check the &lt;code&gt;Setting&lt;/code&gt; page and make sure that the &amp;ldquo;Default&#xA;Data Path&amp;rdquo; setting has been set to &lt;code&gt;/var/lib/rancher/longhorn/&lt;/code&gt; by default.&lt;/li&gt;&#xA;&lt;li&gt;Now, change the &amp;ldquo;Default Data Path&amp;rdquo; setting to something else, such as&#xA;&lt;code&gt;/home&lt;/code&gt;, and save the new settings.&lt;/li&gt;&#xA;&lt;li&gt;Add a new node to the cluster with the proper dependencies to run Longhorn.&#xA;This step will vary depending on how the cluster has been deployed.&lt;/li&gt;&#xA;&lt;li&gt;Go back to the &lt;code&gt;Node&lt;/code&gt; page. The page should now list the new &lt;code&gt;node&lt;/code&gt;.&#xA;Expanding the node should show a default disk of whichever directory was&#xA;specified in step 2.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>NFSv4 Enforcement (No NFSv3 Fallback)</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/nfsv4-enforcement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/nfsv4-enforcement/</guid>
      <description>&lt;p&gt;Since the client falling back to &lt;code&gt;NFSv3&lt;/code&gt; usually results in a failure to mount the &lt;code&gt;NFS&lt;/code&gt; share, the way we can check for &lt;code&gt;NFSv3&lt;/code&gt; fallback is to check the error message returned and see if it mentions &lt;code&gt;rpc.statd&lt;/code&gt;, since dependencies on &lt;code&gt;rpc.statd&lt;/code&gt; and other services are no longer needed for &lt;code&gt;NFSv4&lt;/code&gt;, but are needed for &lt;code&gt;NFSv3&lt;/code&gt;. The &lt;code&gt;NFS&lt;/code&gt; mount &lt;strong&gt;should not&lt;/strong&gt; fall back to &lt;code&gt;NFSv3&lt;/code&gt; and instead only give the user a warning that the server may be &lt;code&gt;NFSv3&lt;/code&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Node disconnection test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-disconnection/node-disconnection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-disconnection/node-disconnection/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1545&#34;&gt;https://github.com/longhorn/longhorn/issues/1545&lt;/a&gt;&#xA;For disconnect node : &lt;a href=&#34;https://github.com/longhorn/longhorn/files/4864127/network_down.sh.zip&#34;&gt;https://github.com/longhorn/longhorn/files/4864127/network_down.sh.zip&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;If auto-salvage is disabled, the auto-reattachment behavior after the node disconnection depends on all replicas are in ERROR state or not.&lt;/p&gt;&#xA;&lt;p&gt;(1) If all replicas are in ERROR state, the volume would remain in detached/faulted state if auto-salvage is disabled.&lt;/p&gt;&#xA;&lt;p&gt;(2) If there is any healthy replica, the volume would be auto-reattached even though auto-salvage is disabled.&lt;/p&gt;&#xA;&lt;p&gt;What makes all replicas in ERROR state? When there is data writing during the disconnection:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Node drain and deletion test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/node-drain-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/node-drain-deletion/</guid>
      <description>&lt;h2 id=&#34;drain-with-force&#34;&gt;Drain with force&lt;/h2&gt;&#xA;&lt;p&gt;Make sure the volumes on the drained/removed node can be detached or recovered correctly. The related issue: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1214&#34;&gt;https://github.com/longhorn/longhorn/issues/1214&lt;/a&gt;&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy a cluster contains 3 worker nodes N1, N2, N3.&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Create a 1-replica deployment with a 3-replica Longhorn volume. The volume is attached to N1.&lt;/li&gt;&#xA;&lt;li&gt;Write some data to the volume and get the md5sum.&lt;/li&gt;&#xA;&lt;li&gt;Force drain and remove N2, which contains one replica only.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl drain &amp;lt;Node name&amp;gt; --delete-emptydir-data=true --force=true --grace-period=-1 --ignore-daemonsets=true --timeout=&amp;lt;Desired timeout in secs&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Wait for the volume Degraded.&lt;/li&gt;&#xA;&lt;li&gt;Force drain and remove N1, which is the node the volume is attached to.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl drain &amp;lt;Node name&amp;gt; --delete-emptydir-data=true --force=true --grace-period=-1 --ignore-daemonsets=true --timeout=&amp;lt;Desired timeout in secs&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify the instance manager pods are gone and not recreated after the drain.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the volume detaching then being recovered. Will get attached to the workload/node.&lt;/li&gt;&#xA;&lt;li&gt;Validate the volume content. The data is intact.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;drain-without-force&#34;&gt;Drain without force&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Cordon the node. Longhorn will automatically disable the node scheduling when a Kubernetes node is cordoned.&lt;/li&gt;&#xA;&lt;li&gt;Evict all the replicas from the node.&lt;/li&gt;&#xA;&lt;li&gt;Run the following command to drain the node with &lt;code&gt;force&lt;/code&gt; flag set to false.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl drain &amp;lt;Node name&amp;gt; --delete-emptydir-data --force=false --grace-period=-1 --ignore-daemonsets=true --timeout=&amp;lt;Desired timeout in secs&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Observe that the workloads move to another node. The volumes should first detach and attach to workloads once they move to another node.&lt;/li&gt;&#xA;&lt;li&gt;Observe the logs, one by one all the pods should get evicted.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;node/&amp;lt;node-name&amp;gt; already cordoned&#xA;WARNING: ignoring DaemonSet-managed Pods: ingress-nginx/nginx-ingress-controller-bpf2t, kube-system/canal-hwk6v, longhorn-system/engine-image-ei-605a0f3e-8gb8l, longhorn-system/longhorn-csi-plugin-flq84, longhorn-system/longhorn-manager-tps6v&#xA;evicting pod longhorn-system/instance-manager-r-1aebab59&#xA;evicting pod kube-system/coredns-849545576b-v54vn&#xA;evicting pod longhorn-system/instance-manager-e-e591dbce&#xA;pod/instance-manager-r-1aebab59 evicted&#xA;pod/instance-manager-e-e591dbce evicted&#xA;pod/coredns-849545576b-v54vn evicted&#xA;node/&amp;lt;node-name&amp;gt; evicted&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify the instance manager pods are gone and not recreated after the drain.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Note: &lt;code&gt;--ignore-daemonsets&lt;/code&gt; should be set to true to ignore some DaemonSets that exist on node such as Longhorn manager, Longhorn CSI plugin, engine image in a Longhorn deployed cluster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Physical node down</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/physical-node-down/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/physical-node-down/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;One physical node down should result in the state of that node change to &lt;code&gt;Down&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;When using with CSI driver, one node with controller (StatefulSet/Deployment) and pod down should result in Kubernetes migrate the pod to another node, and Longhorn volume should be able to be used on that node as well. Test scenarios for this are documented &lt;a href=&#34;../../../node/improve-node-failure-handling/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;In this case, RWX should be excluded.&lt;/p&gt;&#xA;&lt;p&gt;Ref: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/5900#issuecomment-1541360552&#34;&gt;https://github.com/longhorn/longhorn/issues/5900#issuecomment-1541360552&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Physical node reboot</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-restart/physical-node-reboot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-restart/physical-node-reboot/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Reboot the node that the controller (StatefulSet/Deployment) attached to. After reboot complete, the volume should be reattached to the node.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Priority Class Default Setting</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/priorityclass-default-setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/priorityclass-default-setting/</guid>
      <description>&lt;p&gt;There are three different cases we need to test when the user inputs a default setting for &lt;code&gt;Priority Class&lt;/code&gt;:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install &lt;code&gt;Longhorn&lt;/code&gt; with no &lt;code&gt;priority-class&lt;/code&gt; set in the default settings. The &lt;code&gt;Priority Class&lt;/code&gt; setting should be empty after the installation completes according to the &lt;code&gt;longhorn-ui&lt;/code&gt;, and the default &lt;code&gt;Priority&lt;/code&gt; of all &lt;code&gt;Pods&lt;/code&gt; in the &lt;code&gt;longhorn-system&lt;/code&gt; namespace should be &lt;code&gt;0&lt;/code&gt;:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;~ kubectl -n longhorn-system describe pods | grep Priority&#xA;# should be repeated many times&#xA;Priority:     0&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;Install &lt;code&gt;Longhorn&lt;/code&gt; with a nonexistent &lt;code&gt;priority-class&lt;/code&gt; in the default settings. The system should fail to come online. The &lt;code&gt;Priority Class&lt;/code&gt; setting should be set and the status of the &lt;code&gt;Daemon Set&lt;/code&gt; for the &lt;code&gt;longhorn-manager&lt;/code&gt; should indicate that the reason it failed was due to an invalid &lt;code&gt;Priority Class&lt;/code&gt;:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;~ kubectl -n longhorn-system describe lhs priority-class&#xA;Name:         priority-class&#xA;...&#xA;Value:                 nonexistent-priority-class&#xA;...&#xA;~ kubectl -n longhorn-system describe daemonset.apps/longhorn-manager&#xA;Name:           longhorn-manager&#xA;...&#xA;  Priority Class Name:  nonexistent-priority-class&#xA;Events:&#xA;  Type     Reason            Age                From                  Message&#xA;  ----     ------            ----               ----                  -------&#xA;  Normal   SuccessfulCreate  23s                daemonset-controller  Created pod: longhorn-manager-gbskd&#xA;  Normal   SuccessfulCreate  23s                daemonset-controller  Created pod: longhorn-manager-9s7mg&#xA;  Normal   SuccessfulCreate  23s                daemonset-controller  Created pod: longhorn-manager-gtl2j&#xA;  Normal   SuccessfulDelete  17s                daemonset-controller  Deleted pod: longhorn-manager-9s7mg&#xA;  Normal   SuccessfulDelete  17s                daemonset-controller  Deleted pod: longhorn-manager-gbskd&#xA;  Normal   SuccessfulDelete  17s                daemonset-controller  Deleted pod: longhorn-manager-gtl2j&#xA;  Warning  FailedCreate      4s (x14 over 15s)  daemonset-controller  Error creating: pods &amp;#34;longhorn-manager-&amp;#34; is forbidden: no PriorityClass with name nonexistent-priority-class was found&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;Install &lt;code&gt;Longhorn&lt;/code&gt; with a valid &lt;code&gt;priority-class&lt;/code&gt; in the default settings. The &lt;code&gt;Priority Class&lt;/code&gt; setting should be set according to the &lt;code&gt;longhorn-ui&lt;/code&gt;, and all the &lt;code&gt;Pods&lt;/code&gt; in the &lt;code&gt;longhorn-system&lt;/code&gt; namespace should have the right &lt;code&gt;Priority&lt;/code&gt; set:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;~ kubectl -n longhorn-system describe pods | grep Priority&#xA;# should be repeated many times&#xA;Priority:             2000001000&#xA;Priority Class Name:  system-node-critical&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Prometheus Support</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/prometheus_support/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/prometheus_support/</guid>
      <description>&lt;p&gt;Prometheus Support allows user to monitor the longhorn metrics. The details are available at &lt;a href=&#34;https://longhorn.io/docs/1.1.0/monitoring/&#34;&gt;https://longhorn.io/docs/1.1.0/monitoring/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;monitor-longhorn&#34;&gt;Monitor longhorn&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy the Prometheus-operator, ServiceMonitor pointing to longhorn-backend and Prometheus as mentioned in the doc.&lt;/li&gt;&#xA;&lt;li&gt;Create an ingress pointing to Prometheus service.&lt;/li&gt;&#xA;&lt;li&gt;Access the Prometheus web UI using the ingress created in the step 2.&lt;/li&gt;&#xA;&lt;li&gt;Select the metrics from below to monitor the longhorn resources.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;longhorn_volume_actual_size_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_volume_capacity_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_volume_robustness&lt;/li&gt;&#xA;&lt;li&gt;longhorn_volume_state&lt;/li&gt;&#xA;&lt;li&gt;longhorn_instance_manager_cpu_requests_millicpu&lt;/li&gt;&#xA;&lt;li&gt;longhorn_instance_manager_cpu_usage_millicpu&lt;/li&gt;&#xA;&lt;li&gt;longhorn_instance_manager_memory_requests_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_instance_manager_memory_usage_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_manager_cpu_usage_millicpu&lt;/li&gt;&#xA;&lt;li&gt;longhorn_manager_memory_usage_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_count_total&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_status&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_cpu_capacity_millicpu&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_cpu_usage_millicpu&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_memory_capacity_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_memory_usage_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_storage_capacity_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_storage_reservation_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_node_storage_usage_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_disk_capacity_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_disk_reservation_bytes&lt;/li&gt;&#xA;&lt;li&gt;longhorn_disk_usage_bytes&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Deploy workloads which use Longhorn volumes into the cluster. Verify that there is no abnormal data. e.g: volume capacity is 0, cpu usage is over 4000 milicpu etc.&lt;/li&gt;&#xA;&lt;li&gt;Attach a volume to a node. Detach the volume and attach it to a different node. Verify that the volume&amp;rsquo;s information is reported by at most 1 longhorn-manager at any time.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;configure-prometheus-alert-manager&#34;&gt;Configure Prometheus alert manager&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy the Alertmanager as mentioned in the doc.&lt;/li&gt;&#xA;&lt;li&gt;Modify the alert configuration file and set email or slack.&lt;/li&gt;&#xA;&lt;li&gt;Deploy a service using node port to access web UI of the alert manager as mentioned in the doc.&lt;/li&gt;&#xA;&lt;li&gt;Follow the steps from the doc to create PrometheusRule and configure the Prometheus server.&lt;/li&gt;&#xA;&lt;li&gt;Go beyond the threshold set for PrometheusRule in the step 4.&lt;/li&gt;&#xA;&lt;li&gt;Verify the email or slack, user should get the alert message.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;monitor-with-grafana&#34;&gt;Monitor with Grafana&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a ConfigMap referring to the Prometheus. (Refer the doc)&lt;/li&gt;&#xA;&lt;li&gt;Deploy the Grafana and a service to access the UI.&lt;/li&gt;&#xA;&lt;li&gt;Go to Grafana dashboard and import prebuilt longhorn example.&lt;/li&gt;&#xA;&lt;li&gt;Verify the graphs and data are available to monitor.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;monitor-with-rancher-app&#34;&gt;Monitor with Rancher app&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a cluster in Rancher. (1 etcd/control plane and 3 worker nodes)&lt;/li&gt;&#xA;&lt;li&gt;Deploy longhorn v1.1.0.&lt;/li&gt;&#xA;&lt;li&gt;Enable the monitoring for a project.&lt;/li&gt;&#xA;&lt;li&gt;Deploy the ServiceMonitor pointing to longhorn-backend.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: monitoring.coreos.com/v1&#xA;kind: ServiceMonitor&#xA;metadata:&#xA;  name: longhorn-prometheus-servicemonitor&#xA;  namespace: longhorn-system&#xA;  labels:&#xA;    name: longhorn-prometheus-servicemonitor&#xA;spec:&#xA;  selector:&#xA;  matchLabels:&#xA;    app: longhorn-manager&#xA;  namespaceSelector:&#xA;    matchNames:&#xA;    - longhorn-system&#xA;  endpoints:&#xA;  - port: manager&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Access the url provided by the app to access Prometheus or Grafana.&lt;/li&gt;&#xA;&lt;li&gt;Verify the longhorn metrics are available to monitor.&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;a href=&#34;https://v1-1-0.longhorn.io/docs/1.1.0/monitoring/kubelet-volume-metrics/&#34;&gt;kubelet_volume_*&lt;/a&gt; metrics are available if Rancher 2.5 monitoring app is deployed.&lt;/li&gt;&#xA;&lt;li&gt;Import &lt;a href=&#34;https://grafana.com/grafana/dashboards/13032&#34;&gt;Longhorn Example dashboard&lt;/a&gt;. Verify that the graph looks good.&lt;/li&gt;&#xA;&lt;li&gt;Setup alert and alert rules in Rancher monitoring app. Verify that alerts are working ok.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>PVC provisioning with insufficient storage</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/resiliency/pvc_provisioning_with_insufficient_storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/resiliency/pvc_provisioning_with_insufficient_storage/</guid>
      <description>&lt;h4 id=&#34;related-issue&#34;&gt;Related Issue:&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/4654&#34;&gt;https://github.com/longhorn/longhorn/issues/4654&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/3529&#34;&gt;https://github.com/longhorn/longhorn/issues/3529&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;root-cause-analysis&#34;&gt;Root Cause Analysis&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/4654#issuecomment-1264870672&#34;&gt;https://github.com/longhorn/longhorn/issues/4654#issuecomment-1264870672&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;This case need to be tested on both RWO/RWX volumes&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with size larger than 8589934591 GiB.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Deployment keep in pending status, RWO/RWX volume will keep in a create -&amp;gt; delete loop.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create a PVC with size &amp;lt;= 8589934591 GiB, but greater than the actual available space size.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RWO/RWX volume will be created, and volume will have annotation &amp;ldquo;longhorn.io/volume-scheduling-error&amp;rdquo;: &amp;ldquo;insufficient storage volume scheduling failure&amp;rdquo; in it.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create a PVC with size &amp;lt; the actual available space size，Resize the PVC to a not schedulable size&#xA;&lt;ul&gt;&#xA;&lt;li&gt;After resize PVC to a not schedulable size, both RWO/RWX were still in scheduling status.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;We can modify/use &lt;a href=&#34;https://raw.githubusercontent.com/longhorn/longhorn/master/examples/rwx/rwx-nginx-deployment.yaml&#34;&gt;https://raw.githubusercontent.com/longhorn/longhorn/master/examples/rwx/rwx-nginx-deployment.yaml&lt;/a&gt; to deploy RWO/RWX PVC for this test&lt;/p&gt;</description>
    </item>
    <item>
      <title>Re-deploy CSI components when their images change</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/update_csi_components_when_images_change/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/update_csi_components_when_images_change/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn&lt;/li&gt;&#xA;&lt;li&gt;Change the &lt;code&gt;longhorn-driver-deployer&lt;/code&gt; yaml at &lt;a href=&#34;https://github.com/longhorn/longhorn-manager/blob/c2ceb9f3f991810f811601d8c41c09b67fb50746/deploy/install/02-components/04-driver.yaml#L50&#34;&gt;https://github.com/longhorn/longhorn-manager/blob/c2ceb9f3f991810f811601d8c41c09b67fb50746/deploy/install/02-components/04-driver.yaml#L50&lt;/a&gt; to use the new images for some CSI components&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;Kubectl apply -f&lt;/code&gt; the &lt;code&gt;longhorn-driver-deployer&lt;/code&gt; yaml&lt;/li&gt;&#xA;&lt;li&gt;Verify that only CSI components with the new images are re-deployed and have new images&lt;/li&gt;&#xA;&lt;li&gt;Redeploy &lt;code&gt;longhorn-driver-deployer&lt;/code&gt; without changing the images.&lt;/li&gt;&#xA;&lt;li&gt;Verify that no CSI component is re-deployed&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Recurring backup job interruptions</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/recurring-backup-job-interruptions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/recurring-backup-job-interruptions/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related Issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1882&#34;&gt;https://github.com/longhorn/longhorn/issues/1882&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;scenario-1--allow-recurring-job-while-volume-is-detached-disabled-attached-pod-scaled-down-while-the-recurring-backup-was-in-progress&#34;&gt;Scenario 1- &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt; disabled, attached pod scaled down while the recurring backup was in progress.&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume, attach to a pod of a statefulSet, and write 800 Mi data into it.&lt;/li&gt;&#xA;&lt;li&gt;Set a recurring job.&lt;/li&gt;&#xA;&lt;li&gt;While the recurring job is in progress, scale down the pod to 0 of the statefulSet.&lt;/li&gt;&#xA;&lt;li&gt;Volume first detached and cron job gets finished saying unable to complete the backup.&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume again gets auto attached to another node and cron job gets recreated.&lt;/li&gt;&#xA;&lt;li&gt;Verify after backup completion, the volume gets detached.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;scenario-2--allow-recurring-job-while-volume-is-detached-enabled-attached-pod-scaled-down-while-the-recurring-backup-was-in-progress&#34;&gt;Scenario 2- &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt; enabled, attached pod scaled down while the recurring backup was in progress.&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a volume, attach to a pod, and write 800 Mi data into it.&lt;/li&gt;&#xA;&lt;li&gt;Set a recurring job.&lt;/li&gt;&#xA;&lt;li&gt;While the recurring job is in progress, scale down the pod to 0.&lt;/li&gt;&#xA;&lt;li&gt;Volume first detached and cron job gets completed saying unable to complete the backup.&lt;/li&gt;&#xA;&lt;li&gt;Verify volume again gets auto attached to another node and cron job gets recreated.&lt;/li&gt;&#xA;&lt;li&gt;Verify after backup completion, the volume gets detached.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;scenario-3--cron-job-and-volume-attached-to-the-same-node-node-is-powered-down-and-volume-detached-manually&#34;&gt;Scenario 3- Cron job and volume attached to the same node, Node is powered down and volume detached manually.&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Attach a volume to pod of a statefulSet, write data into it and set a recurring backup.&lt;/li&gt;&#xA;&lt;li&gt;Detach from the pod by scaling down the statefulSet.&lt;/li&gt;&#xA;&lt;li&gt;The attached node to volume is power down when the recurring job backup was in progress.&lt;/li&gt;&#xA;&lt;li&gt;The volume is manually detached while the cron job remains in unknown state.&lt;/li&gt;&#xA;&lt;li&gt;The cron job remains in unknown state for about 7 mins and then another pod gets created.&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume get attached to another node and once the job is completed, volume gets detached.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;scenario-4--cron-job-and-volume-attached-to-different-node-node-is-powered-down&#34;&gt;Scenario 4- Cron job and volume attached to different node, Node is powered down.&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Attach a volume to pod of a statefulSet, write data into it and set a recurring backup.&lt;/li&gt;&#xA;&lt;li&gt;Detach from the pod by scaling down the statefulSet.&lt;/li&gt;&#xA;&lt;li&gt;The attached node to volume is power down when the recurring job backup was in progress.&lt;/li&gt;&#xA;&lt;li&gt;Another cron job pod gets created with logs in the previous pod as below.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;time=&amp;#34;2020-11-09T07:39:10Z&amp;#34; level=info msg=&amp;#34;Automatically attach volume volume-test-2 to node node-1&amp;#34;&#xA;time=&amp;#34;2020-11-09T07:39:12Z&amp;#34; level=info msg=&amp;#34;Volume volume-test-2 is in state attached&amp;#34;&#xA;time=&amp;#34;2020-11-09T07:39:12Z&amp;#34; level=info msg=&amp;#34;Running recurring backup for volume volume-test-2&amp;#34;&#xA;time=&amp;#34;2020-11-09T07:39:22Z&amp;#34; level=debug msg=&amp;#34;Creating backup , current progress 0&amp;#34;&#xA;time=&amp;#34;2020-11-09T07:39:27Z&amp;#34; level=debug msg=&amp;#34;Creating backup , current progress 4&amp;#34;&#xA;time=&amp;#34;2020-11-09T07:40:42Z&amp;#34; level=info msg=&amp;#34;Automatically detach the volume volume-test-2&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume get attached to another node and once the job is completed, volume gets detached.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;scenario-5--cron-job-and-volume-attached-to-the-same-node-node-is-restarted&#34;&gt;Scenario 5- Cron job and volume attached to the same node, Node is restarted.&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Attach a volume to pod of a statefulSet, write data into it and set a recurring backup.&lt;/li&gt;&#xA;&lt;li&gt;Detach from the pod by scaling down the statefulSet.&lt;/li&gt;&#xA;&lt;li&gt;The attached node to volume is power down when the recurring job backup was in progress.&lt;/li&gt;&#xA;&lt;li&gt;The volume is manually detached while the cron job remains in unknown state.&lt;/li&gt;&#xA;&lt;li&gt;Power on the node.&lt;/li&gt;&#xA;&lt;li&gt;The cron job which was stuck in unknown state get removed and new cron job get recreated.&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume get attached to another node and the backup job is completed.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;scenario-6--cron-job-and-volume-attached-to-the-samedifferent-node-node-is-powered-down-and-pod-deletion-policy-when-node-is-downis-set-as-delete-both-statefulset-and-deployment-pod&#34;&gt;Scenario 6- Cron job and volume attached to the same/different node, Node is powered down and &lt;code&gt;Pod Deletion Policy When Node is Down&lt;/code&gt;is set as &lt;code&gt;delete-both-statefulset-and-deployment-pod&lt;/code&gt;&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Enable &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Attach a volume to pod of a statefulSet, write data into it and set a recurring backup.&lt;/li&gt;&#xA;&lt;li&gt;The attached node to volume is power down when the recurring job backup was in progress.&lt;/li&gt;&#xA;&lt;li&gt;The cron job (if on the same node) and the pod remains in unknown state for about 7 mins and then another pod gets created for the cron job (if on the same node) and statefulSet. If cron job is on another node, it fails to complete the backup and tries to create new job to complete backup which fails with error &lt;code&gt;level=fatal msg=&amp;quot;Error taking snapshot: failed to complete backupAndCleanup for pvc-4feb233e-9503-4d4b-8cda-a5bdf005b146: could not get volume-head for volume pvc-4feb233e-9503-4d4b-8cda-a5bdf005b146: Bad response statusCode [500]. Status [500 Internal Server Error]. Body: [message=fail to get snapshot: cannot get client for volume pvc-4feb233e-9503-4d4b-8cda-a5bdf005b146: engine is not running, code=Server Error, detail=] from [http://longhorn-backend:9500/v1/volumes/pvc-4feb233e-9503-4d4b-8cda-a5bdf005b146?action=snapshotGet]&amp;quot;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;After 7 min the cron job takes over the creation of another pod of stateful set and the volume gets auto attached to another node, completes the backup and gets detached.&lt;/li&gt;&#xA;&lt;li&gt;Verify the statefulSet pod successfully reattaches to the volume after sometime.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Replica Rebuilding</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/replica-rebuilding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/ha/replica-rebuilding/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create and attach a volume.&lt;/li&gt;&#xA;&lt;li&gt;Write a large amount of data to the volume.&lt;/li&gt;&#xA;&lt;li&gt;Disable disk scheduling and the node scheduling for one replica.&lt;/li&gt;&#xA;&lt;li&gt;Crash the replica progress. Verify&#xA;&lt;ol&gt;&#xA;&lt;li&gt;the corresponding replica will become ERROR.&lt;/li&gt;&#xA;&lt;li&gt;the volume will keep robustness Degraded.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Enable the disk scheduling. Verify nothing changes.&lt;/li&gt;&#xA;&lt;li&gt;Enable the node scheduling. Verify.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;the failed replica is reused by Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;the rebuilding progress in UI page looks good.&lt;/li&gt;&#xA;&lt;li&gt;the data content is correct after rebuilding.&lt;/li&gt;&#xA;&lt;li&gt;volume r/w works fine.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Direct delete one replica via UI. Verify&#xA;&lt;ol&gt;&#xA;&lt;li&gt;a new replica will be replenished immediately.&lt;/li&gt;&#xA;&lt;li&gt;the rebuilding progress in UI page looks good.&lt;/li&gt;&#xA;&lt;li&gt;the data content is correct after rebuilding.&lt;/li&gt;&#xA;&lt;li&gt;volume r/w works fine.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Restore to a new cluster</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/cluster-restore/restore-to-a-new-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/cluster-restore/restore-to-a-new-cluster/</guid>
      <description>&lt;h4 id=&#34;back-up-the-old-cluster&#34;&gt;Back up the old cluster&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy the 1st cluster then install Longhorn system and Velero.&lt;/li&gt;&#xA;&lt;li&gt;Deploy some workloads using Longhorn volumes then write some data:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;A simple pod using multiple volumes. And some volumes are using backing images.&lt;/li&gt;&#xA;&lt;li&gt;A StatefulSet.&lt;/li&gt;&#xA;&lt;li&gt;A Deployment with a RWX volume.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Config some recurring policies for the volumes.&lt;/li&gt;&#xA;&lt;li&gt;Create backups for all volumes.&lt;/li&gt;&#xA;&lt;li&gt;Create a cluster backup via Velero.&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;velero backup create lh-cluster --exclude-resources persistentvolumes,persistentvolumeclaims,backuptargets.longhorn.io,backupvolumes.longhorn.io,backups.longhorn.io,nodes.longhorn.io,volumes.longhorn.io,engines.longhorn.io,replicas.longhorn.io,backingimagedatasources.longhorn.io,backingimagemanagers.longhorn.io,backingimages.longhorn.io,sharemanagers.longhorn.io,instancemanagers.longhorn.io,engineimages.longhorn.io&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;restore-to-a-new-cluster&#34;&gt;Restore to a new cluster&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy the 2nd cluster then install Velero only. You can try with different cluster config (more nodes or disks) here.&lt;/li&gt;&#xA;&lt;li&gt;Restore the cluster backup. e.g.,&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;velero restore create --from-backup lh-cluster&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;Removing all old instance manager pods and backing image manager pods from namespace &lt;code&gt;longhorn-system&lt;/code&gt;. Since there is no corresponding InstanceManager CR or BackingImageManager CR for these old pods.&lt;/li&gt;&#xA;&lt;li&gt;Re-config nodes and disks for the restored Longhorn system if necessary.&lt;/li&gt;&#xA;&lt;li&gt;Re-create backing images.&lt;/li&gt;&#xA;&lt;li&gt;Restore all Longhorn volumes from the remote backup target.&lt;/li&gt;&#xA;&lt;li&gt;Update the access mode to &lt;code&gt;ReadWriteMany&lt;/code&gt; since all restored volumes are mode &lt;code&gt;ReadWriteOnce&lt;/code&gt; by default.&lt;/li&gt;&#xA;&lt;li&gt;Create PVCs and PVs with previous names for the restored volumes.&lt;/li&gt;&#xA;&lt;li&gt;Verify all workloads work fine with correct data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;GitHub issue link: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/3367&#34;&gt;https://github.com/longhorn/longhorn/issues/3367&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Restore to an old cluster</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/cluster-restore/restore-to-an-old-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/cluster-restore/restore-to-an-old-cluster/</guid>
      <description>&lt;p&gt;Notice that the behaviors will be different if the cluster node roles are different. e.g., A cluster contains 1 dedicated master node + 3 worker node is different from a cluster contains 3 nodes which are both master and worker.&#xA;This test may need to be validated for both kind of cluster.&lt;/p&gt;&#xA;&lt;h2 id=&#34;node-creation-and-deletion&#34;&gt;Node creation and deletion&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy a 3-worker-node cluster then install Longhorn system.&lt;/li&gt;&#xA;&lt;li&gt;Deploy some workloads using Longhorn volumes then write some data.&lt;/li&gt;&#xA;&lt;li&gt;Create a cluster snapshot via Rancher.&lt;/li&gt;&#xA;&lt;li&gt;Add a new worker node for this cluster. Deploy workloads on this node.&lt;/li&gt;&#xA;&lt;li&gt;Restore the cluster.&lt;/li&gt;&#xA;&lt;li&gt;Follow the doc after the restore. Verify:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The new node is still in the Longhorn system. All necessary longhorn workloads will be on the node.&lt;/li&gt;&#xA;&lt;li&gt;The workloads and the volumes created in step 2 work fine after restarting.&lt;/li&gt;&#xA;&lt;li&gt;The data of the volumes created in step 4 can be recovered.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create one more cluster snapshot.&lt;/li&gt;&#xA;&lt;li&gt;Delete one node all related volumes/replicas on the node.&lt;/li&gt;&#xA;&lt;li&gt;Restore the cluster.&lt;/li&gt;&#xA;&lt;li&gt;Follow the doc after the restore. Verify:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;There is no corresponding Longhorn node CR.&lt;/li&gt;&#xA;&lt;li&gt;The Longhorn pods on the deleted node will be restored but they will become &lt;code&gt;Terminating&lt;/code&gt; after several minutes. Users need to force deleting them.&lt;/li&gt;&#xA;&lt;li&gt;The volumes or replicas CRs on the gone node will be restored. Users can clean up them.&lt;/li&gt;&#xA;&lt;li&gt;The workloads and the volumes not on the removed node work fine after restarting.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;longhorn-volume-recovery&#34;&gt;Longhorn volume recovery&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Launch a Longhorn system. Deploy one more compatible engine image.&lt;/li&gt;&#xA;&lt;li&gt;Prepare some volumes with workloads:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create and attach volume A via UI. Write some data and do some snapshot operations. (Validate 1 case: The volume deleted after the cluster snapshot will be restored but is invalid)&lt;/li&gt;&#xA;&lt;li&gt;Deploy a single pod with volume B. Write some data, do some snapshot operations, and create some backups. (Validate 3 cases: &amp;lt;1&amp;gt; data modification won&amp;rsquo;t crash the whole volume; &amp;lt;2&amp;gt; backup info will be resynced; &amp;lt;3&amp;gt; users need to manual restart the single pod)&lt;/li&gt;&#xA;&lt;li&gt;Deploy a StatefulSet with volume C. Write some data and do some snapshot operations. (Validate 1 case: Users need to manually recover the volume if all existing replicas are replaced by new replicas)&lt;/li&gt;&#xA;&lt;li&gt;Deploy a StatefulSet with volume D. Write some data and do some snapshot operations. (Validate 2 cases: &amp;lt;1&amp;gt; volume can be recovered automatically if some replicas are removed and some new replicas are replenished; &amp;lt;2&amp;gt; snapshot info will be resynced;)&lt;/li&gt;&#xA;&lt;li&gt;Deploy a Deployment with volume E. Write some data and do some snapshot operations. (Validate 4 cases: &amp;lt;1&amp;gt; engine upgrade; &amp;lt;2&amp;gt; offline expansion)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create a cluster snapshot via Rancher.&lt;/li&gt;&#xA;&lt;li&gt;Do the followings before the restore:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Delete volume A.&lt;/li&gt;&#xA;&lt;li&gt;Write more data to volume B and create more backups.&lt;/li&gt;&#xA;&lt;li&gt;Remove all current replicas one by one for volume C. Then all replicas of volume C are new replicas.&lt;/li&gt;&#xA;&lt;li&gt;Remove some replicas for volume D. Do snapshot creation, deletion, and revert.&lt;/li&gt;&#xA;&lt;li&gt;Scale down the workload. Upgrade volume E from the default image to another engine image. And do expansion.&lt;/li&gt;&#xA;&lt;li&gt;Create and attach volume F via UI. Write some data and do some snapshot operations. (Validate 1 case: Users need to manuall recover the volume if it&amp;rsquo;s created after the cluster snapshot)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Restore the cluster.&lt;/li&gt;&#xA;&lt;li&gt;Check the followings according to the doc:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Volume A is back. But there is no data in it. And users can re-delete it.&lt;/li&gt;&#xA;&lt;li&gt;Volume B can be reattached or keep attached with correct data. The backup info of volume B is resynced when the volume is reattahed. The pod can use the volume after restart.&lt;/li&gt;&#xA;&lt;li&gt;All old removed replicas are back and all newly rebuilt replicas in step4-3 disappear for volume C. There is no data in volume C. The data directories of the disappeared replicas are still on the node. Hence the data are be recovered by exporting a single replica volume.&lt;/li&gt;&#xA;&lt;li&gt;The old removed replicas are back and the newly rebuilt replicas in step4-4 disappear for volume D. The restored replicas will become failed then get rebuilt with correct data. The data directories of the disappeared replicas are still on the node.&lt;/li&gt;&#xA;&lt;li&gt;Volume E re-uses the default engine image, and gets stuck in shrinking the expanded size to the original size. By re-scaling down the workload, re-upgrade and re-expand the volume. The volume can work fine then.&lt;/li&gt;&#xA;&lt;li&gt;Volume F will disappear. The data directories of the disappeared replicas are still on the node. Hence the data are be recovered by exporting a single replica volume.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;longhorn-system-upgrade&#34;&gt;Longhorn system upgrade&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn system. Deploy one more compatible engine image.&lt;/li&gt;&#xA;&lt;li&gt;Deploy some workloads using Longhorn volumes then write some data.&lt;/li&gt;&#xA;&lt;li&gt;Create a cluster snapshot via Rancher.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade the Longhorn system to a newer version. Then modify the settings or node configs (especially the configs introduced in the new version).&lt;/li&gt;&#xA;&lt;li&gt;Restore the cluster.&lt;/li&gt;&#xA;&lt;li&gt;Follow the doc after the restore. Verify:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The system re-upgrade should succeed.&lt;/li&gt;&#xA;&lt;li&gt;The modifications for the settings or configs in step 4 won&amp;rsquo;t be back. But users can re-modify them.&lt;/li&gt;&#xA;&lt;li&gt;The workloads and the volumes created in step 2 work fine after restarting.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;GitHub issue link: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2228&#34;&gt;https://github.com/longhorn/longhorn/issues/2228&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Return an error when fail to remount a volume</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/error-fail-remount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/error-fail-remount/</guid>
      <description>&lt;h3 id=&#34;case-1-volume-with-a-corrupted-filesystem-try-to-remount&#34;&gt;Case 1: Volume with a corrupted filesystem try to remount&lt;/h3&gt;&#xA;&lt;p&gt;Steps to reproduce bug:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume of size 1GB, say &lt;code&gt;terminate-immediatly&lt;/code&gt; volume.&lt;/li&gt;&#xA;&lt;li&gt;Create PV/PVC from the volume &lt;code&gt;terminate-immediatly&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a deployment of 1 pod with image &lt;code&gt;ubuntu:xenial&lt;/code&gt; and the PVC &lt;code&gt;terminate-immediatly&lt;/code&gt; in default namespace&lt;/li&gt;&#xA;&lt;li&gt;Find the node on which the pod is scheduled to. Let&amp;rsquo;s say the node is &lt;code&gt;Node-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;ssh into &lt;code&gt;Node-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;destroy the filesystem of &lt;code&gt;terminate-immediatly&lt;/code&gt; by running command  &lt;code&gt;dd if=/dev/zero of=/dev/longhorn/terminate-immediatly&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Find and kill the engine instance manager in &lt;code&gt;Node-X&lt;/code&gt;. Longhorn manager will notice that the instance manager is down and try to bring up a new instance manager e for &lt;code&gt;Node-X&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;After bringing up the instance manager e, Longhorn manager will try to remount the volume &lt;code&gt;terminate-immediatly&lt;/code&gt;. The remounting should fail bc we already destroyed the filesystem of the volume.&lt;/li&gt;&#xA;&lt;li&gt;We should see this log message&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[longhorn-manager-xv5th] time=&amp;#34;2020-06-23T18:13:15Z&amp;#34; level=info msg=&amp;#34;Event(v1.ObjectReference{Kind:\&amp;#34;Volume\&amp;#34;, Namespace:\&amp;#34;longhorn-system\&amp;#34;, Name:\&amp;#34;terminate-immediatly\&amp;#34;, UID:\&amp;#34;de6ae587-fc7c-40bd-b513-47175ddddf97\&amp;#34;, APIVersion:\&amp;#34;longhorn.io/v1beta1\&amp;#34;, ResourceVersion:\&amp;#34;4088981\&amp;#34;, FieldPath:\&amp;#34;\&amp;#34;}): type: &amp;#39;Warning&amp;#39; reason: &amp;#39;Remount&amp;#39; cannot proceed to remount terminate-immediatly on phan-cluster-v3-worker1: cannot get the filesystem type by using the command blkid /dev/longhorn/terminate-immediatly | sed &amp;#39;s/.*TYPE=//g&amp;#39;&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;case-2-volume-with-no-filesystem-try-to-remount&#34;&gt;Case 2: Volume with no filesystem try to remount&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume of size 1GB, say &lt;code&gt;terminate-immediatly&lt;/code&gt; volume.&lt;/li&gt;&#xA;&lt;li&gt;Attach volume &lt;code&gt;terminate-immediatly&lt;/code&gt; to a node, say &lt;code&gt;Node-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Find and kill the engine instance manager in &lt;code&gt;Node-1&lt;/code&gt;. Longhorn manager will notice that the instance manager is down and try to bring up a new instance manager e for &lt;code&gt;Node-1&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;After bringing up the instance manager e, Longhorn manager will try to remount the volume &lt;code&gt;terminate-immediatly&lt;/code&gt;. The remounting should fail bc the volume does not have a filesystem.&lt;/li&gt;&#xA;&lt;li&gt;We should see that Longhorn reattached but skip the remount the volume &lt;code&gt;terminate-immediatly&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume can be detached.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Reusing failed replica for rebuilding</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/reusing-failed-replica-for-rebuilding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/reusing-failed-replica-for-rebuilding/</guid>
      <description>&lt;h2 id=&#34;longhorn-upgrade-with-node-down-and-removal&#34;&gt;Longhorn upgrade with node down and removal&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Launch Longhorn v1.0.x&lt;/li&gt;&#xA;&lt;li&gt;Create and attach a volume, then write data to the volume.&lt;/li&gt;&#xA;&lt;li&gt;Directly remove a Kubernetes node, and shut down a node.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the related replicas failure. Then record &lt;code&gt;replica.Spec.DiskID&lt;/code&gt; for the failed replicas.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade to Longhorn master&lt;/li&gt;&#xA;&lt;li&gt;Verify the Longhorn node related to the removed node is gone.&lt;/li&gt;&#xA;&lt;li&gt;Verify&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;code&gt;replica.Spec.DiskID&lt;/code&gt; on the down node is updated and the field of the replica on the gone node is unchanged.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;replica.Spec.DataPath&lt;/code&gt; for all replicas becomes empty.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Remove all unscheduled replicas.&lt;/li&gt;&#xA;&lt;li&gt;Power on the down node. Wait for the failed replica on the down node being reused.&lt;/li&gt;&#xA;&lt;li&gt;Wait for a new replica being replenished and available.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;replica-not-available-for-reuse-after-disk-migration&#34;&gt;Replica not available for reuse after disk migration&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy longhorn v1.1.0&lt;/li&gt;&#xA;&lt;li&gt;Create and attach a volume, then write data to the volume.&lt;/li&gt;&#xA;&lt;li&gt;Directly remove a Kubernetes node which has a replica on it.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the related replicas failure.&lt;/li&gt;&#xA;&lt;li&gt;Verify the Longhorn node related to the removed node is gone.&lt;/li&gt;&#xA;&lt;li&gt;Ssh to the node and crash the replica folder or make it readonly.&lt;/li&gt;&#xA;&lt;li&gt;Add the node in the cluster again.&lt;/li&gt;&#xA;&lt;li&gt;Verify a new replica being rebuilt and available.&lt;/li&gt;&#xA;&lt;li&gt;Verify the data of the replica.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Set Tolerations/PriorityClass For System Components</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/tolerations_priorityclass_setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/tolerations_priorityclass_setting/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2120&#34;&gt;https://github.com/longhorn/longhorn/issues/2120&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Manual Tests:&lt;/p&gt;&#xA;&lt;h3 id=&#34;case-1-existing-longhorn-installation&#34;&gt;Case 1: Existing Longhorn installation&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn master.&lt;/li&gt;&#xA;&lt;li&gt;Change toleration in UI setting&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn.io/last-applied-tolerations&lt;/code&gt; annotation and &lt;code&gt;toleration&lt;/code&gt; of manager, drive deployer, UI are not changed.&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn.io/last-applied-tolerations&lt;/code&gt; annotation and &lt;code&gt;toleration&lt;/code&gt; for managed components (CSI components, IM pods, share manager pod, EI daemonset, backing-image-manager, cronjob) are updated correctly&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-2-new-installation-by-helm&#34;&gt;Case 2: New installation by Helm&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn master, set tolerations like:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;defaultSettings&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;taintToleration&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;key=value:NoSchedule&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;longhornManager&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;priorityClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;~&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;key&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Equal&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;value&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NoSchedule&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;longhornDriver&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;priorityClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;~&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;key&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Equal&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;value&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NoSchedule&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;longhornUI&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;priorityClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;~&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;key&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Equal&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;value&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NoSchedule   &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;Verify that the toleration is added for: IM pods, Share Manager pods, CSI deployments, CSI daemonset, the backup jobs, manager, drive deployer, UI&lt;/li&gt;&#xA;&lt;li&gt;Uninstall the Helm release.&#xA;Verify that uninstalling job has the same toleration as Longhorn manager.&#xA;Verify that the uninstallation success.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-3-upgrading-from-helm&#34;&gt;Case 3: Upgrading from Helm&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn v1.0.2 using Helm, set tolerations using Longhorn UI&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to master version, verify that &lt;code&gt;longhorn.io/managed-by: longhorn-manager&lt;/code&gt; is not set for manager, driver deployer and UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn.io/managed-by: longhorn-manager&lt;/code&gt; label is added for:  IM CRs, EI CRs, Share Manager CRs, IM pods, Share Manager pods, CSI services, CSI deployments, CSI daemonset.&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn.io/last-applied-tolerations&lt;/code&gt; is set for: IM pods, Share Manager pods, CSI deployments, CSI daemonset&lt;/li&gt;&#xA;&lt;li&gt;Edit the tolerations using Longhorn UI and verify the tolerations get updated for components other than Longhorn manager, driver deployer and UI only. Longhorn manager, driver deployer and UI pods should not get restarted.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade the chart to specify toleration for manager, drive deployer, UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify that the toleration get applied&lt;/li&gt;&#xA;&lt;li&gt;Repeat this test case with Longhorn v1.1.0 in step 1&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-4-upgrading-from-kubectl&#34;&gt;Case 4: Upgrading from kubectl&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn v1.0.2 using kubectl, set tolerations using Longhorn UI&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to master version, verify that &lt;code&gt;longhorn.io/managed-by: longhorn-manager&lt;/code&gt; is not set for manager, driver deployer and UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn.io/managed-by: longhorn-manager&lt;/code&gt; label is added for:  IM CRs, EI CRs, Share Manager CRs, IM pods, Share Manager pods, CSI services, CSI deployments, CSI daemonset.&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn.io/last-applied-tolerations&lt;/code&gt; is set for: IM pods, Share Manager pods, CSI deployments, CSI daemonset&lt;/li&gt;&#xA;&lt;li&gt;Edit the tolerations using Longhorn UI and verify the tolerations get updated for components other than Longhorn manager, driver deployer and UI only. Longhorn manager, driver deployer and UI pods should not get restarted.&lt;/li&gt;&#xA;&lt;li&gt;Edit the Yaml to specify toleration for manager, drive deployer, UI and upgrade Longhorn using kubectl command.&lt;/li&gt;&#xA;&lt;li&gt;Verify that the toleration get applied&lt;/li&gt;&#xA;&lt;li&gt;Repeat this test case with Longhorn v1.1.0 in step 1&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-5-node-with-taints&#34;&gt;Case 5: Node with taints&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Add some taints to all node in the cluster, e.g., &lt;code&gt;key=value:NoSchedule&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Repeate case 2, 3, 4&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-6-priority-class-ui&#34;&gt;Case 6: Priority Class UI&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Change Priority Class setting in Longhorn UI&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn only updates the managed components&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-7-priority-class-helm&#34;&gt;Case 7: Priority Class Helm&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Change Priority Class in Helm for manager, driver, UI&lt;/li&gt;&#xA;&lt;li&gt;Verify that only priority class name of manager, driver, UI get updated&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Setup and test storage network</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-storage-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-storage-network/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2285&#34;&gt;https://github.com/longhorn/longhorn/issues/2285&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-storage-network&#34;&gt;Test storage network&lt;/h2&gt;&#xA;&lt;h3 id=&#34;create-aws-instances&#34;&gt;Create AWS instances&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Create VPC.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VPC only&lt;/li&gt;&#xA;&lt;li&gt;IPv4 CIDR 10.0.0.0/16&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Create an internet gateway.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Attach to VPC&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Add the internet gateway to the VPC &lt;code&gt;Main route table&lt;/code&gt;, &lt;code&gt;Routes&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Destination 0.0.0.0/0&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Create 2 subnets in the VPC.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Subnet-1: 10.0.1.0/24&lt;/li&gt;&#xA;&lt;li&gt;Subnet-2: 10.0.2.0/24&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Launch 3 EC2 instances.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use the created VPC&lt;/li&gt;&#xA;&lt;li&gt;Use subnet-1 for network interface 1&lt;/li&gt;&#xA;&lt;li&gt;Use subnet-2 for network interface 2&lt;/li&gt;&#xA;&lt;li&gt;Disable &lt;code&gt;Auto-assign public IP&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Add security group inbound rule to allow &lt;code&gt;All traffic&lt;/code&gt; from &lt;code&gt;Anywhere-IPv4&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Stop &lt;code&gt;Source/destination check&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Create 3 elastic IPs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Setup and test storage network when Multus version is above v4.0.0</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-storage-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-storage-network/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/6953&#34;&gt;https://github.com/longhorn/longhorn/issues/6953&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-storage-network&#34;&gt;Test storage network&lt;/h2&gt;&#xA;&lt;h3 id=&#34;create-aws-instances&#34;&gt;Create AWS instances&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Create VPC.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;VPC only&lt;/li&gt;&#xA;&lt;li&gt;IPv4 CIDR 10.0.0.0/16&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Create an internet gateway.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Attach to VPC&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Add the internet gateway to the VPC &lt;code&gt;Main route table&lt;/code&gt;, &lt;code&gt;Routes&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Destination 0.0.0.0/0&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Create 2 subnets in the VPC.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Subnet-1: 10.0.1.0/24&lt;/li&gt;&#xA;&lt;li&gt;Subnet-2: 10.0.2.0/24&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Launch 3 EC2 instances.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use the created VPC&lt;/li&gt;&#xA;&lt;li&gt;Use subnet-1 for network interface 1&lt;/li&gt;&#xA;&lt;li&gt;Use subnet-2 for network interface 2&lt;/li&gt;&#xA;&lt;li&gt;Disable &lt;code&gt;Auto-assign public IP&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Add security group inbound rule to allow &lt;code&gt;All traffic&lt;/code&gt; from &lt;code&gt;Anywhere-IPv4&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Stop &lt;code&gt;Source/destination check&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Create 3 elastic IPs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Single replica node down</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/single-replica-node-down/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/single-replica-node-down/</guid>
      <description>&lt;h2 id=&#34;related-issues&#34;&gt;Related Issues&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2329&#34;&gt;https://github.com/longhorn/longhorn/issues/2329&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2309&#34;&gt;https://github.com/longhorn/longhorn/issues/2309&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/3957&#34;&gt;https://github.com/longhorn/longhorn/issues/3957&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;default-setting&#34;&gt;Default Setting&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;Automatic salvage&lt;/code&gt; is enabled.&lt;/p&gt;&#xA;&lt;h2 id=&#34;node-restartdown-scenario-with-pod-deletion-policy-when-node-is-down-set-to-default-value-do-nothing&#34;&gt;Node restart/down scenario with &lt;code&gt;Pod Deletion Policy When Node is Down&lt;/code&gt; set to default value &lt;code&gt;do-nothing&lt;/code&gt;.&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create RWO|RWX volume with replica count = 1 &amp;amp; data locality = enabled|disabled|strict-local.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For data locality = strict-local, use RWO volume to do test.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create deployment|statefulset for volume.&lt;/li&gt;&#xA;&lt;li&gt;Power down node of volume/replica.&lt;/li&gt;&#xA;&lt;li&gt;The workload pod will get stuck in the &lt;code&gt;terminating&lt;/code&gt; state.&lt;/li&gt;&#xA;&lt;li&gt;Volume will fail to attach since volume is not ready (i.e remains faulted, since single replica is on downed node).&lt;/li&gt;&#xA;&lt;li&gt;Power up node or delete the workload pod so that kubernetes will recreate pod on another node.&lt;/li&gt;&#xA;&lt;li&gt;Verify auto salvage finishes (i.e pod completes start).&lt;/li&gt;&#xA;&lt;li&gt;Verify volume attached &amp;amp; accessible by pod (i.e test data is available).&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For data locality = strict-local volume, volume wiil keep in detaching, attaching status for about 10 minutes, after volume attached to node which replica located, check volume healthy and pod status.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;node-restartdown-scenario-with-pod-deletion-policy-when-node-is-down-set-to-delete-both-statefulset-and-deployment-pod&#34;&gt;Node restart/down scenario with &lt;code&gt;Pod Deletion Policy When Node is Down&lt;/code&gt; set to &lt;code&gt;delete-both-statefulset-and-deployment-pod&lt;/code&gt;&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create RWO|RWX volume with replica count = 1 &amp;amp; data locality = enabled|disabled|strict-local.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For data locality = strict-local, use RWO volume to do test.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Create deployment|statefulset for volume.&lt;/li&gt;&#xA;&lt;li&gt;Power down node of volume/replica.&lt;/li&gt;&#xA;&lt;li&gt;Volume will become faulted.&lt;/li&gt;&#xA;&lt;li&gt;Wait for pod deletion &amp;amp; recreation on another node.&#xA;The pod recreation will not happen immediately.&lt;/li&gt;&#xA;&lt;li&gt;The replacement workload pod will get stuck in the &lt;code&gt;ContainerCreating&lt;/code&gt; state.&lt;/li&gt;&#xA;&lt;li&gt;Power on node of volume/replica.&lt;/li&gt;&#xA;&lt;li&gt;Verify the auto salvage finishes for volumes.&lt;/li&gt;&#xA;&lt;li&gt;Verify volume attached &amp;amp; accessible by pod (i.e test data is available).&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Snapshot while writing data in the volume</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/basic-operations/snapshot-while-writing-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/basic-operations/snapshot-while-writing-data/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2187&#34;&gt;https://github.com/longhorn/longhorn/issues/2187&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;scenario&#34;&gt;Scenario&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a kubernetes pod + pvc that mounts a Longhorn volume.&lt;/li&gt;&#xA;&lt;li&gt;Write 5 Gib into the pod using &lt;code&gt;dd if=/dev/urandom of=/mnt/&amp;lt;volume&amp;gt; count=5000 bs=1M conv=fsync status=progress&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;While running the above command initiate a snapshot.&lt;/li&gt;&#xA;&lt;li&gt;Verify the logs of the instance-manager using &lt;code&gt;kubetail instance-manager -n longhorn-system&lt;/code&gt;. There should some logs related to freezing and unfreezing the filesystem. Like &lt;code&gt;Froze filesystem of volume mounted ...&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify snapshot succeeded and &lt;code&gt;dd&lt;/code&gt; operation will complete.&lt;/li&gt;&#xA;&lt;li&gt;Create another snapshot/backup and verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Note: The above issue is still open and the scenario will not work.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Storage Network Test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/basic-operations/storage-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/basic-operations/storage-network/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2285&#34;&gt;https://github.com/longhorn/longhorn/issues/2285&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-multus-version-below-v400&#34;&gt;Test Multus version below v4.0.0&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Set up the Longhorn environment as mentioned &lt;a href=&#34;https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-storage-network/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Run Longhorn core tests on the environment.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; All the tests should pass.&lt;/p&gt;&#xA;&lt;h2 id=&#34;related-issue-1&#34;&gt;Related issue:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/6953&#34;&gt;https://github.com/longhorn/longhorn/issues/6953&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-multus-version-above-v400&#34;&gt;Test Multus version above v4.0.0&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Set up the Longhorn environment as mentioned &lt;a href=&#34;https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-storage-network/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Run Longhorn core tests on the environment.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; All the tests should pass.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Support Kubelet Volume Metrics</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/kubelet_volume_metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/kubelet_volume_metrics/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;&#xA;&lt;p&gt;Kubelet exposes &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/4b24dca228d61f4d13dcd57b46465b0df74571f6/pkg/kubelet/metrics/collectors/volume_stats.go#L27&#34;&gt;kubelet_volume_stats_* metrics&lt;/a&gt;.&#xA;Those metrics measure PVC&amp;rsquo;s filesystem related information inside a Longhorn block device.&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-steps&#34;&gt;Test steps:&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a cluster and set up this monitoring system: &lt;a href=&#34;https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack&#34;&gt;https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Install Longhorn. Deploy some workloads using Longhorn volumes.&#xA;Make sure there are some workloads using Longhorn PVCs in  &lt;code&gt;volumeMode: Block&lt;/code&gt; and some workloads using Longhorn PVCs in &lt;code&gt;volumeMode: Filesystem&lt;/code&gt;.&#xA;See &lt;a href=&#34;https://longhorn.io/docs/1.0.2/references/examples/&#34;&gt;https://longhorn.io/docs/1.0.2/references/examples/&lt;/a&gt; for examples.&lt;/li&gt;&#xA;&lt;li&gt;Create ingress to Prometheus server and Grafana.&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Prometheus server, verify that all Longhorn PVCs in &lt;code&gt;volumeMode: Filesystem&lt;/code&gt; show up in metrics: &lt;code&gt;kubelet_volume_stats_capacity_bytes kubelet_volume_stats_available_bytes kubelet_volume_stats_used_bytes kubelet_volume_stats_inodes kubelet_volume_stats_inodes_free kubelet_volume_stats_inodes_used&lt;/code&gt;.&lt;br&gt;&#xA;Verify that all Longhorn PVCs in &lt;code&gt;volumeMode: Block&lt;/code&gt; do not show up.&lt;/li&gt;&#xA;&lt;li&gt;Write/Delete files in Longhorn volumes.&#xA;Verify that the Prometheus server shows the correct status of Longhorn PVCs.&#xA;Verify that alerts get fired when we go above &lt;a href=&#34;https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/templates/prometheus/rules/kubernetes-storage.yaml?rgh-link-date=2020-09-24T13%3A00%3A06Z&#34;&gt;the thresholds&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Navigate to Grafana, navigate to &lt;code&gt;Kubernetes/Persistent Volumes&lt;/code&gt; dashboard.&#xA;Verify that graphs report correct data.&lt;/li&gt;&#xA;&lt;li&gt;Negative test case:&#xA;use Longhorn UI to delete a volume of a running pod,&#xA;verify that the PVC corresponding volume still exists but it stops showing up in &lt;code&gt;kubelet_volume_stats_*&lt;/code&gt; metrics&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test `Rebuild` in volume.meta blocks engine start</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-rebuild-in-meta-blocks-engine-start/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-rebuild-in-meta-blocks-engine-start/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/6626&#34;&gt;https://github.com/longhorn/longhorn/issues/6626&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-with-patched-image&#34;&gt;Test with patched image&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; a patched longhorn-engine image with the following code change.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;diff --git a/pkg/sync/sync.go b/pkg/sync/sync.go&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;index b48ddd46..c4523f11 100644&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;--- a/pkg/sync/sync.go&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;+++ b/pkg/sync/sync.go&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;@@ -534,9 +534,9 @@ func (t *Task) reloadAndVerify(address, instanceName string, repClient *replicaC&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;                return err&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-       if err := repClient.SetRebuilding(false); err != nil {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-               return err&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-       }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;+       // if err := repClient.SetRebuilding(false); err != nil {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;+       //      return err&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;+       // }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;&lt;/span&gt;        return nil&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;And&lt;/strong&gt; a patched longhorn-instance-manager image with the longhorn-engine vendor updated.&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; Longhorn is installed with the patched images.&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; the &lt;code&gt;data-locality&lt;/code&gt; setting is set to &lt;code&gt;disabled&lt;/code&gt;.&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; the &lt;code&gt;auto-salvage&lt;/code&gt; setting is set to &lt;code&gt;true&lt;/code&gt;.&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; a new StorageClass is created with &lt;code&gt;NumberOfReplica&lt;/code&gt; set to &lt;code&gt;1&lt;/code&gt;.&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; a StatefulSet is created with &lt;code&gt;Replica&lt;/code&gt; set to &lt;code&gt;1&lt;/code&gt;.&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; the node of the StatefulSet Pod and the node of its volume Replica are different. This is necessary to trigger the rebuilding in reponse to the data locality setting update later.&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; Volume have 1 running Replica.&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; data exists in the volume.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test access style for S3 compatible backupstore</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/test-s3-access-style/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/test-s3-access-style/</guid>
      <description>&lt;h3 id=&#34;case-1-using-alibaba-cloud-oss-bucket-as-backupstore&#34;&gt;Case 1: Using Alibaba Cloud OSS bucket as backupstore&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create an OSS bucket within Region China in Alibaba Cloud(Aliyun).&lt;/li&gt;&#xA;&lt;li&gt;Create a secret without &lt;code&gt;VIRTUAL_HOSTED_STYLE&lt;/code&gt; for the OSS bucket.&lt;/li&gt;&#xA;&lt;li&gt;Set backup target and the secret in Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Try to list backup. Then the error &lt;code&gt;error: AWS Error: SecondLevelDomainForbidden Please use virtual hosted style to access.&lt;/code&gt; is triggered.&lt;/li&gt;&#xA;&lt;li&gt;Add &lt;code&gt;VIRTUAL_HOSTED_STYLE: dHJ1ZQ== # true&lt;/code&gt; to the secret.&lt;/li&gt;&#xA;&lt;li&gt;Backup list/create/delete/restore work fine after the configuration.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-2-using-aws-s3-bucket-as-backupstore&#34;&gt;Case 2: Using AWS S3 bucket as backupstore&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a secret without &lt;code&gt;VIRTUAL_HOSTED_STYLE&lt;/code&gt; for the S3 bucket.&lt;/li&gt;&#xA;&lt;li&gt;Set backup target and the secret in Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify backup list/create/delete/restore work fine without the configuration.&lt;/li&gt;&#xA;&lt;li&gt;Add &lt;code&gt;VIRTUAL_HOSTED_STYLE: dHJ1ZQ== # true&lt;/code&gt; to the secret.&lt;/li&gt;&#xA;&lt;li&gt;Verify backup list/create/delete/restore still work fine after the configuration.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test Additional Printer Columns</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/additional-printer-columns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/additional-printer-columns/</guid>
      <description>&lt;p&gt;For each of the case below:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Fresh installation of Longhorn. (make sure to delete all Longhorn CRDs before installation)&lt;/li&gt;&#xA;&lt;li&gt;Upgrade from older version.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Run:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl get &amp;lt;LONGHORN-CRD&amp;gt; -n longhorn-system&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Verify that the output contains information as specify in the &lt;code&gt;additionalPrinerColumns&lt;/code&gt;&#xA;at &lt;a href=&#34;https://github.com/longhorn/longhorn-manager/blob/master/deploy/install/01-prerequisite/03-crd.yaml&#34;&gt;here&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test backing image</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.0/test-backing-image-upload/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.0/test-backing-image-upload/</guid>
      <description>&lt;h2 id=&#34;test-upload&#34;&gt;Test upload&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare a large backing image file (make sure the size is greater than 1Gi and the uploading time is longer than 1 minute) in local.&lt;/li&gt;&#xA;&lt;li&gt;Click the backing image creation button in UI, choose &lt;code&gt;Upload From Local&lt;/code&gt;, select the file then start upload.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the initialization complete. Then the upload progress will be shown.&lt;/li&gt;&#xA;&lt;li&gt;During the uploading, verify the corresponding backing image data source pod won&amp;rsquo;t use too many CPU (50 ~ 200m) and memory(50 ~ 200Mi) resources.&lt;/li&gt;&#xA;&lt;li&gt;Open another backing image UI page, the progress can be still found in the backing image detail page.&lt;/li&gt;&#xA;&lt;li&gt;When the upload is in progress, refresh the UI page to interrupt the upload.&lt;/li&gt;&#xA;&lt;li&gt;Verified that the upload failed without retry (typically the retry will happen after 1~2 minute). And there is a message indicates the failure.&lt;/li&gt;&#xA;&lt;li&gt;Delete the failed one then restart the uploading by creating a new backing image.&lt;/li&gt;&#xA;&lt;li&gt;Create and attach a volume with the backing image. Verify the data content is correct.&lt;/li&gt;&#xA;&lt;li&gt;Do cleanup.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;upload-via-rancher-uri&#34;&gt;Upload via Rancher URI&lt;/h3&gt;&#xA;&lt;p&gt;&lt;sup&gt;Related issue: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/3129&#34;&gt;3129&lt;/a&gt; with fix in Longhorn v1.3.0+&lt;/sup&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test backing image checksum mismatching</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.3/test-backing-image-checksum-mismatching/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.3/test-backing-image-checksum-mismatching/</guid>
      <description>&lt;h3 id=&#34;test-step&#34;&gt;Test step&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Modify setting &lt;code&gt;Backing Image Recovery Wait Interval&lt;/code&gt; to a shorter value so that the backing image will start auto recovery eariler.&lt;/li&gt;&#xA;&lt;li&gt;Create a backing image file with type &lt;code&gt;Download From URL&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Launch a volume using the backing image file so that there are 2 disk records for the backing image.&lt;/li&gt;&#xA;&lt;li&gt;Modify one disk file for the backing image and make sure the file size is not changed. This will lead to data inconsistency/corruption later. e.g.,&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;root@shuo-cluster-worker-2:/# echo test &amp;gt; /var/lib/longhorn/backing-images/bi-test-5cea928b/backing &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;root@shuo-cluster-worker-2:/# truncate -s 500M /var/lib/longhorn/backing-images/bi-test-5cea928b/backing&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;5&#34;&gt;&#xA;&lt;li&gt;Remove another disk file then crash backing image manager processes for &lt;strong&gt;both&lt;/strong&gt; files &lt;strong&gt;immediately and simultaneously&lt;/strong&gt;. e.g.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;root@shuo-cluster-worker-2:/var/lib/longhorn/backing-images# ps aux | grep backing&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;root      &lt;span style=&#34;color:#ae81ff&#34;&gt;577081&lt;/span&gt;  0.1  0.2 &lt;span style=&#34;color:#ae81ff&#34;&gt;1454408&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;20740&lt;/span&gt; ?       SLsl 10:00   0:06 backing-image-manager --debug daemon --listen 0.0.0.0:8000&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;root      &lt;span style=&#34;color:#ae81ff&#34;&gt;650943&lt;/span&gt;  1.5  0.8 &lt;span style=&#34;color:#ae81ff&#34;&gt;745556&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;71096&lt;/span&gt; ?        SLsl 11:11   0:01 longhorn-manager -d daemon --engine-image longhornio/longhorn-engine:master-head --instance-manager-image longhornio/longhorn-instance-manager:v1_20210731 --share-manager-image longhornio/longhorn-share-manager:v1_20211020 --backing-image-manager-image shuowu/backing-image-manager:v2_20211025-1 --manager-image shuowu/longhorn-manager:4a8782e4-dirty-2 --service-account longhorn-service-account&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;root      &lt;span style=&#34;color:#ae81ff&#34;&gt;653188&lt;/span&gt;  0.0  0.0   &lt;span style=&#34;color:#ae81ff&#34;&gt;6432&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;740&lt;/span&gt; pts/1    S+   11:13   0:00 grep --color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;auto backing&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;root@shuo-cluster-worker-3:~# ps aux | grep backing&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;root     &lt;span style=&#34;color:#ae81ff&#34;&gt;2198716&lt;/span&gt;  0.0  0.2 &lt;span style=&#34;color:#ae81ff&#34;&gt;1528140&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;20600&lt;/span&gt; ?       SLsl 10:00   0:03 backing-image-manager --debug daemon --listen 0.0.0.0:8000&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;root     &lt;span style=&#34;color:#ae81ff&#34;&gt;2290980&lt;/span&gt;  1.5  0.9 &lt;span style=&#34;color:#ae81ff&#34;&gt;745556&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;76248&lt;/span&gt; ?        SLsl 11:11   0:01 longhorn-manager -d daemon --engine-image longhornio/longhorn-engine:master-head --instance-manager-image longhornio/longhorn-instance-manager:v1_20210731 --share-manager-image longhornio/longhorn-share-manager:v1_20211020 --backing-image-manager-image shuowu/backing-image-manager:v2_20211025-1 --manager-image shuowu/longhorn-manager:4a8782e4-dirty-2 --service-account longhorn-service-account&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;root     &lt;span style=&#34;color:#ae81ff&#34;&gt;2293575&lt;/span&gt;  0.0  0.0   &lt;span style=&#34;color:#ae81ff&#34;&gt;6432&lt;/span&gt;   &lt;span style=&#34;color:#ae81ff&#34;&gt;676&lt;/span&gt; pts/1    S+   11:13   0:00 grep --color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;auto backing&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;root@shuo-cluster-worker-3:~# rm /var/lib/longhorn/backing-images/bi-test-5cea928b/backing &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; kill -9 &lt;span style=&#34;color:#ae81ff&#34;&gt;2198716&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;6&#34;&gt;&#xA;&lt;li&gt;Check the backing image:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The state of both files will become &lt;code&gt;unknown&lt;/code&gt; then &lt;code&gt;failed&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;The error message of the modified file is like &lt;code&gt;backing image expected checksum xxx doesn&#39;t match the existing file checksum xxxx&lt;/code&gt;) then &lt;code&gt;stat /data/backing-images/xxx/backing: no such file or directory&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;The current checksum of the backing image keeps unchanged.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Wait for a while then there will be a backing image data source pod restarting the download. After re-downloading, the backing image will get recovered.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test backing image download to local</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-backing-image-download-to-local/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-backing-image-download-to-local/</guid>
      <description>&lt;h3 id=&#34;test-step&#34;&gt;Test step&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create and attach a volume (recommended volume size &amp;gt; 1Gi).&lt;/li&gt;&#xA;&lt;li&gt;Write some data into the file then calculate the SHA512 checksum of the volume block device.&lt;/li&gt;&#xA;&lt;li&gt;Create a backing image from the above volume. And wait for the 1st backing image file ready.&lt;/li&gt;&#xA;&lt;li&gt;Download the backing image to local via UI (Clicking button &lt;code&gt;Download&lt;/code&gt; in &lt;code&gt;Operation&lt;/code&gt; list of the backing image).&#xA;=&amp;gt; Verify the downloaded file checksum is the same as the volume checksum &amp;amp; the backing image current checksum (when &lt;code&gt;Exported Backing Image Type&lt;/code&gt; is raw).&lt;/li&gt;&#xA;&lt;li&gt;Create and attach the volume with the backing image. Wait for the attachment complete.&lt;/li&gt;&#xA;&lt;li&gt;Re-download the backing image to local. =&amp;gt; Verify the downloaded file checksum still matches.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;GitHub Issue: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/3155&#34;&gt;https://github.com/longhorn/longhorn/issues/3155&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Backing Image during Longhorn upgrade</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/backing-image-during-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/backing-image-during-upgrade/</guid>
      <description>&lt;h2 id=&#34;system-upgrade-with-compatible-backing-image-manager-image&#34;&gt;System upgrade with compatible backing image manager image&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy Longhorn. Then set &lt;code&gt;Concurrent Automatic Engine Upgrade Per Node Limit&lt;/code&gt; to a positive value to enable volume engine auto upgrade.&lt;/li&gt;&#xA;&lt;li&gt;Create 2 backing images: a large one and a small one. Longhorn will start preparing the 1st file for both backing image immediately via launching backing image data source pods.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the small backing image being ready in the 1st disk. Then create and attach volumes with the backing image.&lt;/li&gt;&#xA;&lt;li&gt;Wait for volumes attachment. Verify the backing image content then write random data in the volumes.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the large backing image being ready in the 1st disk. Then create and attach one more volume with this large backing image.&lt;/li&gt;&#xA;&lt;li&gt;Before the large backing image is synced to other nodes and the volume becomes attached, upgrade the whole Longhorn system:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;A new engine image will be used.&lt;/li&gt;&#xA;&lt;li&gt;The default backing image manager image will be updated.&lt;/li&gt;&#xA;&lt;li&gt;The new longhorn manager is compatible with the old backing image manager.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Wait for system upgrade complete. Then verify:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;All old backing image manager and the related pod will be cleaned up automatically after the current downloading is complete. And the existing backing image files won&amp;rsquo;t be removed.&lt;/li&gt;&#xA;&lt;li&gt;New default backing image manager will take over all backing image ownerships and show the info in the status map:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;For the small backing image, the new backing image manager will directly take over all ready files.&lt;/li&gt;&#xA;&lt;li&gt;For the large backing image, the new backing image manager will take over the only ready file and mark all in-progress files as failed first. Then it will re-sync the files after the backoff window.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;All attached volumes still work fine without replica crash, and the content is correct in the volumes during/after the upgrade.&lt;/li&gt;&#xA;&lt;li&gt;The last volume get attached successfully without replica crash, and the content is correct.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Verify volumes and backing images can be deleted.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;system-upgrade-with-incompatible-backing-image-manager-image&#34;&gt;System upgrade with incompatible backing image manager image&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Create a backing images. Wait for the backing image being ready in the 1st disk.&lt;/li&gt;&#xA;&lt;li&gt;Create and attach volumes with the backing image.&lt;/li&gt;&#xA;&lt;li&gt;Wait for volumes attachment. Verify the backing image content then write random data in the volumes.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade the whole Longhorn system:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The default backing image manager image will be updated.&lt;/li&gt;&#xA;&lt;li&gt;The new longhorn manager is not compatible with the old backing image manager.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Wait for system upgrade complete. Then verify:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;All old incompatible backing image manager and the related pod will be cleaned up automatically.&lt;/li&gt;&#xA;&lt;li&gt;New default backing image manager will take over all backing image ownerships and show the info in the status map.&lt;/li&gt;&#xA;&lt;li&gt;All attached volumes still work fine without replica crash, and the content is correct in the volumes during/after the upgrade.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;system-upgrade-with-the-same-backing-image-manager-image&#34;&gt;System upgrade with the same backing image manager image&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Create a backing images. Wait for the backing image being ready in the 1st disk.&lt;/li&gt;&#xA;&lt;li&gt;Create and attach volumes with the backing image. Wait for all disk files of the backing image being ready.&lt;/li&gt;&#xA;&lt;li&gt;Run &lt;code&gt;kubectl -n longhorn system get pod -w&lt;/code&gt; in a seperate session.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn manager but with the backing image manager image unchanged. (Actually we can mock this upgrade by removing all longhorn manager pods simultaneously.)&lt;/li&gt;&#xA;&lt;li&gt;Check if all disk file status of the backing image becomes &lt;code&gt;unknown&lt;/code&gt; then &lt;code&gt;ready&lt;/code&gt; during the longhorn manager pods termination and restart. (May need to refresh the UI page after restart.)&lt;/li&gt;&#xA;&lt;li&gt;After the longhorn manager pods restart, Verify there is no backing image data source pod launched for the backing image in the output of step4.&lt;/li&gt;&#xA;&lt;li&gt;Repeat step4 ~ step8 for 10 times.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;available-test-backing-image-urls&#34;&gt;Available test backing image URLs:&lt;/h4&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&#xA;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.raw&#xA;https://cloud-images.ubuntu.com/minimal/releases/focal/release-20200729/ubuntu-20.04-minimal-cloudimg-amd64.img&#xA;https://github.com/rancher/k3os/releases/download/v0.11.0/k3os-amd64.iso &#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;the-way-to-generate-a-longhorn-manager-image-with-higher-api-version&#34;&gt;The way to generate a longhorn-manager image with higher API version&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Download longhorn manager repo with command &lt;code&gt;git clone https://github.com/longhorn/longhorn-manager.git&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Increase the constant &lt;code&gt;CurrentBackingImageManagerAPIVersion&lt;/code&gt; in &lt;code&gt;longhorn-manager/engineapi/backing_image_manager.go&lt;/code&gt; by 1.&lt;/li&gt;&#xA;&lt;li&gt;Run &lt;code&gt;make&lt;/code&gt; to build a longhorn-manager image then push it to docker hub.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;the-way-to-generate-a-backing-image-manager-image-with-higher-api-version&#34;&gt;The way to generate a backing-image-manager image with higher API version&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Download backing image manager repo with command &lt;code&gt;git clone https://github.com/longhorn/backing-image-manager.git&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Increase the constants &lt;code&gt;BackingImageManagerAPIVersion&lt;/code&gt; and &lt;code&gt;BackingImageManagerAPIMinVersion&lt;/code&gt; in &lt;code&gt;backing-image-manager/pkg/meta/version.go&lt;/code&gt; by 1.&lt;/li&gt;&#xA;&lt;li&gt;Run &lt;code&gt;make&lt;/code&gt; to build a longhorn-manager image then push it to docker hub.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test backing image space usage with sparse files</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.3/test-backing-image-space-usage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.3/test-backing-image-space-usage/</guid>
      <description>&lt;h3 id=&#34;prerequisite&#34;&gt;Prerequisite&lt;/h3&gt;&#xA;&lt;p&gt;A sparse file should be prepared before test. e.g.:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;~ touch empty-filesystem.raw&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;~ truncate -s 500M empty-filesystem.raw&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;~ mkfs.ext4 empty-filesystem.raw&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mke2fs 1.46.1 &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;9-Feb-2021&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Creating filesystem with &lt;span style=&#34;color:#ae81ff&#34;&gt;512000&lt;/span&gt; 1k blocks and &lt;span style=&#34;color:#ae81ff&#34;&gt;128016&lt;/span&gt; inodes&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Filesystem UUID: fe6cfb58-134a-42b3-afab-59474d9515e0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Superblock backups stored on blocks:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#x9;8193, 24577, 40961, 57345, 73729, 204801, 221185, &lt;span style=&#34;color:#ae81ff&#34;&gt;401409&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Allocating group tables: &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Writing inode tables: &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Creating journal &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8192&lt;/span&gt; blocks&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Writing superblocks and filesystem accounting information: &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;~ shasum -a &lt;span style=&#34;color:#ae81ff&#34;&gt;512&lt;/span&gt; empty-filesystem.raw&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;4277f6174bf43d1f03f328eaf507f4baf84a645d79239b3ef4593a87b5127ceb097d540281e1f3557d9ad1d2591135bbcf24db480c1bd732b633b93cf4fe50c9  empty-filesystem.raw&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For convenience, the above example file is already uploaded to the S3 server. The URL is &lt;code&gt;https://longhorn-backing-image.s3.us-west-1.amazonaws.com/empty-filesystem.raw&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Backup Creation With Old Engine Image</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.0/backup-creation-with-old-engine-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.0/backup-creation-with-old-engine-image/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2897&#34;&gt;https://github.com/longhorn/longhorn/issues/2897&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-step&#34;&gt;Test Step&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; with Longhorn v1.2.0-rc2 or above.&#xA;&lt;em&gt;And&lt;/em&gt; deploy engine image &lt;code&gt;oldEI&lt;/code&gt; older than v1.2.0 (for example: &lt;code&gt;longhornio/longhorn-engine:v1.1.2&lt;/code&gt;).&#xA;&lt;em&gt;And&lt;/em&gt; create volume &lt;code&gt;vol-old-engine&lt;/code&gt;.&#xA;&lt;em&gt;And&lt;/em&gt; attach volume &lt;code&gt;vol-old-engine&lt;/code&gt; to one of a node.&#xA;&lt;em&gt;And&lt;/em&gt; upgrade volume &lt;code&gt;vol-old-engine&lt;/code&gt; to engine image &lt;code&gt;oldEI&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; create backup of volume &lt;code&gt;vol-old-engine&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; watch kubectl &lt;code&gt;kubectl get backups.longhorn.io -l backup-volume=vol-old-engine -w&lt;/code&gt;.&#xA;&lt;em&gt;And&lt;/em&gt; should see two backups temporarily (in transition state).&#xA;&lt;em&gt;And&lt;/em&gt; should see only one backup be left after a while.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test backup listing S3/NFS</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/stress/backup-listing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/stress/backup-listing/</guid>
      <description>&lt;h2 id=&#34;1-backup-listing-with-more-than-1000-backups---s3&#34;&gt;1. Backup listing with more than 1000 backups - S3&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy Longhorn on a kubernetes cluster.&lt;/li&gt;&#xA;&lt;li&gt;Set up S3 backupStore.&lt;/li&gt;&#xA;&lt;li&gt;Create a volume of 2Gi and attach it to a pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data into it and compute md5sum.&lt;/li&gt;&#xA;&lt;li&gt;Open browser developer tool.&lt;/li&gt;&#xA;&lt;li&gt;Create one backup by clicking LH GUI (It&amp;rsquo;ll call &lt;code&gt;snapshotCreate&lt;/code&gt; and &lt;code&gt;snapshotBackup&lt;/code&gt; APIs).&lt;/li&gt;&#xA;&lt;li&gt;Copy the &lt;code&gt;snapshotBackup&lt;/code&gt; API call, right click &lt;code&gt;Copy&lt;/code&gt; -&amp;gt; &lt;code&gt;Copy as cURL&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Run the curl command over 1k times in the Shell.&lt;/li&gt;&#xA;&lt;li&gt;On LH GUI, click the Backup -&amp;gt; Volume Name to display total volume backups, you should see all the backup backups listed on the page.&lt;/li&gt;&#xA;&lt;li&gt;Restore any backup and verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;2-backup-listing-with-more-than-1000-backups---nfs&#34;&gt;2. Backup listing with more than 1000 backups - NFS&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Repeat the steps from test scenario &lt;code&gt;1. Backup listing with more than 1000 backups - S3&lt;/code&gt; with NFS backupStore.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;3-backup-listing-of-volume-bigger-than-200-gi---s3&#34;&gt;3. Backup listing of volume bigger than 200 Gi - S3&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy Longhorn on a kubernetes cluster.&lt;/li&gt;&#xA;&lt;li&gt;Set up S3 backupStore.&lt;/li&gt;&#xA;&lt;li&gt;Create a volume &lt;code&gt;vol-1&lt;/code&gt; of 250Gi and attach it to a pod.&lt;/li&gt;&#xA;&lt;li&gt;Write data of 240Gi.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup.&lt;/li&gt;&#xA;&lt;li&gt;Go to the backup page and click on the volume &lt;code&gt;vol-1&lt;/code&gt; to list the backups, you should see the backup getting listed.&lt;/li&gt;&#xA;&lt;li&gt;Verify the size of data by restoring it.&lt;/li&gt;&#xA;&lt;li&gt;Create another volume &lt;code&gt;vol-2&lt;/code&gt; of 200Gi and attach it to a pod.&lt;/li&gt;&#xA;&lt;li&gt;Write data of 150Gi.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup.&lt;/li&gt;&#xA;&lt;li&gt;Go to the backup page, you should be able to see both the backups.&lt;/li&gt;&#xA;&lt;li&gt;Click the volume &lt;code&gt;vol-2&lt;/code&gt;, you should be able to see the backup.&lt;/li&gt;&#xA;&lt;li&gt;Write 40Gi in the &lt;code&gt;vol-2&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Take one more backup and verify the listing of the backup in the backup page.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; - Use the same cluster from the test scenario-1 where 1k backups are already there to increase the stress on the system.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test cases to reproduce attachment-detachment issues</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/test-cases-to-reproduce-attach-detach-issues/attachment-detachment-issues-reproducibility/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/test-cases-to-reproduce-attach-detach-issues/attachment-detachment-issues-reproducibility/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt; Have an environment with just with 2 worker nodes or taint 1 out of 3 worker node to be &lt;code&gt;NoExecute&lt;/code&gt; &amp;amp; &lt;code&gt;NoSchedule&lt;/code&gt;.&#xA;This will serve as a constrained fallback and limited source of recovery in the event of failure.&lt;/p&gt;&#xA;&lt;h4 id=&#34;1-kill-the-engines-and-instance-manager-repeatedly&#34;&gt;1. Kill the engines and instance manager repeatedly&lt;/h4&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; 1 RWO and 1 RWX volume is attached to a pod.&#xA;And Both the volumes have 2 replicas.&#xA;And Random data is continuously being written to the volume using command &lt;code&gt;dd if=/dev/urandom of=file1 count=100 bs=1M conv=fsync status=progress oflag=direct,sync&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test CronJob For Volumes That Are Detached For A Long Time</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.2/delete-cronjob-for-detached-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.2/delete-cronjob-for-detached-volumes/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2513&#34;&gt;https://github.com/longhorn/longhorn/issues/2513&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;steps&#34;&gt;Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Make sure the setting &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt; is &lt;code&gt;disabled&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a volume. Attach to a node. Create a recurring backup job that run every minute.&lt;/li&gt;&#xA;&lt;li&gt;Wait for the cronjob to be scheduled a few times.&lt;/li&gt;&#xA;&lt;li&gt;Detach the volume.&lt;/li&gt;&#xA;&lt;li&gt;Verify that the CronJob get deleted.&lt;/li&gt;&#xA;&lt;li&gt;Wait 2 hours (&amp;gt; 100 mins).&lt;/li&gt;&#xA;&lt;li&gt;Attach the volume to a node.&lt;/li&gt;&#xA;&lt;li&gt;Verify that the CronJob get created.&lt;/li&gt;&#xA;&lt;li&gt;Verify that Kubernetes schedules a run for the CronJob at the beginning of the next minute.&lt;/li&gt;&#xA;&lt;li&gt;Make sure the setting &lt;code&gt;Allow Recurring Job While Volume Is Detached&lt;/code&gt; is &lt;code&gt;enabled&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Detach the volume.&lt;/li&gt;&#xA;&lt;li&gt;Verify that the CronJob is not deleted.&lt;/li&gt;&#xA;&lt;li&gt;Wait for Kubernetes to schedule a run for the CronJob at the beginning of the next minute.&#xA;Verify that the volume is attached, CronJob runs, then volume is detached.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test CSI plugin liveness probe</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-csi-plugin-liveness-probe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-csi-plugin-liveness-probe/</guid>
      <description>&lt;h2 id=&#34;related-discussion&#34;&gt;Related discussion&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/3907&#34;&gt;https://github.com/longhorn/longhorn/issues/3907&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-csi-plugin-liveness-probe-should-recover-csi-socket-file&#34;&gt;Test CSI plugin liveness probe should recover CSI socket file&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; healthy Longhorn cluster&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; delete the Longhorn CSI socket file on one of the node(&lt;code&gt;node-1&lt;/code&gt;).&#xA;&lt;code&gt;rm /var/lib/kubelet/plugins/driver.longhorn.io/csi.sock&lt;/code&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; the &lt;code&gt;longhorn-csi-plugin-*&lt;/code&gt; pod on &lt;code&gt;node-1&lt;/code&gt; should be restarted.&lt;/p&gt;&#xA;&lt;p&gt;And the &lt;code&gt;csi-provisioner-*&lt;/code&gt; pod on &lt;code&gt;node-1&lt;/code&gt; should be restarted.&lt;br&gt;&#xA;And the &lt;code&gt;csi-resizer-*&lt;/code&gt;     pod on &lt;code&gt;node-1&lt;/code&gt; should be restarted.&lt;br&gt;&#xA;And the &lt;code&gt;csi-snapshotter-*&lt;/code&gt; pod on &lt;code&gt;node-1&lt;/code&gt; should be restarted.&lt;br&gt;&#xA;And the &lt;code&gt;csi-attacher-*&lt;/code&gt;    pod on &lt;code&gt;node-1&lt;/code&gt; should be restarted.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Disable IPv6</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/disable_ipv6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/disable_ipv6/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2136&#34;&gt;https://github.com/longhorn/longhorn/issues/2136&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2197&#34;&gt;https://github.com/longhorn/longhorn/issues/2197&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Longhorn v1.1.1 should work with IPv6 disabled.&lt;/p&gt;&#xA;&lt;h2 id=&#34;scenario&#34;&gt;Scenario&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Kubernetes&lt;/li&gt;&#xA;&lt;li&gt;Disable IPv6 on all the worker nodes using the following&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Go to the folder /etc/default&#xA;In the grub file, edit the value GRUB_CMDLINE_LINUX_DEFAULT=&amp;#34;ipv6.disable=1&amp;#34;&#xA;Once the file is saved update by the command update-grub&#xA;Reboot the node and once the node becomes active, &#xA;Use the command cat /proc/cmdline to verify &amp;#34;ipv6.disable=1&amp;#34; is reflected in the values &#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn and test basic use cases.&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Accessing the UI for CRUD RWO/RWX volumes&lt;/li&gt;&#xA;&lt;li&gt;Use kubectl to create workloads using RWO/RWX volumes.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Re-enable ipv6 on all the nodes using:&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;By deleting the &amp;#34;ipv6.disable=1&amp;#34; from the GRUB_CMDLINE_LINUX_DEFAULT accessing grub file from /etc/default&#xA;Perform a reboot and once the node comes active&#xA;Use the command cat /proc/cmdline to verify &amp;#34;ipv6.disable=1&amp;#34; is not reflected in the values&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Make sure the basic use cases still works.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test engine binary recovery</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-engine-binary-recovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-engine-binary-recovery/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/4380&#34;&gt;https://github.com/longhorn/longhorn/issues/4380&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;steps&#34;&gt;Steps&lt;/h2&gt;&#xA;&lt;h3 id=&#34;test-remove-engine-binary-on-host-should-recover&#34;&gt;Test remove engine binary on host should recover&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; EngineImage custom resource deployed&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; kubectl -n longhorn-system get engineimage&#xA;NAME          STATE      IMAGE                                    REFCOUNT   BUILDDATE   AGE&#xA;ei-b907910b   deployed   longhornio/longhorn-engine:master-head   0          3d23h       2m25s&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;And engine image pods &lt;code&gt;Ready&lt;/code&gt; are &lt;code&gt;1/1&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt; kubectl -n longhorn-system get pod | grep engine-image&#xA;engine-image-ei-b907910b-g4kpd  1/1   Running   0   2m43s&#xA;engine-image-ei-b907910b-46k6t  1/1   Running   0   2m43s&#xA;engine-image-ei-b907910b-t6wnd  1/1   Running   0   2m43s&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Delete engine binary on host&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Engine Crash During Live Upgrade</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/engine-crash-during-live-upgrade/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/engine-crash-during-live-upgrade/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Create and attach a volume.&lt;/li&gt;&#xA;&lt;li&gt;Deploy an extra engine image.&lt;/li&gt;&#xA;&lt;li&gt;Send live upgrade request then immediately delete the related engine manager pod/engine process (The new replicas are not in active in this case).&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume will detach then reattach automatically.&lt;/li&gt;&#xA;&lt;li&gt;Verify the upgrade is done during the reattachment. (It actually becomes offline upgrade.)&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test File Sync Cancellation</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-file-sync-cancellation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-file-sync-cancellation/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2416&#34;&gt;https://github.com/longhorn/longhorn/issues/2416&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-step&#34;&gt;Test step&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;For test convenience, manually launch the backing image manager pods:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: apps/v1&#xA;kind: DaemonSet&#xA;metadata:&#xA;  labels:&#xA;    app: backing-image-manager&#xA;  name: backing-image-manager&#xA;  namespace: longhorn-system&#xA;spec:&#xA;  selector:&#xA;    matchLabels:&#xA;      app: backing-image-manager&#xA;  template:&#xA;    metadata:&#xA;      labels:&#xA;        app: backing-image-manager&#xA;    spec:&#xA;      containers:&#xA;      - name: backing-image-manager&#xA;        image: longhornio/backing-image-manager:master&#xA;        imagePullPolicy: Always&#xA;        securityContext:&#xA;          privileged: true&#xA;        command:&#xA;        - backing-image-manager&#xA;        - --debug&#xA;        - daemon&#xA;        - --listen&#xA;        - 0.0.0.0:8000&#xA;        readinessProbe:&#xA;          tcpSocket:&#xA;            port: 8000&#xA;        volumeMounts:&#xA;        - name: disk-path&#xA;          mountPath: /data&#xA;      volumes:&#xA;      - name: disk-path&#xA;        hostPath:&#xA;          path: /var/lib/longhorn/&#xA;      serviceAccountName: longhorn-service-account&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;Download a backing image in the first pod:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# alias bm=&amp;#34;backing-image-manager backing-image&amp;#34;&#xA;# bm pull --name bi-test --uuid uuid-bi-test --download-url https://cloud-images.ubuntu.com/minimal/releases/focal/release-20200729/ubuntu-20.04-minimal-cloudimg-amd64.img&#xA;# bm ls&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;In the 2nd pod, limit the bandwidth first so that the downloading won&amp;rsquo;t be done within several seconds&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;tc qdisc del dev eth0 root&#xA;tc qdisc add dev eth0 root tbf rate 500kbit latency 0.1ms burst 1000kbit&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;Then start to get the backing image file from the 1st BIM pod&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# alias bm=&amp;#34;backing-image-manager backing-image&amp;#34;&#xA;# bm sync --name bi-test --uuid uuid-bi-test --download-url https://cloud-images.ubuntu.com/minimal/releases/focal/release-20200729/ubuntu-20.04-minimal-cloudimg-amd64.img --size 208601088 --from-host &amp;lt;the IP of 1st BIM pod&amp;gt; --to-host &amp;lt;the IP of 2nd BIM pod&amp;gt;&#xA;# bm ls&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;5&#34;&gt;&#xA;&lt;li&gt;During the syncing, directly deleting the downloading backing image in the 2nd pod&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# bm delete bi-test&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;6&#34;&gt;&#xA;&lt;li&gt;Wait 1 minute and check the log of the 2nd pod. Make sure there is no download/sync (failure) related logs after the deletion. Then restarting sync without deletion, the backing image can be downloaded successfully.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test filesystem trim</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-filesystem-trim/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-filesystem-trim/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/836&#34;&gt;https://github.com/longhorn/longhorn/issues/836&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;case-1-test-filesystem-trim-during-writing&#34;&gt;Case 1: Test filesystem trim during writing&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; A 10G volume created.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Volume attached to &lt;code&gt;node-1&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Make a filesystem like EXT4 or XFS for the volume.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Mount the filesystem on a mount point.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Run the below shell script with the correct mount point specified:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#!/usr/bin/env bash&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;MOUNT_POINT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;1&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;dd &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/dev/urandom of&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/mnt/data bs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1M count&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8000&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sync&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;CKSUM&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;md5sum /mnt/data | awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{print $1}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; INDEX in &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;1..10..1&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  rm -rf &lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;MOUNT_POINT&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;/data&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  dd &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/mnt/data of&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;MOUNT_POINT&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;/data &amp;amp;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  RAND_SLEEP_INTERVAL&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(($((&lt;/span&gt;$RANDOM&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;))&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;))&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  sleep &lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;RAND_SLEEP_INTERVAL&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  fstrim &lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;MOUNT_POINT&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;ps aux | grep &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;dd if&amp;#34;&lt;/span&gt; | grep -v grep | wc -l&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt; -eq &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    sleep &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  CUR_CKSUM&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;md5sum &lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;MOUNT_POINT&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;/data | awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{print $1}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt; $CUR_CKSUM !&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; $CKSUM &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;then&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;invalid file found, expected checksum &lt;/span&gt;$CKSUM&lt;span style=&#34;color:#e6db74&#34;&gt;, current checksum &lt;/span&gt;$CUR_CKSUM&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    exit &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;fi&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  rm -f &lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;MOUNT_POINT&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;/data&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; The script execution should succeed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Frontend Traffic</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/ws-traffic-flood/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/ws-traffic-flood/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2372&#34;&gt;https://github.com/longhorn/longhorn/issues/2372&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-frontend-traffic&#34;&gt;Test Frontend Traffic&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; 100 pvc created.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; all pvcs deployed and detached.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; monitor traffic in frontend pod with nload.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apk add nload&#xA;nload&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; should not see a continuing large amount of traffic when there is no operation happening. The smaller spikes are mostly coming from event resources which possibly could be enhanced later (&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2433)&#34;&gt;https://github.com/longhorn/longhorn/issues/2433)&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Frontend Web-socket Data Transfer When Resource Not Updated</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.2/full-ws-data-tranfer-when-no-updates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.2/full-ws-data-tranfer-when-no-updates/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn-manager/pull/918&#34;&gt;https://github.com/longhorn/longhorn-manager/pull/918&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2646&#34;&gt;https://github.com/longhorn/longhorn/issues/2646&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2591&#34;&gt;https://github.com/longhorn/longhorn/issues/2591&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-data-send-over-web-socket-when-no-resource-updated&#34;&gt;Test Data Send Over Web-socket When No Resource Updated&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; 1 PVC/Pod created.&#xA;&lt;em&gt;And&lt;/em&gt; the Pod is not writing to the mounted volume.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; monitor network traffic with browser inspect tool.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; wait for 3 mins&#xA;&lt;em&gt;And&lt;/em&gt; should not see data send over web-socket when there are no updates to the resources.&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-data-send-over-web-socket-resource-updated&#34;&gt;Test Data Send Over Web-socket Resource Updated&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; monitor network traffic with browser inspect tool.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test helm on Rancher deployed Windows Cluster</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-helm-install-on-rancher-deployed-windows-cluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-helm-install-on-rancher-deployed-windows-cluster/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/4246&#34;&gt;https://github.com/longhorn/longhorn/issues/4246&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-install&#34;&gt;Test Install&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Rancher cluster.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; 3 new instances for the Windows cluster following &lt;a href=&#34;https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/rke-clusters/windows-clusters/#architecture-requirements&#34;&gt;Architecture Requirements&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; docker installed on the 3 Windows cluster instances.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; &lt;a href=&#34;https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/rke-clusters/windows-clusters/host-gateway-requirements/#disabling-private-ip-address-checks&#34;&gt;Disabled Private IP Address Checks&lt;/a&gt; for the 3 Windows cluster instances.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Created new &lt;code&gt;Custom&lt;/code&gt; Windows cluster with Rancher.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Select &lt;code&gt;Flannel&lt;/code&gt; for &lt;code&gt;Network Provider&lt;/code&gt; &lt;br&gt;&#xA;Enable &lt;code&gt;Windows Support&lt;/code&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Added the 3 nodes to the Rancher Windows cluster.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/rke-clusters/windows-clusters/#add-linux-master-node&#34;&gt;Add Linux Master Node&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/rke-clusters/windows-clusters/#add-linux-master-node&#34;&gt;Add Linux Worker Node&lt;/a&gt;&lt;br&gt;&#xA;&lt;a href=&#34;https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/rke-clusters/windows-clusters/#add-a-windows-worker-node&#34;&gt;Add Windows Worker Node&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Helm uninstall Longhorn in different namespace</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-helm-uninstall-different-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-helm-uninstall-different-namespace/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2034&#34;&gt;https://github.com/longhorn/longhorn/issues/2034&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test&#34;&gt;Test&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; helm install Longhorn in different namespace&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; helm uninstall Longhorn&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Longhorn should complete uninstalling.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test IM Proxy connection metrics</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-grpc-proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-grpc-proxy/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2821&#34;&gt;https://github.com/longhorn/longhorn/issues/2821&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/4038&#34;&gt;https://github.com/longhorn/longhorn/issues/4038&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-grpc-proxy&#34;&gt;Test gRPC proxy&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Longhorn exist in the cluster.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Monitoring stack exist in the cluster.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Execute &lt;code&gt;longhorn_instance_manager_proxy_grpc_connection&lt;/code&gt; in Prometheus UI.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Metric data shows in Prometheus UI.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Monitor &lt;code&gt;longhorn_instance_manager_proxy_grpc_connection&lt;/code&gt; in Grafana UI Panel.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Run automation regression.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Connections should return to 0 when tests complete.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test instance manager cleanup during uninstall</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.0/test-instance-manager-cleanup-during-uninstall/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.0/test-instance-manager-cleanup-during-uninstall/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Deploy Longhorn v1.1.2&lt;/li&gt;&#xA;&lt;li&gt;Launch some running volumes.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade to v1.2.0. ==&amp;gt; All old unused engine managers should be cleaned up automatically.&lt;/li&gt;&#xA;&lt;li&gt;Make sure all running volumes keep state &lt;code&gt;running&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade all volumes. ==&amp;gt; All old replica managers should be cleaned up automatically.&lt;/li&gt;&#xA;&lt;li&gt;Detach all running volumes. ==&amp;gt; All old engine managers should be cleaned up automatically.&lt;/li&gt;&#xA;&lt;li&gt;do offline upgrade then reattach these volumes.&lt;/li&gt;&#xA;&lt;li&gt;Directly uninstall the Longhorn system.&#xA;And use &lt;code&gt;kubectl -n longhorn-system get lhim -w&lt;/code&gt; to verify that the system doesn&amp;rsquo;t loop in instance manager cleanup-recreation.&#xA;==&amp;gt; The uninstaller should work fine.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test Instance Manager IP Sync</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/instance-manager-ip-sync/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/instance-manager-ip-sync/</guid>
      <description>&lt;h2 id=&#34;test-step&#34;&gt;Test step:&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Launch longhorn system&lt;/li&gt;&#xA;&lt;li&gt;Create and attach a volume&lt;/li&gt;&#xA;&lt;li&gt;Follow this &lt;a href=&#34;https://github.com/longhorn/longhorn/wiki/dev:-How-to-modify-the-status-subresource-with-%60kubectl-edit%60-(CRD)&#34;&gt;doc&lt;/a&gt; to manually modify the IP of one instance-manager-r. e.g.,&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -k -XPATCH -H &amp;#34;Accept: application/json&amp;#34; -H &amp;#34;Content-Type: application/merge-patch+json&amp;#34; -H &amp;#34;Authorization: Bearer kubeconfig-xxxxxx&amp;#34; --data &amp;#39;{&amp;#34;status&amp;#34;:{&amp;#34;ip&amp;#34;:&amp;#34;1.1.1.1&amp;#34;}}&amp;#39; https://172.104.72.64/k8s/clusters/c-znrxc/apis/longhorn.io/v1beta1/namespaces/longhorn-system/instancemanagers/instance-manager-r-63ece607/status&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol&gt;&#xA;&lt;li&gt;Notice that the bearer token &lt;code&gt;kubeconfig-xxx&lt;/code&gt; can be found in your kube config file&lt;/li&gt;&#xA;&lt;li&gt;Remember to add &lt;code&gt;/status&lt;/code&gt; at the end of the URL&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Verify the IP of the instance manager still matches the pod IP&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume can be detached.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test instance manager NPE</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-instance-manager-npe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-instance-manager-npe/</guid>
      <description>&lt;h3 id=&#34;test-step&#34;&gt;Test step&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create and attach a 1-replica volume.&lt;/li&gt;&#xA;&lt;li&gt;Create 2 snapshots with large amount of data so that rebuilding each snapshot would take some time.&lt;/li&gt;&#xA;&lt;li&gt;Disable the scheduling for the nodes so that there is one node could accept new replicas of the volume.&lt;/li&gt;&#xA;&lt;li&gt;Update the replica count to 2 for the volume and wait for the rebuilding start.&lt;/li&gt;&#xA;&lt;li&gt;While syncing the 1st snapshot file, create a directory with the name of &lt;strong&gt;another snapshot meta file&lt;/strong&gt;.&#xA;Later the rebuilding replica will fail to create this meta file then error out.&lt;/li&gt;&#xA;&lt;li&gt;Verify there is no NPE issue (no following log) in the instance manager pod when the failure mentioned above is triggered.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-log&#34; data-lang=&#34;log&#34;&gt;2021/12/24 16:29:02 http: panic serving 10.42.2.251:42464: runtime error: invalid memory address or nil pointer dereference&#xA;goroutine 88514 [running]:&#xA;net/http.(*conn).serve.func1(0xc00032e000)&#xA;    /usr/local/go/src/net/http/server.go:1772 +0x139&#xA;panic(0xd73a40, 0x168d100)&#xA;    /usr/local/go/src/runtime/panic.go:975 +0x3e3&#xA;github.com/longhorn/sparse-tools/sparse/rest.(*SyncServer).close(0xc00041c050, 0x108cf60, 0xc00015c000, 0xc000364200)&#xA;    /go/src/github.com/longhorn/longhorn-engine/vendor/github.com/longhorn/sparse-tools/sparse/rest/handlers.go:119 +0x63&#xA;net/http.HandlerFunc.ServeHTTP(0xc0001f6830, 0x108cf60, 0xc00015c000, 0xc000364200)&#xA;    /usr/local/go/src/net/http/server.go:2012 +0x44&#xA;github.com/gorilla/mux.(*Router).ServeHTTP(0xc0004f6000, 0x108cf60, 0xc00015c000, 0xc00020a300)&#xA;    /go/src/github.com/longhorn/longhorn-engine/vendor/github.com/gorilla/mux/mux.go:212 +0xe2&#xA;net/http.serverHandler.ServeHTTP(0xc000428000, 0x108cf60, 0xc00015c000, 0xc00020a300)&#xA;    /usr/local/go/src/net/http/server.go:2807 +0xa3&#xA;net/http.(*conn).serve(0xc00032e000, 0x10915a0, 0xc0003ae240)&#xA;    /usr/local/go/src/net/http/server.go:1895 +0x86c&#xA;created by net/http.(*Server).Serve&#xA;    /usr/local/go/src/net/http/server.go:2933 +0x35c&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify the rebuilding will be restarted and succeed.&lt;/li&gt;&#xA;&lt;li&gt;Verify the data of the volume.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;GitHub Issue: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2820&#34;&gt;https://github.com/longhorn/longhorn/issues/2820&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Instance Manager Streaming Connection Recovery</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.2/instance-manager-streaming-connection-recovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.2/instance-manager-streaming-connection-recovery/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2561&#34;&gt;https://github.com/longhorn/longhorn/issues/2561&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-step&#34;&gt;Test Step&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; A cluster with Longhorn&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; create a volume and attach it to a pod.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; &lt;code&gt;exec&lt;/code&gt; into a longhorn manager pod and kill the connection with an engine or replica instance manager pod. The connections are instance manager pods&amp;rsquo; IP with port &lt;code&gt;8500&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kl exec -it longhorn-manager-5z8zn -- bash&#xA;&#xA;root@longhorn-manager-5z8zn:/# ss&#xA;Netid                    State                     Recv-Q                     Send-Q                                           Local Address:Port                                            Peer Address:Port&#xA;tcp                      ESTAB                     0                          0                                                  10.42.1.124:59414                                            10.42.1.123:8500&#xA;tcp                      ESTAB                     0                          0                                                  10.42.1.124:39096                                           52.36.54.134:https&#xA;tcp                      ESTAB                     0                          0                                                  10.42.1.124:45302                                              10.43.0.1:https&#xA;tcp                      ESTAB                     0                          0                                                  10.42.1.124:34124                                            10.42.1.122:8500&#xA;root@longhorn-manager-5z8zn:/# ss -K dst 10.42.1.122:8500&#xA;Netid                    State                     Recv-Q                     Send-Q                                            Local Address:Port                                            Peer Address:Port&#xA;tcp                      ESTAB                     0                          0                                                   10.42.1.124:34124                                            10.42.1.122:8500&#xA;root@longhorn-manager-5z8zn:/# exit&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Check the longhorn manager pod log. There must be following logs:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test ISCSI Installation on EKS</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/iscsi_installation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/iscsi_installation/</guid>
      <description>&lt;p&gt;This is for EKS or similar users who doesn&amp;rsquo;t need to log into each host to install &amp;lsquo;ISCSI&amp;rsquo; individually.&lt;/p&gt;&#xA;&lt;p&gt;Test steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create an EKS cluster with 3 nodes.&lt;/li&gt;&#xA;&lt;li&gt;Run the following command to install iscsi on every nodes.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/iscsi/longhorn-iscsi-installation.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;In Longhorn Manager Repo Directory run:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl apply -Rf ./deploy/install/&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;Longhorn should be able installed successfully.&lt;/li&gt;&#xA;&lt;li&gt;Try to create a pod with a pvc:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/examples/simple_pvc.yaml&#xA;kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/examples/simple_pod.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;6&#34;&gt;&#xA;&lt;li&gt;Check the pod is created successfully with a valid Longhorn volume mounted.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test kubelet restart on a node of the cluster</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/kubelet-restart/kubelet-restart-on-a-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/kubelet-restart/kubelet-restart-on-a-node/</guid>
      <description>&lt;h3 id=&#34;related-issues&#34;&gt;Related issues:&lt;/h3&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2629&#34;&gt;https://github.com/longhorn/longhorn/issues/2629&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;case-1-kubelet-restart-on-rke1-multi-node-cluster&#34;&gt;Case 1: Kubelet restart on RKE1 multi node cluster:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a RKE1 cluster with config of 1 etcd/control plane and 3 worker nodes.&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn on the cluster.&lt;/li&gt;&#xA;&lt;li&gt;Deploy prometheus monitoring app on the cluster which is using Longhorn storage class or deploy a statefulSet with Longhorn volume.&lt;/li&gt;&#xA;&lt;li&gt;Write some data into the mount point and compute the md5sum.&lt;/li&gt;&#xA;&lt;li&gt;Restart the kubelet on the node where the statefulSet or Prometheus pod is running using the command &lt;code&gt;sudo docker restart kubelet&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Observe the volume. It becomes degraded but is still running.&lt;/li&gt;&#xA;&lt;li&gt;Once the node is back, the volume of the workload should work fine and the data is intact.&lt;/li&gt;&#xA;&lt;li&gt;Scale down then re-scale up the workload. Verify the existing data is correct.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-2-kubelet-restart-on-one-node-rke1-cluster&#34;&gt;Case 2: Kubelet restart on one node RKE1 cluster:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a RKE1 cluster with config of 1 node all role.&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn on the cluster.&lt;/li&gt;&#xA;&lt;li&gt;Deploy prometheus monitoring app on the cluster which is using Longhorn storage class or deploy a statefulSet with Longhorn volume.&lt;/li&gt;&#xA;&lt;li&gt;Write some data into the mount point and compute the md5sum.&lt;/li&gt;&#xA;&lt;li&gt;Restart the kubelet on the node using the command &lt;code&gt;sudo docker restart kubelet&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Check the instance manager pods on the node are still running.&lt;/li&gt;&#xA;&lt;li&gt;Observe the volume. It gets detached and the pod gets terminated (since the only replica of the volume becomes failed).&lt;/li&gt;&#xA;&lt;li&gt;Once the pod is terminated, a new pod should be created and get attached to the volume successfully.&lt;/li&gt;&#xA;&lt;li&gt;Verify that the mount of the volume is successful and data is safe.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-3-rke2-serverrke2-agent-restart-on-rke2-multi-node-cluster&#34;&gt;Case 3: rke2-server/rke2-agent restart on RKE2 multi node cluster:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a RKE2 cluster with config of 1 control plane and 3 worker nodes.&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn on the cluster.&lt;/li&gt;&#xA;&lt;li&gt;Deploy prometheus monitoring app on the cluster which is using Longhorn storage class or deploy a statefulSet with Longhorn volume.&lt;/li&gt;&#xA;&lt;li&gt;Write some data into the mount point and compute the md5sum.&lt;/li&gt;&#xA;&lt;li&gt;Restart the rke-agent service on the node where the statefulSet or Prometheus pod is running using the command &lt;code&gt;systemctl restart rke2-agent.service&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Observe the volume. It becomes degraded but is still running.&lt;/li&gt;&#xA;&lt;li&gt;Once the node is back, the volume of the workload should work fine and the data is intact.&lt;/li&gt;&#xA;&lt;li&gt;Scale down then re-scale up the workload. Verify the existing data is correct.&lt;/li&gt;&#xA;&lt;li&gt;Create a StatefulSet with Longhorn volume on the control plane node.&lt;/li&gt;&#xA;&lt;li&gt;Once the StatefulSet is up and running, Write some data into the mount point and compute the md5sum.&lt;/li&gt;&#xA;&lt;li&gt;Restart the rke2-service on the control plane node using the command &lt;code&gt;systemctl restart rke2-server.service&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Observe the volume. It becomes degraded but is still running.&lt;/li&gt;&#xA;&lt;li&gt;Once the node is back, the volume of the workload should work fine and the data is intact.&lt;/li&gt;&#xA;&lt;li&gt;Scale down then re-scale up the workload. Verify the existing data is correct.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-4-kubelet-restart-on-a-node-with-rwx-volume-on-a-rke1-cluster&#34;&gt;Case 4: Kubelet restart on a node with RWX volume on a RKE1 Cluster:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a RKE1 cluster with config of 1 etcd/control plane and 3 worker nodes.&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn on the cluster.&lt;/li&gt;&#xA;&lt;li&gt;Deploy a statefulSet attached with an RWX volume.&lt;/li&gt;&#xA;&lt;li&gt;Write some data into the mount point and compute the md5sum.&lt;/li&gt;&#xA;&lt;li&gt;Restart the kubelet on the node where the share-manager pod is running using the command &lt;code&gt;sudo docker restart kubelet&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Observe the volume. It gets detached and the share-manager pod gets terminated.&lt;/li&gt;&#xA;&lt;li&gt;Watch the pod of the StatefulSet using the command &lt;code&gt;kubectl get pods -n &amp;lt;namespace&amp;gt; -w&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;The pods (share manager and StatefulSet, instance manager pods are not included) should be terminated and restarted.&lt;/li&gt;&#xA;&lt;li&gt;Verify that the mount of the volume is successful and data is safe.&lt;/li&gt;&#xA;&lt;li&gt;Repeat the above steps where the StatefulSet pod and share-manager pod are attached to different nodes and restart the node where the statefulSet pod is running.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-5-kubelet-restart-on-a-node-with-rwx-volume-on-a-rke2-cluster&#34;&gt;Case 5: Kubelet restart on a node with RWX volume on a RKE2 Cluster:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Repeat the steps from the case 4 on an RKE2 cluster.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test Label-driven Recurring Job</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.0/label-driven-recurring-job/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.0/label-driven-recurring-job/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/467&#34;&gt;https://github.com/longhorn/longhorn/issues/467&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-recurring-job-concurrency&#34;&gt;Test Recurring Job Concurrency&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; create &lt;code&gt;snapshot&lt;/code&gt; recurring job with &lt;code&gt;concurrency&lt;/code&gt; set to &lt;code&gt;2&lt;/code&gt; and include &lt;code&gt;snapshot&lt;/code&gt; recurring job &lt;code&gt;default&lt;/code&gt; in groups.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; create volume &lt;code&gt;test-job-1&lt;/code&gt;.&lt;br&gt;&#xA;&lt;em&gt;And&lt;/em&gt; create volume &lt;code&gt;test-job-2&lt;/code&gt;.&lt;br&gt;&#xA;&lt;em&gt;And&lt;/em&gt; create volume &lt;code&gt;test-job-3&lt;/code&gt;.&lt;br&gt;&#xA;&lt;em&gt;And&lt;/em&gt; create volume &lt;code&gt;test-job-4&lt;/code&gt;.&lt;br&gt;&#xA;&lt;em&gt;And&lt;/em&gt; create volume &lt;code&gt;test-job-5&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; moniter the cron job pod log.&lt;br&gt;&#xA;&lt;em&gt;And&lt;/em&gt; should see 2 jobs created concurrently.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; update &lt;code&gt;snapshot1&lt;/code&gt; recurring job with &lt;code&gt;concurrency&lt;/code&gt; set to &lt;code&gt;3&lt;/code&gt;.&lt;br&gt;&#xA;&lt;strong&gt;Then&lt;/strong&gt; moniter the cron job pod log.&#xA;&lt;em&gt;And&lt;/em&gt; should see 3 jobs created concurrently.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Longhorn components recovery</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/resiliency/test-longhorn-component-recovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/resiliency/test-longhorn-component-recovery/</guid>
      <description>&lt;p&gt;This is a simple test is check if all the components are recoverable.&lt;/p&gt;&#xA;&lt;h4 id=&#34;test-data-setup&#34;&gt;Test data setup:&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy Longhorn on a 3 nodes cluster.&lt;/li&gt;&#xA;&lt;li&gt;Create a volume &lt;code&gt;vol-1&lt;/code&gt; using Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Create a volume &lt;code&gt;vol-2&lt;/code&gt; using the Longhorn storage class.&lt;/li&gt;&#xA;&lt;li&gt;Create a volume &lt;code&gt;vol-3&lt;/code&gt; with backing image.&lt;/li&gt;&#xA;&lt;li&gt;Create an RWX volume &lt;code&gt;vol-4&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in all the volumes created and compute the md5sum.&lt;/li&gt;&#xA;&lt;li&gt;Have all the volumes in attached state.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h4 id=&#34;test-steps&#34;&gt;Test steps:&lt;/h4&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Delete the IM-e from every volume and make sure every volume recovers. Check the data as well.&lt;/li&gt;&#xA;&lt;li&gt;Start replica rebuilding for the aforementioned volumes, and delete the IM-e while it is rebuilding. Verify the recovered volumes.&lt;/li&gt;&#xA;&lt;li&gt;Delete the Share-manager pod and verify the RWX volume &lt;code&gt;vol-4&lt;/code&gt; is able recover. Verify the data too.&lt;/li&gt;&#xA;&lt;li&gt;Delete the backing image manager pod and verify the pod gets recreated.&lt;/li&gt;&#xA;&lt;li&gt;Delete one pod of all the Longhorn components like longhorn-manager, ui, csi components etc and verify they are able to recover.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test Longhorn deployment on RKE2 v1.24- with CIS-1.6 profile</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/environment/rke2-cis-1.6-profile/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/environment/rke2-cis-1.6-profile/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;This test was created in response to &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2292&#34;&gt;2292&lt;/a&gt;, which used CSI-1.5.&#xA;However, RKE2 generally does not recommend or encourage using CIS-1.5 in favor of CIS1.6.&lt;/p&gt;&#xA;&lt;h2 id=&#34;scenario&#34;&gt;Scenario&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare 1 control plane node and 3 worker nodes.&lt;/li&gt;&#xA;&lt;li&gt;Install RKE2 v1.24 with CIS-1.6 profile on 1 control plane node.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo su -&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl disable firewalld &lt;span style=&#34;color:#75715e&#34;&gt;# On a supporting OS.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl stop firewalld &lt;span style=&#34;color:#75715e&#34;&gt;# On a supporting OS.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;yum install iscsi-initiator-utils &lt;span style=&#34;color:#75715e&#34;&gt;# Or the OS equivalent.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl enable --now iscsid&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -sfL https://get.rke2.io | INSTALL_RKE2_CHANNEL&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;v1.24 sh -&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl enable rke2-server.service&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; /etc/sysctl.d/60-rke2-cis.conf&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;vm.panic_on_oom=0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;vm.overcommit_memory=1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kernel.panic=10&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kernel.panic_on_oops=1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo systemctl restart systemd-sysctl&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;useradd -r -c &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;etcd user&amp;#34;&lt;/span&gt; -s /sbin/nologin -M etcd&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir -p /etc/rancher/rke2/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; /etc/rancher/rke2/config.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;profile: &amp;#34;cis-1.6&amp;#34;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl start rke2-server.service&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;SERVER_NODE_TOKEN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;cat /var/lib/rancher/rke2/server/node-token&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;Install RKE2 v1.24 with CIS-1.6 profile on 3 worker nodes.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo su -&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl disable firewalld &lt;span style=&#34;color:#75715e&#34;&gt;# On a supporting OS.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl stop firewalld &lt;span style=&#34;color:#75715e&#34;&gt;# On a supporting OS.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;yum install iscsi-initiator-utils &lt;span style=&#34;color:#75715e&#34;&gt;# Or the OS equivalent.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl enable --now iscsid&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -sfL https://get.rke2.io | INSTALL_RKE2_CHANNEL&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;v1.24 INSTALL_RKE2_TYPE&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;agent&amp;#34;&lt;/span&gt; sh -&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl enable rke2-agent.service&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; /etc/sysctl.d/60-rke2-cis.conf&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;vm.panic_on_oom=0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;vm.overcommit_memory=1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kernel.panic=10&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kernel.panic_on_oops=1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo systemctl restart systemd-sysctl&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir -p /etc/rancher/rke2/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; /etc/rancher/rke2/config.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;server: https://$CONTROL_PLANE_NODE_IP_ADDRESS:9345&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;token: $SERVER_NODE_TOKEN&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;profile: &amp;#34;cis-1.6&amp;#34;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl start rke2-agent.service&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;Prepare the &lt;code&gt;longhorn-values.yaml&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;enablePSP&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;5&#34;&gt;&#xA;&lt;li&gt;Test install Longhorn by helm without problem. (These commands assume a pre-release chart. When testing a released&#xA;chart, use &lt;code&gt;helm repo add&lt;/code&gt; etc.)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git clone https://github.com/longhorn/longhorn.git&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd longhorn&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git checkout $VERSION_TO_TEST&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;helm install --namespace longhorn-system longhorn ./chart&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;6&#34;&gt;&#xA;&lt;li&gt;Prepare the &lt;code&gt;longhorn-values.yaml&lt;/code&gt; again.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;enablePSP&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;enabled&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;7&#34;&gt;&#xA;&lt;li&gt;Test upgrade Longhorn by helm without problem.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;helm upgrade --namespace longhorn-system --values longhorn-values.yaml longhorn ./chart&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;8&#34;&gt;&#xA;&lt;li&gt;Verify the upgrade &amp;ldquo;worked&amp;rdquo; by checking for the deployed ingress.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get ingress -n longhorn-system&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;9&#34;&gt;&#xA;&lt;li&gt;Test uninstall Longhorn by helm without problem.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;helm uninstall longhorn --namespace longhorn-system&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;10&#34;&gt;&#xA;&lt;li&gt;Test install Longhorn by kubectl without problem.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl apply -f deploy/podsecuritypolicy.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl apply -f deploy/longhorn.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;11&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Uncomment all Pod Security Policy related lines from deploy/uninstall.yaml.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Longhorn deployment on RKE2 v1.25&#43; with CIS-1.23 profile</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/environment/rke2-cis-1.23-profile/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/environment/rke2-cis-1.23-profile/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;This is an expansion of &lt;a href=&#34;./rke2-cis-1.6-profile.md&#34;&gt;Test Longhorn deployment on RKE2 v1.24- with CIS-1.6 profile&lt;/a&gt;,&#xA;which was created in response to &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2292&#34;&gt;2292&lt;/a&gt;. However, later versions of&#xA;RKE2 only support CIS-1.23.&lt;/p&gt;&#xA;&lt;h2 id=&#34;scenario&#34;&gt;Scenario&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Prepare 1 control plane node and 3 worker nodes.&lt;/li&gt;&#xA;&lt;li&gt;Install the latest RKE2 with CIS-1.23 profile on 1 control plane node.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo su -&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl disable firewalld &lt;span style=&#34;color:#75715e&#34;&gt;# On a supporting OS.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl stop firewalld &lt;span style=&#34;color:#75715e&#34;&gt;# On a supporting OS.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;yum install iscsi-initiator-utils &lt;span style=&#34;color:#75715e&#34;&gt;# Or the OS equivalent.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl enable --now iscsid&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -sfL https://get.rke2.io | sh -&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl enable rke2-server.service&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; /etc/sysctl.d/60-rke2-cis.conf&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;vm.panic_on_oom=0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;vm.overcommit_memory=1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kernel.panic=10&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kernel.panic_on_oops=1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo systemctl restart systemd-sysctl&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;useradd -r -c &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;etcd user&amp;#34;&lt;/span&gt; -s /sbin/nologin -M etcd&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir -p /etc/rancher/rke2/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; /etc/rancher/rke2/config.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;profile: &amp;#34;cis-1.23&amp;#34;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl start rke2-server.service&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;SERVER_NODE_TOKEN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;cat /var/lib/rancher/rke2/server/node-token&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;Install RKE2 with CIS-1.23 profile on 3 worker nodes.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo su -&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl disable firewalld &lt;span style=&#34;color:#75715e&#34;&gt;# On a supporting OS.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl stop firewalld &lt;span style=&#34;color:#75715e&#34;&gt;# On a supporting OS.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;yum install iscsi-initiator-utils &lt;span style=&#34;color:#75715e&#34;&gt;# Or the OS equivalent.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl enable --now iscsid&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;agent&amp;#34;&lt;/span&gt; sh -&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl enable rke2-agent.service&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; /etc/sysctl.d/60-rke2-cis.conf&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;vm.panic_on_oom=0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;vm.overcommit_memory=1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kernel.panic=10&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kernel.panic_on_oops=1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo systemctl restart systemd-sysctl&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir -p /etc/rancher/rke2/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; EOF &amp;gt; /etc/rancher/rke2/config.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;server: https://$CONTROL_PLANE_NODE_IP_ADDRESS:9345&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;token: $SERVER_NODE_TOKEN&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;profile: &amp;#34;cis-1.23&amp;#34;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl start rke2-agent.service&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;Test install Longhorn by helm without problem. (These commands assume a pre-release chart. When testing a released&#xA;chart, use &lt;code&gt;helm repo add&lt;/code&gt; etc.)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git clone https://github.com/longhorn/longhorn.git&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd longhorn&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git checkout $VERSION_TO_TEST&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Pod security admission labels are required for CIS-1.23.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl label ns longhorn-system pod-security.kubernetes.io/enforce&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;privileged&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl label ns longhorn-system pod-security.kubernetes.io/enforce-version&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;latest&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl label ns longhorn-system pod-security.kubernetes.io/audit&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;privileged&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl label ns longhorn-system pod-security.kubernetes.io/audit-version&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;latest&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl label ns longhorn-system pod-security.kubernetes.io/warn&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;privileged&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl label ns longhorn-system pod-security.kubernetes.io/warn-version&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;latest&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;helm install --namespace longhorn-system longhorn ./chart&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;5&#34;&gt;&#xA;&lt;li&gt;Prepare the &lt;code&gt;longhorn-values.yaml&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;enabled&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;6&#34;&gt;&#xA;&lt;li&gt;Test upgrade Longhorn by helm without problem.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;helm upgrade --namespace longhorn-system --values longhorn-values.yaml longhorn ./chart&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;7&#34;&gt;&#xA;&lt;li&gt;Verify the upgrade &amp;ldquo;worked&amp;rdquo; by checking for the deployed ingress.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get ingress -n longhorn-system&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;8&#34;&gt;&#xA;&lt;li&gt;Test uninstall Longhorn by helm without problem.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;helm uninstall longhorn --namespace longhorn-system&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;9&#34;&gt;&#xA;&lt;li&gt;Test install Longhorn by kubectl without problem.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl apply -f deploy/longhorn.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;10&#34;&gt;&#xA;&lt;li&gt;Test uninstall Longhorn by kubectl without problem.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl apply -f uninstall/uninstall.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl delete -f deploy/longhorn.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl delete -f uninstall/uninstall.yaml&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Test longhorn manager NPE caused by backup creation</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-longhorn-manager-npe/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-longhorn-manager-npe/</guid>
      <description>&lt;h3 id=&#34;test-step&#34;&gt;Test step&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Add the following rule to the ClusterRole &lt;code&gt;longhorn-test-role&lt;/code&gt;:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn.io&amp;#34;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*&amp;#34;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*&amp;#34;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;Put the below test case into the integration test work directory then run it.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; random&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; string&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; time&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; common&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; common &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; client, volume_name  &lt;span style=&#34;color:#75715e&#34;&gt;# NOQA&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; backupstore &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; set_random_backupstore &lt;span style=&#34;color:#75715e&#34;&gt;# NOQA&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Mi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1024&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1024&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Gi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1024&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; Mi)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LH_API_GROUP &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn.io&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LH_API_VERSION &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;v1beta1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LH_NAMESPACE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn-system&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LHE_PLURAL &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;engines&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;LHB_PLURAL &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;backups&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;test_backup_npe&lt;/span&gt;(client, volume_name, set_random_backupstore):  &lt;span style=&#34;color:#75715e&#34;&gt;# NOQA&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    host_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; common&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_self_host_id()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    client&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create_volume(name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;volume_name, size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;str(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;Gi), numberOfReplicas&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    volume &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; common&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;wait_for_volume_detached(client, volume_name)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    volume&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;attach(hostId&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;host_id)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    volume &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; common&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;wait_for_volume_healthy(client, volume_name)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    common&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create_backup(client, volume_name)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    volume&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;detach()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    common&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;wait_for_volume_detached(client, volume_name)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    snap_name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;non-existing-snapshot&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cr_api &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; common&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_custom_object_api_client()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    lhb_name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;backup-test-&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(random&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;choice(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ascii_lowercase &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;digits) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;))&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    lhb_manifest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;kind&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Backup&amp;#39;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;apiVersion&amp;#39;&lt;/span&gt;: LH_API_GROUP&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;LH_API_VERSION,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;metadata&amp;#39;&lt;/span&gt;: {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;: lhb_name,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;labels&amp;#39;&lt;/span&gt;: {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;backup-volume&amp;#39;&lt;/span&gt;: volume_name,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            },&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;namespace&amp;#39;&lt;/span&gt;: LH_NAMESPACE,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        },&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;spec&amp;#39;&lt;/span&gt;: {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;snapshotName&amp;#39;&lt;/span&gt;: snap_name,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        },&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        volume &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; client&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;by_id_volume(volume_name)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        engine_name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; common&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_volume_engine(volume)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            cr_api&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create_namespaced_custom_object(group&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LH_API_GROUP,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                   version&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LH_API_VERSION,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                   namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LH_NAMESPACE,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                   plural&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LHB_PLURAL,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                   body&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;lhb_manifest)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            cr_api&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;delete_namespaced_custom_object(group&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LH_API_GROUP,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                   version&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LH_API_VERSION,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                   namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LH_NAMESPACE,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                   plural&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LHE_PLURAL,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                   name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;engine_name)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Exception&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; e:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;exception creating lhb or deleting lhe &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; e)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        time&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sleep(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            cr_api&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;delete_namespaced_custom_object(group&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LH_API_GROUP,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                   version&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LH_API_VERSION,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                   namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LH_NAMESPACE,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                   plural&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LHB_PLURAL,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                   name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;lhb_name)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Exception&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; e:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;exception deleting lhb &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%s&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; e)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        time&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sleep(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    common&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;delete_backup_volume(client, volume_name)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    client&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;delete(volume)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Notice:&#xA;If you want to run this test in v1.2.x, please modify &lt;code&gt;LH_API_VERSION&lt;/code&gt; from &lt;code&gt;v1beta2&lt;/code&gt; to &lt;code&gt;v1beta1&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test longhorn manager pod starting scalability</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-longhorn-manager-starting-scalability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-longhorn-manager-starting-scalability/</guid>
      <description>&lt;h3 id=&#34;test-step&#34;&gt;Test step&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy a cluster with multiple nodes. e.g., 20 worker nodes.&lt;/li&gt;&#xA;&lt;li&gt;Launch an old Longhorn version without the fix PR. e.g., Longhorn version v1.2.3.&lt;/li&gt;&#xA;&lt;li&gt;Create and attach multiple volumes on different nodes. e.g.,:&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn.io/v1beta2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;BackingImage&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bi-test1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-system&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;download&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceParameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;url&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-test1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;driver.longhorn.io&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;allowVolumeExpansion&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;reclaimPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;volumeBindingMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Immediate&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;numberOfReplicas&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;staleReplicaTimeout&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2880&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;fromBackup&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ext4&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;backingImage&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bi-test1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;web&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NodePort&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StatefulSet&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bi-scalability-test&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;serviceName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nginx&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;podManagementPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Parallel&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Always&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;terminationGracePeriodSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.k8s.io/nginx-slim:0.8&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;web&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/usr/share/nginx/html&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/usr/share/nginx/html2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www3&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/usr/share/nginx/html3&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www4&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/usr/share/nginx/html4&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www5&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/usr/share/nginx/html5&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www6&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/usr/share/nginx/html6&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www7&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/usr/share/nginx/html7&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www8&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/usr/share/nginx/html8&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www9&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/usr/share/nginx/html9&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www10&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;              &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/usr/share/nginx/html10&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;volumeClaimTemplates&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn-test1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32Mi&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn-test1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32Mi&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www3&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn-test1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32Mi&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www4&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn-test1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32Mi&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www5&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn-test1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32Mi&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www6&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn-test1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32Mi&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www7&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn-test1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32Mi&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www8&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn-test1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32Mi&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www9&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn-test1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32Mi&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www10&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;longhorn-test1&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;32Mi&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;Delete DaemonSet &lt;code&gt;longhorn-manager&lt;/code&gt; and wait for all pods cleanup.&lt;/li&gt;&#xA;&lt;li&gt;Recreate DaemonSet &lt;code&gt;longhorn-manager&lt;/code&gt; and record the time elapsed when all pods become running and ready. This should take a long time.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to another version that does not contain the fix (the old upgrade path). Then record the time elapsed when all pods become running and ready. This should take a long time as well.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to the version that contains the fix (the new upgrade path) and record the time. This should take a relatively short time.&lt;/li&gt;&#xA;&lt;li&gt;Delete DaemonSet &lt;code&gt;longhorn-manager&lt;/code&gt; and wait for all pods cleanup.&lt;/li&gt;&#xA;&lt;li&gt;Recreate DaemonSet &lt;code&gt;longhorn-manager&lt;/code&gt; and record the time. This should take a relatively short time, too.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;GitHub Issue: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/3087&#34;&gt;https://github.com/longhorn/longhorn/issues/3087&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Longhorn system backup should sync from the remote backup target</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-system-backup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-system-backup/</guid>
      <description>&lt;h2 id=&#34;steps&#34;&gt;Steps&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Custom resource SystemBackup (foo) exist in AWS S3,&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; System backup (foo) downloaded from AWS S3.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Custom resource SystemBackup (foo) deleted.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Upload the system backup (foo) to AWS S3.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Create a new custom resource SystemBackup(foo).&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;This needs to be done before the system backup gets synced to the cluster.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Should see the synced messages in the custom resource SystemBackup(foo).&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Events:&#xA;  Type    Reason   Age    From                               Message&#xA;  ----    ------   ----   ----                               -------&#xA;  Normal  Syncing  9m29s  longhorn-system-backup-controller  Syncing system backup from backup target&#xA;  Normal  Synced   9m28s  longhorn-system-backup-controller  Synced system backup from backup target&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Test Node Delete</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/delete-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/delete-node/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2186&#34;&gt;https://github.com/longhorn/longhorn/issues/2186&lt;/a&gt;&#xA;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2462&#34;&gt;https://github.com/longhorn/longhorn/issues/2462&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;delete-method&#34;&gt;Delete Method&lt;/h2&gt;&#xA;&lt;p&gt;Should verify with both of the delete methods.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Bulk Delete - This is the &lt;code&gt;Delete&lt;/code&gt; on the Node page.&lt;/li&gt;&#xA;&lt;li&gt;Node Delete - This is the &lt;code&gt;Remove Node&lt;/code&gt; for each node &lt;code&gt;Operation&lt;/code&gt; drop-down list.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;test-node-delete---should-grey-out-when-node-not-down&#34;&gt;Test Node Delete - should grey out when node not down&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; node not &lt;code&gt;Down&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Try to delete any node.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Should see button greyed out.&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-node-delete&#34;&gt;Test Node Delete&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; pod with pvc created.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test node deletion</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/node-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/node-not-ready/node-down/node-deletion/</guid>
      <description>&lt;h3 id=&#34;case-1-delete-multiple-kinds-of-nodes&#34;&gt;Case 1: Delete multiple kinds of nodes:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Shut down the VM for one node and wait for the node &lt;code&gt;Down&lt;/code&gt;. Disable another node.&lt;/li&gt;&#xA;&lt;li&gt;Delete the above 2 nodes. Make sure the corresponding Kubernetes node object is deleted. &amp;ndash;&amp;gt; The related Longhorn node objects will be cleaned up immediately, too.&lt;/li&gt;&#xA;&lt;li&gt;Add new nodes with the same names for the cluster. &amp;ndash;&amp;gt; The new nodes are available.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;case-2-delete-nodes-when-there-are-running-volumes&#34;&gt;Case 2: Delete nodes when there are running volumes:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;For each node that will be deleted later:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;create and attach 4 volumes:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The 1st volume contains 1 replica only. Both the engines and the replicas are on the pre-delete node. (Attached to the pre-delete node.)&lt;/li&gt;&#xA;&lt;li&gt;The 2nd volume contains 1 replica only. The engine is on the pre-delete node and the replica is on another node. (Attached to the pre-delete node.)&lt;/li&gt;&#xA;&lt;li&gt;The 3rd and the 4th volume contain 3 replicas. Both volumes are attached to a node except for the pre-delete node.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Write some data to all volumes.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Delete multiple nodes in the cluster simultaneously. Make sure the corresponding Kubernetes node object is deleted. &amp;ndash;&amp;gt; The related Longhorn node objects will be cleaned up immediately, too.&lt;/li&gt;&#xA;&lt;li&gt;For each deleted node:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Verify the volume Health state  &amp;ndash;&amp;gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The 1st volume should become &lt;code&gt;Faulted&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;The 2nd volume should keep &lt;code&gt;Unknown&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;The 3rd and the 4th volume should become &lt;code&gt;Degraded&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Delete the 1st volume. &amp;ndash;&amp;gt; The volume can be deleted.&lt;/li&gt;&#xA;&lt;li&gt;Detach then reattach the 2nd volume. &amp;ndash;&amp;gt; The volume works fine and the data is correct.&lt;/li&gt;&#xA;&lt;li&gt;Crash all replicas of the 3rd volume and trigger the auto salvage. &amp;ndash;&amp;gt; The auto salvage should work. The volume works fine and the data is correct after the salvage.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;An example to crash every volume replica instance manager pods with kubectl:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;kubectl delete pods -l longhorn.io/instance-manager-type=replica -n longhorn-system --wait&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Disabled the auto salvage setting. Then crash all replicas of the 3rd volume again. &amp;ndash;&amp;gt; The replica on the deleted node cannot be salvaged manually, too. The salvage feature still works fine.&lt;/li&gt;&#xA;&lt;li&gt;Deleted the replica on the deleted node for the 4th volume. &amp;ndash;&amp;gt; The replica can be deleted.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test Node Drain Policy Setting</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/test-node-drain-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/test-node-drain-policy/</guid>
      <description>&lt;h2 id=&#34;with-node-drain-policy-is-block-if-contains-last-replica&#34;&gt;With &lt;code&gt;node-drain-policy&lt;/code&gt; is &lt;code&gt;block-if-contains-last-replica&lt;/code&gt;&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Note:&#xA;Starting from v1.5.x, it is not necessary to check for the presence of longhorn-admission-webhook and longhorn-conversion-webhook.&#xA;Please refer to the Longhorn issue &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/5590&#34;&gt;#5590&lt;/a&gt; for more details.&lt;/p&gt;&#xA;&lt;p&gt;Starting from v1.5.x, observe that the instance-manager-r and instance-manager-e are combined into instance-manager.&#xA;Ref &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/5208&#34;&gt;5208&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h3 id=&#34;1-basic-unit-tests&#34;&gt;1. Basic unit tests&lt;/h3&gt;&#xA;&lt;h4 id=&#34;11-single-worker-node-cluster-with-separate-master-node&#34;&gt;1.1 Single worker node cluster with separate master node&lt;/h4&gt;&#xA;&lt;p&gt;1.1.1 RWO volumes&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Deploy Longhorn&lt;/li&gt;&#xA;&lt;li&gt;Verify that there is no PDB for &lt;code&gt;csi-attacher&lt;/code&gt;, &lt;code&gt;csi-provisioner&lt;/code&gt;, &lt;code&gt;longhorn-admission-webhook&lt;/code&gt;, and &lt;code&gt;longhorn-conversion-webhook&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Manually create a PVC (simulate the volume which has never been attached scenario)&lt;/li&gt;&#xA;&lt;li&gt;Verify that there is no PDB for &lt;code&gt;csi-attacher&lt;/code&gt;, &lt;code&gt;csi-provisioner&lt;/code&gt;, &lt;code&gt;longhorn-admission-webhook&lt;/code&gt;, and &lt;code&gt;longhorn-conversion-webhook&lt;/code&gt; because there is no attached volume&lt;/li&gt;&#xA;&lt;li&gt;Create a deployment that uses one RW0 Longhorn volume.&lt;/li&gt;&#xA;&lt;li&gt;Verify that there is PDB for &lt;code&gt;csi-attacher&lt;/code&gt;, &lt;code&gt;csi-provisioner&lt;/code&gt;, &lt;code&gt;longhorn-admission-webhook&lt;/code&gt;, and &lt;code&gt;longhorn-conversion-webhook&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create another deployment that uses one RWO Longhorn volume. Scale down this deployment so that the volume is detached&lt;/li&gt;&#xA;&lt;li&gt;Drain the node by &lt;code&gt;kubectl drain &amp;lt;node-name&amp;gt; --ignore-daemonsets --delete-emptydir-data --force&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Observe that the workload pods are evited first -&amp;gt; PDB of &lt;code&gt;csi-attacher&lt;/code&gt;, &lt;code&gt;csi-provisioner&lt;/code&gt;, &lt;code&gt;longhorn-admission-webhook&lt;/code&gt;, and &lt;code&gt;longhorn-conversion-webhook&lt;/code&gt; are removed -&amp;gt; &lt;code&gt;csi-attacher&lt;/code&gt;, &lt;code&gt;csi-provisioner&lt;/code&gt;, &lt;code&gt;longhorn-admission-webhook&lt;/code&gt;, and &lt;code&gt;longhorn-conversion-webhook&lt;/code&gt;, and instance-manager-e pods are evicted -&amp;gt; all volumes are successfully detached&lt;/li&gt;&#xA;&lt;li&gt;Observe that instance-manager-r is NOT evicted.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;1.1.2 RWX volume&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Node ID Change During Backing Image Creation</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-node-id-change-during-backing-image-creation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-node-id-change-during-backing-image-creation/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/4887&#34;&gt;https://github.com/longhorn/longhorn/issues/4887&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;steps&#34;&gt;Steps&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; A relatively large file so that uploading it would take several minutes at least.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Upload the file as a backing image.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Monitor the longhorn manager pod logs.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Add new nodes for the cluster or new disks for the existing Longhorn nodes during the upload.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Should see the upload success.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Should not see error messages like below in the longhorn manager pods.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Node Selector</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-node-selector/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-node-selector/</guid>
      <description>&lt;h3 id=&#34;prepare-the-cluster&#34;&gt;Prepare the cluster&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Using Rancher RKE to create a cluster of 2 Windows worker nodes and 3 Linux worker nodes.&lt;/li&gt;&#xA;&lt;li&gt;Rancher will add the taint &lt;code&gt;cattle.io/os=linux:NoSchedule&lt;/code&gt; to Linux nodes&lt;/li&gt;&#xA;&lt;li&gt;Kubernetes will add label &lt;code&gt;kubernetes.io/os:linux&lt;/code&gt; to Linux nodes&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;test-steps&#34;&gt;Test steps&lt;/h3&gt;&#xA;&lt;p&gt;Repeat the following steps for each type of Longhorn installation: Rancher, Helm, Kubectl:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Follow the Longhorn document at the PR &lt;a href=&#34;https://github.com/longhorn/website/pull/287&#34;&gt;https://github.com/longhorn/website/pull/287&lt;/a&gt; to install Longhorn with toleration &lt;code&gt;cattle.io/os=linux:NoSchedule&lt;/code&gt; and node selector &lt;code&gt;kubernetes.io/os:linux&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn get deployed successfully on the 3 Linux nodes&lt;/li&gt;&#xA;&lt;li&gt;Verify all volume basic functionalities is working ok&lt;/li&gt;&#xA;&lt;li&gt;Create a volume of 3 replica named &lt;code&gt;vol-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Add label &lt;code&gt;longhorn.io/rating:best&lt;/code&gt; to 2 Linux nodes&lt;/li&gt;&#xA;&lt;li&gt;Follow the Longhorn document at the PR &lt;a href=&#34;https://github.com/longhorn/website/pull/287&#34;&gt;https://github.com/longhorn/website/pull/287&lt;/a&gt; to set 2 node selectors &lt;code&gt;kubernetes.io/os:linux&lt;/code&gt; and &lt;code&gt;longhorn.io/rating:best&lt;/code&gt; for Longhorn&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn gets deployed successfully on the 2 Linux nodes&lt;/li&gt;&#xA;&lt;li&gt;Attach the &lt;code&gt;vol-1&lt;/code&gt; to a node, verify that the attachment succeeds. One replica of the volume fails because it is on the down node.&lt;/li&gt;&#xA;&lt;li&gt;Delete all failed replicas on the down node&lt;/li&gt;&#xA;&lt;li&gt;Delete the down node, verify the deletion succeeds&lt;/li&gt;&#xA;&lt;li&gt;Detach the &lt;code&gt;vol-1&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Follow the Longhorn document at the PR &lt;a href=&#34;https://github.com/longhorn/website/pull/287&#34;&gt;https://github.com/longhorn/website/pull/287&lt;/a&gt; to set 1 node selector &lt;code&gt;kubernetes.io/os:linux&lt;/code&gt; for Longhorn&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn get redeployed successfully on the 3 Linux nodes&lt;/li&gt;&#xA;&lt;li&gt;Attach the &lt;code&gt;vol-1&lt;/code&gt; to a node, verify that the attachment succeeds. Longhorn starts rebuild the 3rd replica on the new node&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test NPE when longhorn UI deployment CR not exist</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-npe-when-longhorn-ui-deployment-not-exist/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-npe-when-longhorn-ui-deployment-not-exist/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/4065&#34;&gt;https://github.com/longhorn/longhorn/issues/4065&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test&#34;&gt;Test&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; helm install Longhorn&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; delete &lt;code&gt;deployment/longhorn-ui&lt;/code&gt;&#xA;&lt;em&gt;And&lt;/em&gt; update &lt;code&gt;setting/kubernetes-cluster-autoscaler-enabled&lt;/code&gt; to true or false&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; longhorn-manager pods should still be &lt;code&gt;Running&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Online Expansion</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-online-expansion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-online-expansion/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1674&#34;&gt;https://github.com/longhorn/longhorn/issues/1674&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-online-expansion-with-continuous-readingwriting&#34;&gt;Test online expansion with continuous reading/writing&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Prepare a relatively large file (5Gi for example) with the checksum calculated.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Create and attach a volume.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Monitor the instance manager pod logs.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Use &lt;code&gt;dd&lt;/code&gt; to copy data from the file to the Longhorn block device.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;dd if=/mnt/data of=/dev/longhorn/vol bs=1M&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Do online expansion for the volume during the copying.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; The expansion should success. The corresponding block device on the attached node is expanded.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test PVC Name and Namespace included in the volume metrics</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-pvc-name-and-namespace-included-in-volume-metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-pvc-name-and-namespace-included-in-volume-metrics/</guid>
      <description>&lt;h2 id=&#34;related-issues&#34;&gt;Related issues&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/5297&#34;&gt;https://github.com/longhorn/longhorn/issues/5297&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn-manager/pull/2284&#34;&gt;https://github.com/longhorn/longhorn-manager/pull/2284&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;test-step&#34;&gt;Test step&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; created 2 volumes (volume-1, volume-2)&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; PVC created for volume (volume-1)&#xA;And attached volumes (volume-1, volume-2)&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; metrics with &lt;code&gt;longhorn_volume_&lt;/code&gt; prefix should include &lt;code&gt;pvc=&amp;quot;volume-1&amp;quot;&lt;/code&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -sSL http://10.0.2.212:32744/metrics | grep longhorn_volume | grep ip-10-0-2-151 | grep volume-1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_actual_size_bytes&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_capacity_bytes&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; 1.073741824e+09&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_read_iops&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_read_latency&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_read_throughput&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_robustness&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_state&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_write_iops&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_write_latency&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_write_throughput&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-1&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And metrics with &lt;code&gt;longhorn_volume_&lt;/code&gt; prefix should include &lt;code&gt;pvc=&amp;quot;&amp;quot;&lt;/code&gt; for (volume-2)&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&amp;gt; curl -sSL http://10.0.2.212:32744/metrics | grep longhorn_volume | grep ip-10-0-2-151 | grep volume-2&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_actual_size_bytes&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-2&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_capacity_bytes&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-2&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; 1.073741824e+09&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_read_iops&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-2&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_read_latency&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-2&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_read_throughput&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-2&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_robustness&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-2&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_state&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-2&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_write_iops&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-2&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_write_latency&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-2&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;longhorn_volume_write_throughput&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;pvc_namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,node&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ip-10-0-2-151&amp;#34;&lt;/span&gt;,pvc&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,volume&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;volume-2&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Test Read Write Many Feature</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/rwx_feature/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/rwx_feature/</guid>
      <description>&lt;h1 id=&#34;prerequisite&#34;&gt;Prerequisite:&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Set up a Cluster of 4 nodes (1 etc/control plane and 3 workers)&lt;/li&gt;&#xA;&lt;li&gt;Deploy Latest Longhorn-master&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-statefulsetdeployment-with-single-pod-with-volume-attached-in-rwx-mode&#34;&gt;Create StatefulSet/Deployment with single pod with volume attached in RWX mode.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 1 pod.&lt;/li&gt;&#xA;&lt;li&gt;Verify that a PVC, ShareManger pod, CRD and volume in Longhorn get created.&lt;/li&gt;&#xA;&lt;li&gt;Verify share-manager pod come up healthy.&lt;/li&gt;&#xA;&lt;li&gt;Verify there is directory with the name of PVC exists in the ShareManager mount point.&#xA;Example - &lt;code&gt;ls /export/pvc-8c3481c7-4127-47c3-b840-5f41dc9d603e&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod and verify the same data reflects in the ShareManager.&lt;/li&gt;&#xA;&lt;li&gt;Verify the longhorn volume, it should reflect the correct size.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-statefulsetdeployment-with-more-than-1-pod-with-volume-attached-in-rwx-mode&#34;&gt;Create StatefulSet/Deployment with more than 1 pod with volume attached in RWX mode.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Verify that 2 volumes in Longhorn get created.&lt;/li&gt;&#xA;&lt;li&gt;Verify there is directory with the name of PVC exists in the ShareManager mount point i.e. &lt;code&gt;export&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn UI shows all the pods name attached to the volume.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in all the pod and verify all the data reflects in the ShareManager.&lt;/li&gt;&#xA;&lt;li&gt;Verify the longhorn volume, it should reflect the correct size.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-statefulsetdeployment-with-the-existing-pvc-of-a-rwx-volume&#34;&gt;Create StatefulSet/Deployment with the existing PVC of a RWX volume.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 1 pod.&lt;/li&gt;&#xA;&lt;li&gt;Verify that a PVC, ShareManger pod, CRD and volume in Longhorn get created.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod and verify the same data reflects in the ShareManager.&lt;/li&gt;&#xA;&lt;li&gt;Create another StatefulSet/Deployment using the above created PVC.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the new pod, the same should be reflected in the ShareManager pod.&lt;/li&gt;&#xA;&lt;li&gt;Verify the longhorn volume, it should reflect the correct size.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;scale-up-statefulsetdeployment-with-one-pod-attached-with-volume-in-rwx-mode&#34;&gt;Scale up StatefulSet/Deployment with one pod attached with volume in RWX mode.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 1 pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod and verify the same data reflects in the ShareManager.&lt;/li&gt;&#xA;&lt;li&gt;Scale up the StatefulSet/Deployment.&lt;/li&gt;&#xA;&lt;li&gt;Verify a new volume gets created.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the new pod, the same should be reflected in the ShareManager pod.&lt;/li&gt;&#xA;&lt;li&gt;Verify the longhorn volume, it should reflect the correct size.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;scale-down-statefulsetdeployment-attached-with-volume-in-rwx-mode-to-zero&#34;&gt;Scale down StatefulSet/Deployment attached with volume in RWX mode to zero.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 1 pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod and verify the same data reflects in the ShareManager.&lt;/li&gt;&#xA;&lt;li&gt;Scale down the StatefulSet/Deployment to zero&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager pod gets deleted.&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume should be in detached state.&lt;/li&gt;&#xA;&lt;li&gt;Create a new StatefulSet/Deployment with the existing PVC.&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager should get created and volume should become attached.&lt;/li&gt;&#xA;&lt;li&gt;Verify the data.&lt;/li&gt;&#xA;&lt;li&gt;Delete the newly created StatefulSet/Deployment.&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager pod gets deleted again.&lt;/li&gt;&#xA;&lt;li&gt;Scale up the first StatefulSet/Deployment.&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager should get created and volume should become attached.&lt;/li&gt;&#xA;&lt;li&gt;Verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;delete-the-workload-statefulsetdeployment-attached-with-rwx-volume&#34;&gt;Delete the Workload StatefulSet/Deployment attached with RWX volume.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod and verify the same data reflects in the ShareManager.&lt;/li&gt;&#xA;&lt;li&gt;Delete the workload.&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager pod gets deleted but the CRD should not be deleted.&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager.status.state == &amp;ldquo;stopped&amp;rdquo;. &lt;code&gt;kubectl get ShareManager -n longhorn-system&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify the volume should be in detached state.&lt;/li&gt;&#xA;&lt;li&gt;Create another StatefulSet with existing PVC.&lt;/li&gt;&#xA;&lt;li&gt;Verify the ShareManager should get created and volume should become attached.&lt;/li&gt;&#xA;&lt;li&gt;Verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;take-snapshot-and-backup-of-a-rwx-volume-in-longhorn&#34;&gt;Take snapshot and backup of a RWX volume in Longhorn.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a snapshot and a backup.&lt;/li&gt;&#xA;&lt;li&gt;Write some more data into the pod.&lt;/li&gt;&#xA;&lt;li&gt;Revert to snapshot 1 and verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;restore-a-backup-taken-from-a-rwx-volume-in-longhorn&#34;&gt;Restore a backup taken from a RWX volume in Longhorn.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup of the RWX volume.&lt;/li&gt;&#xA;&lt;li&gt;Restore from the backup and select access mode as &lt;code&gt;rwx&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Verify the restored volume has &lt;code&gt;volume.spec.accessMode&lt;/code&gt; as &lt;code&gt;rwx&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Create PV/PVC with &lt;code&gt;accessMode&lt;/code&gt; as &lt;code&gt;rwx&lt;/code&gt; for restored volume or create PV/PVC using Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Attach a pod to the PVC created and verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;restore-an-rwx-backup-into-an-rwo-volume&#34;&gt;Restore an RWX backup into an RWO volume.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup of the RWX volume.&lt;/li&gt;&#xA;&lt;li&gt;Restore from the backup and select access mode as &lt;code&gt;rwo&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Verify the restored volume has &lt;code&gt;volume.spec.accessMode&lt;/code&gt; as &lt;code&gt;rwo&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Create a PV and PVC with &lt;code&gt;accessMode&lt;/code&gt; as &lt;code&gt;rwo&lt;/code&gt; for the restored volume or create them using Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Attach a pod to the PVC and verify the data.&lt;/li&gt;&#xA;&lt;li&gt;Try to attach the PVC to another pod on another node, user should get &lt;code&gt;multi attach&lt;/code&gt; error.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;restore-an-rwo-backup-into-an-rwx-volume&#34;&gt;Restore an RWO backup into an RWX volume.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWO mode using longhorn class by selecting the option &lt;code&gt;read write once&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 1 pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup of the RWO volume.&lt;/li&gt;&#xA;&lt;li&gt;Restore from the backup and select access mode as &lt;code&gt;rwx&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Create a PV and PVC with &lt;code&gt;accessMode&lt;/code&gt; as &lt;code&gt;rwx&lt;/code&gt; for the restored volume or create them using Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify the restored volume has &lt;code&gt;volume.spec.accessMode&lt;/code&gt; as &lt;code&gt;rwx&lt;/code&gt; now.&lt;/li&gt;&#xA;&lt;li&gt;Attach a pod to the PVC and verify the data.&lt;/li&gt;&#xA;&lt;li&gt;Attach more pods to the PVC, verify the volume is accessible from multiple pods.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-pv-and-pvc-using-longhorn-ui-for-rwxrwo-volume&#34;&gt;Create PV and PVC using Longhorn UI for RWX/RWO volume&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create an RWX volume using Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Select the volume and create PV/PVC.&lt;/li&gt;&#xA;&lt;li&gt;Verify the PV and PVC are created with &lt;code&gt;rwx&lt;/code&gt; access mode.&lt;/li&gt;&#xA;&lt;li&gt;Create an RWO volume using Longhorn UI.&lt;/li&gt;&#xA;&lt;li&gt;Select the volume and create PV/PVC.&lt;/li&gt;&#xA;&lt;li&gt;Verify the PV and PVC are created with &lt;code&gt;rwo&lt;/code&gt; access mode.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-rwx-dr-volume-of-a-rwx-volume-in-longhorn&#34;&gt;Create RWX DR volume of a RWX volume in Longhorn.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup of the volume.&lt;/li&gt;&#xA;&lt;li&gt;Create a DR volume of the backup by selecting &lt;code&gt;rwx&lt;/code&gt; in access mode.&lt;/li&gt;&#xA;&lt;li&gt;Write more data in the pods and take more backups.&lt;/li&gt;&#xA;&lt;li&gt;Verify the DR volume is getting synced with latest backup.&lt;/li&gt;&#xA;&lt;li&gt;Activate the DR volume, attach it to multiple pods and verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-rwo-dr-volume-of-a-rwx-volume-in-longhorn&#34;&gt;Create RWO DR volume of a RWX volume in Longhorn.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup of the volume.&lt;/li&gt;&#xA;&lt;li&gt;Create a DR volume of the backup by selecting &lt;code&gt;rwo&lt;/code&gt; in access mode.&lt;/li&gt;&#xA;&lt;li&gt;Write more data in the pods and take more backups.&lt;/li&gt;&#xA;&lt;li&gt;Verify the DR volume is getting synced with latest backup.&lt;/li&gt;&#xA;&lt;li&gt;Activate the DR volume, attach it to a pod and verify the data.&lt;/li&gt;&#xA;&lt;li&gt;Try to attach it to multiple pods, it should show &lt;code&gt;multi attach error&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;create-rwx-dr-volume-of-a-rwo-volume-in-longhorn&#34;&gt;Create RWX DR volume of a RWO volume in Longhorn.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWO mode using longhorn class by selecting the option &lt;code&gt;read write once&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 1 pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Take a backup of the volume.&lt;/li&gt;&#xA;&lt;li&gt;Create a DR volume of the backup by selecting &lt;code&gt;rwx&lt;/code&gt; in access mode.&lt;/li&gt;&#xA;&lt;li&gt;Write more data in the pod and take more backups.&lt;/li&gt;&#xA;&lt;li&gt;Verify the DR volume is getting synced with latest backup.&lt;/li&gt;&#xA;&lt;li&gt;Activate the DR volume, attach it to multiple pods and verify the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;expand-the-rwx-volume&#34;&gt;Expand the RWX volume.&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Attach the PVC to a StatefulSet/Deployment with 2 pods.&lt;/li&gt;&#xA;&lt;li&gt;Write some data in the pod.&lt;/li&gt;&#xA;&lt;li&gt;Scale down the StatefulSet/Deployment.&lt;/li&gt;&#xA;&lt;li&gt;Once the volume is detached, expand the volume.&lt;/li&gt;&#xA;&lt;li&gt;Scale up the StatefulSet/Deployment and verify that user is able to write data in the expanded volume.&lt;/li&gt;&#xA;&lt;li&gt;Verify the new size of the volume (same approach as in writing the data).&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;recurring-backupsnapshot-with-rwx-volume&#34;&gt;Recurring Backup/Snapshot with RWX volume.&lt;/h1&gt;&#xA;&lt;p&gt;Create a PVC with RWX mode using longhorn class by selecting the option &lt;code&gt;read write many&lt;/code&gt;.&#xA;2. Attach the PVC to a StatefulSet/Deployment with 2 pods.&#xA;3. Write some data in the pod.&#xA;4. Schedule a recurring backup/Snapshot.&#xA;5. Verify the recurring jobs are getting created and is taking backup/snapshot successfully.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Replica Disk Soft Anti-Affinity</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-replica-disk-soft-anti-affinity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-replica-disk-soft-anti-affinity/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/3823&#34;&gt;https://github.com/longhorn/longhorn/issues/3823&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-initial-behavior-of-global-replica-disk-soft-anti-affinity-setting&#34;&gt;Test initial behavior of global Replica Disk Soft Anti-Affinity setting&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; A newly created Longhorn cluster&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; &lt;code&gt;Replica Zone Disk Anti-Affinity&lt;/code&gt; shows as &lt;code&gt;false&lt;/code&gt; in the UI&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; the &lt;code&gt;replica-soft-anti-affinity&lt;/code&gt; setting shows &lt;code&gt;false&lt;/code&gt; with kubectl&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-initial-behavior-of-global-replica-disk-soft-anti-affinity-setting-after-upgrade&#34;&gt;Test initial behavior of global Replica Disk Soft Anti-Affinity setting after upgrade&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; A newly upgraded Longhorn cluster&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; &lt;code&gt;Replica Zone Disk Anti-Affinity&lt;/code&gt; shows as &lt;code&gt;false&lt;/code&gt; in the UI&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; the &lt;code&gt;replica-soft-anti-affinity&lt;/code&gt; shows &lt;code&gt;false&lt;/code&gt; with kubectl&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test replica scale-down warning</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-replica-scale-down-warning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-replica-scale-down-warning/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/4120&#34;&gt;https://github.com/longhorn/longhorn/issues/4120&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;steps&#34;&gt;Steps&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; &lt;code&gt;Replica Auto Balance&lt;/code&gt; set to &lt;code&gt;least-effort&lt;/code&gt; or &lt;code&gt;best-effort&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Volume with 3 replicas created.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Volume attached to &lt;code&gt;node-1&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Monitor &lt;code&gt;node-1&lt;/code&gt; manager pod events.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl alpha events -n longhorn-system pod &amp;lt;node-1 manager pod&amp;gt; -w&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Update replica count to 1.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; Should see &lt;code&gt;Normal&lt;/code&gt; replice delete event.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Normal    Delete                   Engine/t1-e-6a846a7a                                Removed unknown replica tcp://10.42.2.94:10000 from engine&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Should not see &lt;code&gt;Warning&lt;/code&gt; unknown replica detect event.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Warning   Faulted                  Engine/t1-e-6a846a7a                                   Detected replica UNKNOWN-tcp://10.42.1.98:10000 (10.42.1.98:10000) in error&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Test RWX share-mount ownership reset</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/rwx-mount-ownership-reset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/rwx-mount-ownership-reset/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2357&#34;&gt;https://github.com/longhorn/longhorn/issues/2357&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-rwx-share-mount-ownership&#34;&gt;Test RWX share-mount ownership&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Setup one of cluster node to use host FQDN.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;root@ip-172-30-0-139:/home/ubuntu# cat /etc/hosts&#xA;127.0.0.1 localhost&#xA;54.255.224.72 ip-172-30-0-139.lan ip-172-30-0-139&#xA;&#xA;root@ip-172-30-0-139:/home/ubuntu# hostname&#xA;ip-172-30-0-139&#xA;&#xA;root@ip-172-30-0-139:/home/ubuntu# hostname -f&#xA;ip-172-30-0-139.lan&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; &lt;code&gt;Domain = localdomain&lt;/code&gt; is commented out in &lt;code&gt;/etc/idmapd.conf&lt;/code&gt; on cluster hosts.&#xA;This is to ensure &lt;code&gt;localdomain&lt;/code&gt; is not enforce to sync between server and client.&#xA;Ref: &lt;a href=&#34;https://github.com/longhorn/website/pull/279&#34;&gt;https://github.com/longhorn/website/pull/279&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;root@ip-172-30-0-139:~# cat /etc/idmapd.conf &#xA;[General]&#xA;&#xA;Verbosity = 0&#xA;Pipefs-Directory = /run/rpc_pipefs&#xA;# set your own domain here, if it differs from FQDN minus hostname&#xA;# Domain = localdomain&#xA;&#xA;[Mapping]&#xA;&#xA;Nobody-User = nobody&#xA;Nobody-Group = nogroup&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; pod with rwx pvc deployed to the node with host FQDN.&#xA;Here need to update &lt;code&gt;nodeSelector.kubernetes.io/hostname&lt;/code&gt; below to match&#xA;the node name.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test S3 backupstore in a cluster sitting behind a HTTP proxy</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/test-s3-backupstore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/test-s3-backupstore/</guid>
      <description>&lt;p&gt;&lt;sup&gt;Related issue: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/3136&#34;&gt;3136&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;&#xA;&lt;p&gt;Requirement:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Set up a stand alone Squid, HTTP web proxy&#xA;&lt;ul&gt;&#xA;&lt;li&gt;To configure Squid proxy: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1967#issuecomment-736959332&#34;&gt;a comment about squid config&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;If setting up instance on AWS: &lt;a href=&#34;https://user-images.githubusercontent.com/22139961/87112575-0ca3eb80-c221-11ea-86ef-91ed5f8384cc.png&#34;&gt;a EC2 security group setting&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;S3 with existing backups&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create credential for &lt;strong&gt;Backup Target&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; $ secret_name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;aws-secret-proxy&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; $ proxy_ip&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;123.123.123.123&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; $ no_proxy_params&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,192.168.0.0/16&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; $ kubectl create secret generic $secret_name &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;AWS_ACCESS_KEY_ID&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$AWS_ID &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;AWS_SECRET_ACCESS_KEY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$AWS_KEY &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;HTTP_PROXY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$proxy_ip:3128 &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;HTTPS_PROXY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$proxy_ip:3128 &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;NO_PROXY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$no_proxy_params &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt; -n longhorn-system&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;Open Longhorn UI&lt;/li&gt;&#xA;&lt;li&gt;Click on &lt;em&gt;Setting&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Scroll down to &lt;em&gt;Backup Target Credential Secret&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Fill in &lt;code&gt;$secret_name&lt;/code&gt; assigned in step 1. and save setting&lt;/li&gt;&#xA;&lt;li&gt;Go to &lt;em&gt;Backup&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Restore from existing backups and watch the volume become ready&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test scalability with backing image</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.3/test-scalability-with-backing-image/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.3/test-scalability-with-backing-image/</guid>
      <description>&lt;h3 id=&#34;test-step&#34;&gt;Test step&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Deploy a cluster with 3 worker nodes. The recommended nodes is 4v cores CPU + 8G memory at least.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Deploy Longhorn.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Launch 10 backing images with the following YAML:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;BackingImage&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bi-test1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-system&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;download&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceParameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;url&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;BackingImage&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bi-test2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-system&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;download&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceParameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;url&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;BackingImage&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bi-test3&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-system&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;download&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceParameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;url&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;BackingImage&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bi-test4&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-system&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;download&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceParameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;url&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;BackingImage&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bi-test5&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-system&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;download&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceParameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;url&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;BackingImage&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bi-test6&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-system&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;download&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceParameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;url&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;BackingImage&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bi-test7&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-system&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;download&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceParameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;url&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;BackingImage&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bi-test8&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-system&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;download&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceParameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;url&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;BackingImage&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bi-test9&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-system&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;download&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceParameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;url&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn.io/v1beta1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;BackingImage&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bi-test10&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;longhorn-system&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;download&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;sourceParameters&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;url&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://longhorn-backing-image.s3-us-west-1.amazonaws.com/parrot.qcow2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Use the following YAML to launch some workloads with 250 volumes. Each volume uses 1 backing image and contains 3 replicas.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Service Account mount on host</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-service-account-mount/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/test-service-account-mount/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;This test case should be tested on both yaml installation, chart installation (Helm and Rancher UI), as well as upgrade scenarios&lt;/li&gt;&#xA;&lt;li&gt;After install Longhorn using on of the above method, ssh into a worker node that has a longhorn-manager pod running&lt;/li&gt;&#xA;&lt;li&gt;check the mount point &lt;code&gt;/run/secrets/kubernetes.io/serviceaccount&lt;/code&gt; by running:&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;root@node-1:~# findmnt  /run/secrets/kubernetes.io/serviceaccount&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that there is no such mount point&lt;/li&gt;&#xA;&lt;li&gt;Kill the longhorn-manager pod on the above node and wait for it to be recreated and running&lt;/li&gt;&#xA;&lt;li&gt;check the mount point &lt;code&gt;/run/secrets/kubernetes.io/serviceaccount&lt;/code&gt; by running:&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;root@node-1:~# findmnt  /run/secrets/kubernetes.io/serviceaccount&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that there is no such mount point&lt;/li&gt;&#xA;&lt;li&gt;Repeat the step 5 to 7 a few times&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test Snapshot Purge Error Handling</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/snapshot-purge-error-handling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/snapshot-purge-error-handling/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1895&#34;&gt;https://github.com/longhorn/longhorn/issues/1895&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Longhorn v1.1.1 handles the error during snapshot purge better and reports to Longhorn-manager.&lt;/p&gt;&#xA;&lt;h2 id=&#34;scenario-1&#34;&gt;Scenario-1&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume with 3 replicas and attach to a pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data into the volume and take a snapshot.&lt;/li&gt;&#xA;&lt;li&gt;Delete a replica that will result in creating a system generated snapshot.&lt;/li&gt;&#xA;&lt;li&gt;Wait for replica to finish and take another snapshot.&lt;/li&gt;&#xA;&lt;li&gt;ssh into a node and resize the latest snapshot. (e.g &lt;code&gt;dd if=/dev/urandom count=50 bs=1M of=&amp;lt;SNAPSHOT-NAME&amp;gt;.img&lt;/code&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Trigger snapshot purge by delete the oldest snapshot.&lt;/li&gt;&#xA;&lt;li&gt;Verify the replica (on the node from step 5) shows error &lt;code&gt;file sizes are not equal and the parent file is larger than the child file&lt;/code&gt; and starts to rebuild.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;scenario-2&#34;&gt;Scenario-2&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a volume with 3 replicas and attach to a pod.&lt;/li&gt;&#xA;&lt;li&gt;Write some data into the volume and take two snapshots.&lt;/li&gt;&#xA;&lt;li&gt;Delete a replica that will result in creating a system generated snapshot.&lt;/li&gt;&#xA;&lt;li&gt;While the rebuilding is in progress, delete a snapshot to trigger SnapshotPurge.&lt;/li&gt;&#xA;&lt;li&gt;Verify that Longhorn manager reports error like &lt;code&gt;Failed to purge snapshots: REPLICA_ADDRESS: cannot purge snapshots because REPLICA_ADDRESS is rebuilding&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test snapshot purge retry</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-snapshot-purge-retry/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.0/test-snapshot-purge-retry/</guid>
      <description>&lt;h2 id=&#34;scenario&#34;&gt;Scenario&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create and attach a Longhorn volumes.&lt;/li&gt;&#xA;&lt;li&gt;Write some data to the volume then create the 1st snapshot. e.g.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;dd if=/dev/urandom of=/dev/longhorn/&amp;lt;Longhorn volume name&amp;gt; bs=1M count=100&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Try to delete the 1st snapshot. The snapshot will be marked as &lt;code&gt;Removed&lt;/code&gt; then hidden on the volume detail page.&lt;/li&gt;&#xA;&lt;li&gt;Write some non-overlapping data to the volume then create the 2nd snapshot. e.g.&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;dd if=/dev/urandom of=/dev/longhorn/&amp;lt;Longhorn volume name&amp;gt; bs=1M count=100 seek=100&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Re-try deleting the 1st snapshot via UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify snapshot purge is triggered:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The 1st snapshot will be coalesced with the 2nd one, which means the size should be 200Mi.&lt;/li&gt;&#xA;&lt;li&gt;The final snapshot name is the name of the 2nd one.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test Support Bundle Metadata File</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-support-bundle-metadata-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-support-bundle-metadata-file/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/6997&#34;&gt;https://github.com/longhorn/longhorn/issues/6997&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test&#34;&gt;Test&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Longhorn installed on SUSE Linux&lt;br&gt;&#xA;&lt;strong&gt;When&lt;/strong&gt; generated support-bundle with description and issue URL&lt;br&gt;&#xA;&lt;strong&gt;Then&lt;/strong&gt; &lt;code&gt;issuedescription&lt;/code&gt; has the description in the metadata.yaml&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; &lt;code&gt;issueurl&lt;/code&gt; has the issue URL in the metadata.yaml&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Support Bundle Should Include Kubelet Log When On K3s Cluster</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-support-bundle-kubelet-log-for-k3s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-support-bundle-kubelet-log-for-k3s/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/7121&#34;&gt;https://github.com/longhorn/longhorn/issues/7121&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test&#34;&gt;Test&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Longhorn installed on K3s cluster&lt;br&gt;&#xA;&lt;strong&gt;When&lt;/strong&gt; generated support-bundle&lt;br&gt;&#xA;&lt;strong&gt;Then&lt;/strong&gt; should have worker node kubelet logs in &lt;code&gt;k3s-agent-service.log&lt;/code&gt;&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; should have control-plan node kubelet log in &lt;code&gt;k3s-service.log&lt;/code&gt; (if Longhorn is deployed on control-plan node)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Support Bundle Syslog Paths</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-support-bundle-syslog-paths/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-support-bundle-syslog-paths/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/6544&#34;&gt;https://github.com/longhorn/longhorn/issues/6544&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-varlogmessages&#34;&gt;Test /var/log/messages&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Longhorn installed on SUSE Linux&lt;br&gt;&#xA;&lt;strong&gt;When&lt;/strong&gt; generated support-bundle&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; syslog exists in the messages file&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-varlogsyslog&#34;&gt;Test /var/log/syslog&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Longhorn installed on Ubuntu Linux&lt;br&gt;&#xA;&lt;strong&gt;When&lt;/strong&gt; generated support-bundle&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; syslog exists in the syslog file&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test system upgrade with a new storage class being default</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/upgrade-with-customized-storage-class/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/upgrade-with-customized-storage-class/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Install a previous stable Longhorn on a K8s cluster.&lt;/li&gt;&#xA;&lt;li&gt;Create a storage class &amp;rsquo;longhorn-rep-2&amp;rsquo; with replica 2 and make it default.&lt;/li&gt;&#xA;&lt;li&gt;Create some volumes with the above created storage class and attach them to workloads.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to latest version.&lt;/li&gt;&#xA;&lt;li&gt;Longhorn should be upgraded.&lt;/li&gt;&#xA;&lt;li&gt;Storage class &amp;rsquo;longhorn-rep-2&amp;rsquo; should be the default storage class.&lt;/li&gt;&#xA;&lt;li&gt;Create two volumes, one with &amp;rsquo;longhorn&amp;rsquo; storage class and other with &amp;rsquo;longhorn-rep-2&#39;.&lt;/li&gt;&#xA;&lt;li&gt;Verify the volumes are created as per their storage class.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test System Upgrade with New Instance Manager</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/upgrade-with-new-instance-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/upgrade-with-new-instance-manager/</guid>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;Prepare 3 sets of longhorn-manager and longhorn-instance-manager images.&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn with the 1st set of images.&lt;/li&gt;&#xA;&lt;li&gt;Set &lt;code&gt;Guaranteed Instance Manager CPU&lt;/code&gt; to 40, respectively.&#xA;Then wait for the instance manager recreation.&lt;/li&gt;&#xA;&lt;li&gt;Create and attach a volume to a node (node1).&lt;/li&gt;&#xA;&lt;li&gt;Upgrade the Longhorn system with the 2nd set of images.&#xA;Verify the CPU requests in the pods of both instance managers match the settings.&lt;/li&gt;&#xA;&lt;li&gt;Create and attach one more volume to node1.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade the Longhorn system with the 3rd set of images.&lt;/li&gt;&#xA;&lt;li&gt;Verify the pods of the 3rd instance manager cannot be launched on node1 since there is no available CPU for the allocation.&lt;/li&gt;&#xA;&lt;li&gt;Detach the volume in the 1st instance manager pod.&#xA;Verify the related instance manager pods will be cleaned up and the new instance manager pod can be launched on node1.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test system upgrade with the deprecated CPU setting</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/system-upgrade-with-deprecated-cpu-setting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.1/system-upgrade-with-deprecated-cpu-setting/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2207&#34;&gt;https://github.com/longhorn/longhorn/issues/2207&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-step&#34;&gt;Test step&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy a cluster that each node has different CPUs.&lt;/li&gt;&#xA;&lt;li&gt;Launch Longhorn v1.1.0.&lt;/li&gt;&#xA;&lt;li&gt;Deploy some workloads using Longhorn volumes.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade to the latest Longhorn version. Validate:&#xA;&lt;ol&gt;&#xA;&lt;li&gt;all workloads work fine and no instance manager pod crash during the upgrade.&lt;/li&gt;&#xA;&lt;li&gt;The fields &lt;code&gt;node.Spec.EngineManagerCPURequest&lt;/code&gt; and &lt;code&gt;node.Spec.ReplicaManagerCPURequest&lt;/code&gt; of each node are the same as the setting &lt;code&gt;Guaranteed Engine CPU&lt;/code&gt; value in the old version * 1000.&lt;/li&gt;&#xA;&lt;li&gt;The old setting &lt;code&gt;Guaranteed Engine CPU&lt;/code&gt; is deprecated with an empty value.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Modify new settings &lt;code&gt;Guaranteed Engine Manager CPU&lt;/code&gt; and &lt;code&gt;Guaranteed Replica Manager CPU&lt;/code&gt;. Validate all workloads work fine and no instance manager pod restart.&lt;/li&gt;&#xA;&lt;li&gt;Scale down all workloads and wait for the volume detachment.&lt;/li&gt;&#xA;&lt;li&gt;Set &lt;code&gt;node.Spec.EngineManagerCPURequest&lt;/code&gt; and &lt;code&gt;node.Spec.ReplicaManagerCPURequest&lt;/code&gt; to 0 for some node. Verify the new settings will be applied to those node and the related instance manager pods will be recreated with the CPU requests matching the new settings.&lt;/li&gt;&#xA;&lt;li&gt;Scale up all workloads and verify the data as well as the volume r/w.&lt;/li&gt;&#xA;&lt;li&gt;Do cleanup.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test the trim related option update for old volumes</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.1/test-the-trim-related-option-update-for-old-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.1/test-the-trim-related-option-update-for-old-volumes/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/5218&#34;&gt;https://github.com/longhorn/longhorn/issues/5218&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-step&#34;&gt;Test step&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Deploy Longhorn v1.3.2&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Created and attached a volume.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Upgrade Longhorn to the latest.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Do live upgrade for the volume. (The 1st volume using the latest engine image but running in the old instance manager.)&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Created and attached a volume with the v1.3.2 engine image. (The 2nd volume using the old engine image but running in the new instance manager.)&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Try to update &lt;code&gt;volume.spec.unmapMarkSnapChainRemoved&lt;/code&gt; for both volumes via &lt;code&gt;kubectl&lt;/code&gt; or GUI&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test timeout on loss of network connectivity</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/resiliency/timeout/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/resiliency/timeout/</guid>
      <description>&lt;h2 id=&#34;rw-timeout-block-device&#34;&gt;R/W Timeout Block Device&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a docker network:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker network create -d bridge --subnet 192.168.22.0/24 longhorn-network&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;Start a replica:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --net longhorn-network --ip 192.168.22.2 &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;         -v /volume longhornio/longhorn-engine:&amp;lt;tag&amp;gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;         longhorn replica --listen 192.168.22.2:9502 --size 10g /volume&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;Start another replica:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --net longhorn-network --ip 192.168.22.3 &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;         -v /volume longhornio/longhorn-engine:&amp;lt;tag&amp;gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;         longhorn replica --listen 192.168.22.3:9502 --size 10g /volume&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;4&#34;&gt;&#xA;&lt;li&gt;In another terminal, start the controller:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run --net longhorn-network --ip 192.168.22.4 --privileged &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;         -v /dev:/dev -v /proc:/host/proc &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;         longhornio/longhorn-engine:&amp;lt;tag&amp;gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;         longhorn controller --replica tcp://192.168.22.2:9502 &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;         --replica tcp://192.168.22.3:9502 &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;         --frontend tgt-blockdev timeout-test&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;5&#34;&gt;&#xA;&lt;li&gt;In another terminal, perform I/O:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;fio --name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;random-writers --ioengine&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;libaio --rw&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;randwrite --bs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;16k &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;           --direct&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; --size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1000m --numjobs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;           --filename&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/dev/longhorn/timeout-test --iodepth&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;6&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;In another terminal, find the name of the container running the replica with &lt;code&gt;docker ps&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test transient error in engine status during eviction</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.1/test-backing-image-download-to-local/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.3.1/test-backing-image-download-to-local/</guid>
      <description>&lt;h3 id=&#34;test-step&#34;&gt;Test step&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create and attach a multi-replica volume.&lt;/li&gt;&#xA;&lt;li&gt;Prepare one extra disk for a node that contains at least one volume replica.&lt;/li&gt;&#xA;&lt;li&gt;Keep monitoring the engine YAML. e.g., &lt;code&gt;watch -n  &amp;quot;kubectl -n longhorn-system get lhe &amp;lt;engine name&amp;gt;&amp;quot;&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Evicting the old disk for node. =&amp;gt; Verify that there is no transient error in engine Status during eviction. A counter example is like:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: longhorn.io/v1beta2&#xA;kind: Engine&#xA;metadata:&#xA;  creationTimestamp: &amp;#34;2022-07-27T04:46:03Z&amp;#34;&#xA;  finalizers:&#xA;  - longhorn.io&#xA;  generation: 4&#xA;  labels:&#xA;    longhornnode: shuo-k8s-worker-1&#xA;    longhornvolume: vol1&#xA;  name: vol1-e-998b62c7&#xA;  namespace: longhorn-system&#xA;  ownerReferences:&#xA;  - apiVersion: longhorn.io/v1beta2&#xA;    kind: Volume&#xA;    name: vol1&#xA;    uid: fe656464-78af-4abf-8068-0742ba247fef&#xA;  resourceVersion: &amp;#34;34220387&amp;#34;&#xA;  uid: f6a339e2-d606-479f-910f-c787f9efa906&#xA;spec:&#xA;  active: true&#xA;  backupVolume: &amp;#34;&amp;#34;&#xA;  desireState: running&#xA;  disableFrontend: false&#xA;  engineImage: longhornio/longhorn-engine:master-head&#xA;  frontend: blockdev&#xA;  logRequested: false&#xA;  nodeID: shuo-k8s-worker-1&#xA;  replicaAddressMap:&#xA;    vol1-r-769e039f: 10.42.4.30:10000&#xA;    vol1-r-56144d78: 10.42.2.81:10000&#xA;    vol1-r-8724804e: 10.42.1.59:10000&#xA;  requestedBackupRestore: &amp;#34;&amp;#34;&#xA;  requestedDataSource: &amp;#34;&amp;#34;&#xA;  revisionCounterDisabled: false&#xA;  salvageRequested: false&#xA;  upgradedReplicaAddressMap: {}&#xA;  volumeName: vol1&#xA;  volumeSize: &amp;#34;1073741824&amp;#34;&#xA;status:&#xA;  backupStatus: null&#xA;  cloneStatus:&#xA;    tcp://10.42.1.59:10000:&#xA;      error: &amp;#34;&amp;#34;&#xA;      fromReplicaAddress: &amp;#34;&amp;#34;&#xA;      isCloning: false&#xA;      progress: 0&#xA;      snapshotName: &amp;#34;&amp;#34;&#xA;      state: &amp;#34;&amp;#34;&#xA;    tcp://10.42.2.81:10000:&#xA;      error: &amp;#34;&amp;#34;&#xA;      fromReplicaAddress: &amp;#34;&amp;#34;&#xA;      isCloning: false&#xA;      progress: 0&#xA;      snapshotName: &amp;#34;&amp;#34;&#xA;      state: &amp;#34;&amp;#34;&#xA;    tcp://10.42.4.30:10000:&#xA;      error: &amp;#34;&amp;#34;&#xA;      fromReplicaAddress: &amp;#34;&amp;#34;&#xA;      isCloning: false&#xA;      progress: 0&#xA;      snapshotName: &amp;#34;&amp;#34;&#xA;      state: &amp;#34;&amp;#34;&#xA;    tcp://10.42.4.30:10015:&#xA;      error: &amp;#39;failed to get snapshot clone status of tcp://10.42.4.30:10015: failed&#xA;        to get snapshot clone status: rpc error: code = Unavailable desc = all SubConns&#xA;        are in TransientFailure, latest connection error: connection error: desc =&#xA;        &amp;#34;transport: Error while dialing dial tcp 10.42.4.30:10017: connect: connection&#xA;        refused&amp;#34;&amp;#39;&#xA;      fromReplicaAddress: &amp;#34;&amp;#34;&#xA;      isCloning: false&#xA;      progress: 0&#xA;      snapshotName: &amp;#34;&amp;#34;&#xA;      state: &amp;#34;&amp;#34;&#xA;  currentImage: longhornio/longhorn-engine:master-head&#xA;  currentReplicaAddressMap:&#xA;    vol1-r-769e039f: 10.42.4.30:10000&#xA;    vol1-r-56144d78: 10.42.2.81:10000&#xA;    vol1-r-8724804e: 10.42.1.59:10000&#xA;  currentSize: &amp;#34;1073741824&amp;#34;&#xA;  currentState: running&#xA;  endpoint: /dev/longhorn/vol1&#xA;  instanceManagerName: instance-manager-e-3bdc3f00&#xA;  ip: 10.42.4.31&#xA;  isExpanding: false&#xA;  lastExpansionError: &amp;#34;&amp;#34;&#xA;  lastExpansionFailedAt: &amp;#34;&amp;#34;&#xA;  lastRestoredBackup: &amp;#34;&amp;#34;&#xA;  logFetched: false&#xA;  ownerID: shuo-k8s-worker-1&#xA;  port: 10001&#xA;  purgeStatus:&#xA;    tcp://10.42.1.59:10000:&#xA;      error: &amp;#34;&amp;#34;&#xA;      isPurging: false&#xA;      progress: 0&#xA;      state: &amp;#34;&amp;#34;&#xA;    tcp://10.42.2.81:10000:&#xA;      error: &amp;#34;&amp;#34;&#xA;      isPurging: false&#xA;      progress: 0&#xA;      state: &amp;#34;&amp;#34;&#xA;    tcp://10.42.4.30:10000:&#xA;      error: &amp;#34;&amp;#34;&#xA;      isPurging: false&#xA;      progress: 0&#xA;      state: &amp;#34;&amp;#34;&#xA;    tcp://10.42.4.30:10015:&#xA;      error: &amp;#34;&amp;#34;&#xA;      isPurging: false&#xA;      progress: 0&#xA;      state: &amp;#34;&amp;#34;&#xA;  rebuildStatus: {}&#xA;  replicaModeMap:&#xA;    vol1-r-769e039f: RW&#xA;    vol1-r-47418d68: RW&#xA;    vol1-r-56144d78: RW&#xA;    vol1-r-8724804e: RW&#xA;  restoreStatus:&#xA;    tcp://10.42.1.59:10000:&#xA;      backupURL: &amp;#34;&amp;#34;&#xA;      currentRestoringBackup: &amp;#34;&amp;#34;&#xA;      isRestoring: false&#xA;      lastRestored: &amp;#34;&amp;#34;&#xA;      state: &amp;#34;&amp;#34;&#xA;    tcp://10.42.2.81:10000:&#xA;      backupURL: &amp;#34;&amp;#34;&#xA;      currentRestoringBackup: &amp;#34;&amp;#34;&#xA;      isRestoring: false&#xA;      lastRestored: &amp;#34;&amp;#34;&#xA;      state: &amp;#34;&amp;#34;&#xA;    tcp://10.42.4.30:10000:&#xA;      backupURL: &amp;#34;&amp;#34;&#xA;      currentRestoringBackup: &amp;#34;&amp;#34;&#xA;      isRestoring: false&#xA;      lastRestored: &amp;#34;&amp;#34;&#xA;      state: &amp;#34;&amp;#34;&#xA;    tcp://10.42.4.30:10015:&#xA;      backupURL: &amp;#34;&amp;#34;&#xA;      currentRestoringBackup: &amp;#34;&amp;#34;&#xA;      error: &amp;#39;Failed to get restoring status on tcp://10.42.4.30:10015: failed to&#xA;        get restore status: rpc error: code = Unavailable desc = all SubConns are&#xA;        in TransientFailure, latest connection error: connection error: desc = &amp;#34;transport:&#xA;        Error while dialing dial tcp 10.42.4.30:10017: connect: connection refused&amp;#34;&amp;#39;&#xA;      isRestoring: false&#xA;      lastRestored: &amp;#34;&amp;#34;&#xA;      state: &amp;#34;&amp;#34;&#xA;  salvageExecuted: false&#xA;  snapshots:&#xA;    35491870-d26d-4083-abf5-8fe36453eaec:&#xA;      children:&#xA;        volume-head: true&#xA;      created: &amp;#34;2022-07-27T04:50:53Z&amp;#34;&#xA;      labels: {}&#xA;      name: 35491870-d26d-4083-abf5-8fe36453eaec&#xA;      parent: &amp;#34;&amp;#34;&#xA;      removed: false&#xA;      size: &amp;#34;0&amp;#34;&#xA;      usercreated: false&#xA;    volume-head:&#xA;      children: {}&#xA;      created: &amp;#34;2022-07-27T04:50:53Z&amp;#34;&#xA;      labels: {}&#xA;      name: volume-head&#xA;      parent: 35491870-d26d-4083-abf5-8fe36453eaec&#xA;      removed: false&#xA;      size: &amp;#34;0&amp;#34;&#xA;      usercreated: false&#xA;  snapshotsError: &amp;#34;&amp;#34;&#xA;  started: true&#xA;  storageIP: 10.42.4.31&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;p&gt;GitHub Issue: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/4294&#34;&gt;https://github.com/longhorn/longhorn/issues/4294&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test uninstallation</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/uninstallation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/uninstallation/</guid>
      <description>&lt;h2 id=&#34;stability-of-uninstallation&#34;&gt;Stability of uninstallation&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Launch Longhorn system.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Use scripts to continuously create then delete multiple DaemonSets.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;e.g., putting the following python test into the manager integration test directory and run it:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;from common import get_apps_api_client # NOQA&#xA;&#xA;&#xA;def test_uninstall_script():&#xA;    apps_api = get_apps_api_client()&#xA;    while True:&#xA;        for i in range(10):&#xA;            name = &amp;#34;ds-&amp;#34; + str(i)&#xA;            try:&#xA;                ds = apps_api.read_namespaced_daemon_set(name, &amp;#34;default&amp;#34;)&#xA;                if ds.status.number_ready == ds.status.number_ready:&#xA;                    apps_api.delete_namespaced_daemon_set(name, &amp;#34;default&amp;#34;)&#xA;            except Exception:&#xA;                apps_api.create_namespaced_daemon_set(&#xA;                    &amp;#34;default&amp;#34;, ds_manifest(name))&#xA;&#xA;&#xA;def ds_manifest(name):&#xA;    return {&#xA;        &amp;#39;apiVersion&amp;#39;: &amp;#39;apps/v1&amp;#39;,&#xA;        &amp;#39;kind&amp;#39;: &amp;#39;DaemonSet&amp;#39;,&#xA;        &amp;#39;metadata&amp;#39;: {&#xA;            &amp;#39;name&amp;#39;: name&#xA;        },&#xA;        &amp;#39;spec&amp;#39;: {&#xA;            &amp;#39;selector&amp;#39;: {&#xA;                &amp;#39;matchLabels&amp;#39;: {&#xA;                    &amp;#39;app&amp;#39;: name&#xA;                }&#xA;            },&#xA;            &amp;#39;template&amp;#39;: {&#xA;                &amp;#39;metadata&amp;#39;: {&#xA;                    &amp;#39;labels&amp;#39;: {&#xA;                        &amp;#39;app&amp;#39;: name&#xA;                    }&#xA;                },&#xA;                &amp;#39;spec&amp;#39;: {&#xA;                    &amp;#39;terminationGracePeriodSeconds&amp;#39;: 10,&#xA;                    &amp;#39;containers&amp;#39;: [{&#xA;                        &amp;#39;image&amp;#39;: &amp;#39;busybox&amp;#39;,&#xA;                        &amp;#39;imagePullPolicy&amp;#39;: &amp;#39;IfNotPresent&amp;#39;,&#xA;                        &amp;#39;name&amp;#39;: &amp;#39;sleep&amp;#39;,&#xA;                        &amp;#39;args&amp;#39;: [&#xA;                            &amp;#39;/bin/sh&amp;#39;,&#xA;                            &amp;#39;-c&amp;#39;,&#xA;                            &amp;#39;while true;do date;sleep 5; done&amp;#39;&#xA;                        ],&#xA;                    }]&#xA;                }&#xA;            },&#xA;        }&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Start to uninstall longhorn.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test upgrade for migrated Longhorn on Rancher</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-upgrade-for-migrated-longhorn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.4.0/test-upgrade-for-migrated-longhorn/</guid>
      <description>&lt;h2 id=&#34;related-discussion&#34;&gt;Related discussion&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/discussions/4198&#34;&gt;https://github.com/longhorn/longhorn/discussions/4198&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;: since few customers used our broken chart longhorn 100.2.1+up1.3.1 on Rancher (Now fixed) with the workaround. We would like to verify the future upgrade path for those customers.&lt;/p&gt;&#xA;&lt;h2 id=&#34;steps&#34;&gt;Steps&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Set up a cluster of Kubernetes 1.20.&lt;/li&gt;&#xA;&lt;li&gt;Adding this repo to the apps section in new rancher UI&#xA;&lt;ol&gt;&#xA;&lt;li&gt;repo: &lt;a href=&#34;https://github.com/PhanLe1010/charts.git&#34;&gt;https://github.com/PhanLe1010/charts.git&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;branch: release-v2.6-longhorn-1.3.1.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Access old rancher UI by navigating to &lt;code&gt;&amp;lt;your-rancher-url&amp;gt;/g&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Install Longhorn 1.0.2.&lt;/li&gt;&#xA;&lt;li&gt;Create/attach some volumes. Create a few recurring snapshot/backup job that run every minutes.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to v1.2.4.&lt;/li&gt;&#xA;&lt;li&gt;Migrate Longhorn to new chart in new rancher UI &lt;a href=&#34;https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/&#34;&gt;https://longhorn.io/kb/how-to-migrate-longhorn-chart-installed-in-old-rancher-ui-to-the-chart-in-new-rancher-ui/&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade to longhorn 100.2.1+up1.3.1 in the UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify the upgrade would be stuck at with error:&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Patch CustomResourceDefinition &amp;#34;volumes.longhorn.io&amp;#34; in namespace&#xA;error updating the resource &amp;#34;volumes.longhorn.io&amp;#34;:&#xA;         cannot patch &amp;#34;volumes.longhorn.io&amp;#34; with kind CustomResourceDefinition: CustomResourceDefinition.apiextensions.k8s.io &amp;#34;volumes.longhorn.io&amp;#34; is invalid: spec.conversion.strategy: Invalid value: &amp;#34;Webhook&amp;#34;: must be None if spec.preserveUnknownFields is true&#xA;Error: UPGRADE FAILED: cannot patch &amp;#34;engineimages.longhorn.io&amp;#34; with kind CustomResourceDefinition: CustomResourceDefinition.apiextensions.k8s.io &amp;#34;engineimages.longhorn.io&amp;#34; is invalid: spec.conversion.strategy: Invalid value: &amp;#34;Webhook&amp;#34;: must be None if spec.preserveUnknownFields is true &amp;amp;&amp;amp; cannot patch &amp;#34;nodes.longhorn.io&amp;#34; with kind CustomResourceDefinition: CustomResourceDefinition.apiextensions.k8s.io &amp;#34;nodes.longhorn.io&amp;#34; is invalid: spec.conversion.strategy: Invalid value: &amp;#34;Webhook&amp;#34;: must be None if spec.preserveUnknownFields is true &amp;amp;&amp;amp; cannot patch &amp;#34;volumes.longhorn.io&amp;#34; with kind CustomResourceDefinition: CustomResourceDefinition.apiextensions.k8s.io &amp;#34;volumes.longhorn.io&amp;#34; is invalid: spec.conversion.strategy: Invalid value: &amp;#34;Webhook&amp;#34;: must be None if spec.preserveUnknownFields is true&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Apply the &lt;a href=&#34;https://github.com/longhorn/longhorn/pull/4237#issuecomment-1195339461&#34;&gt;workaround&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Retry upgrade to longhorn 100.2.1+up1.3.1 in the UI.&lt;/li&gt;&#xA;&lt;li&gt;Verify the upgrade is ok.&lt;/li&gt;&#xA;&lt;li&gt;Verify that the user can successfully upgrade to future chart releases (e.g., Longhorn v1.3.2) from this version longhorn 100.2.1+up1.3.1.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Test upgrade responder collecting extra info</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.5.0/test-upgrade-responder-collect-extra-info/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.5.0/test-upgrade-responder-collect-extra-info/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/5235&#34;&gt;https://github.com/longhorn/longhorn/issues/5235&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-step&#34;&gt;Test step&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Patch build and deploy Longhorn.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;diff --git a/controller/setting_controller.go b/controller/setting_controller.go&#xA;index de77b7246..ac6263ac5 100644&#xA;--- a/controller/setting_controller.go&#xA;+++ b/controller/setting_controller.go&#xA;@@ -49,7 +49,7 @@ const (&#xA; var (&#xA; &#x9;upgradeCheckInterval          = time.Hour&#xA; &#x9;settingControllerResyncPeriod = time.Hour&#xA;-&#x9;checkUpgradeURL               = &amp;#34;https://longhorn-upgrade-responder.rancher.io/v1/checkupgrade&amp;#34;&#xA;+&#x9;checkUpgradeURL               = &amp;#34;http://longhorn-upgrade-responder.default.svc.cluster.local:8314/v1/checkupgrade&amp;#34;&#xA; )&#xA; &#xA; type SettingController struct {&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;&#xA;&lt;p&gt;Match the checkUpgradeURL with the application name: &lt;code&gt;http://&amp;lt;APP_NAME&amp;gt;-upgrade-responder.default.svc.cluster.local:8314/v1/checkupgrade&lt;/code&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;And&lt;/strong&gt; &lt;a href=&#34;https://github.com/longhorn/longhorn/tree/master/dev/upgrade-responder&#34;&gt;Deploy upgrade responder stack&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Wait 1~2 hours for collection data to send to the influxDB database.&lt;br&gt;&#xA;&lt;strong&gt;Then&lt;/strong&gt; Collection data should exist the influxDB database.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test upgrade responder should collect SPDK related info</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-upgrade-responder-collect-spdk-related-info/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.6.0/test-upgrade-responder-collect-spdk-related-info/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/6033&#34;&gt;https://github.com/longhorn/longhorn/issues/6033&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-step&#34;&gt;Test step&lt;/h2&gt;&#xA;&lt;h3 id=&#34;prerequisite&#34;&gt;Prerequisite&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; Patch build and deploy Longhorn.&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;diff --git a/controller/setting_controller.go b/controller/setting_controller.go&#xA;index de77b7246..ac6263ac5 100644&#xA;--- a/controller/setting_controller.go&#xA;+++ b/controller/setting_controller.go&#xA;@@ -49,7 +49,7 @@ const (&#xA; var (&#xA; &#x9;upgradeCheckInterval          = time.Hour&#xA; &#x9;settingControllerResyncPeriod = time.Hour&#xA;-&#x9;checkUpgradeURL               = &amp;#34;https://longhorn-upgrade-responder.rancher.io/v1/checkupgrade&amp;#34;&#xA;+&#x9;checkUpgradeURL               = &amp;#34;http://longhorn-upgrade-responder.default.svc.cluster.local:8314/v1/checkupgrade&amp;#34;&#xA; )&#xA;&#xA; type SettingController struct {&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;&#xA;&lt;p&gt;Match the checkUpgradeURL with the application name: &lt;code&gt;http://&amp;lt;APP_NAME&amp;gt;-upgrade-responder.default.svc.cluster.local:8314/v1/checkupgrade&lt;/code&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;And&lt;/strong&gt; Set setting &lt;code&gt;v2-data-engine&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;br&gt;&#xA;&lt;strong&gt;And&lt;/strong&gt; &lt;a href=&#34;https://longhorn.io/docs/1.5.3/spdk/quick-start/#add-block-type-disks-in-longhorn-nodes&#34;&gt;Add two block-type Disks in Longhorn Nodes&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Version Bump of Kubernetes, API version group, CSI component&#39;s dependency version</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.0/test_version_bump/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.2.0/test_version_bump/</guid>
      <description>&lt;p&gt;GitHub issue: &lt;a href=&#34;https://github.com/longhorn/longhorn/issues/2757&#34;&gt;https://github.com/longhorn/longhorn/issues/2757&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;test-with-specific-kubernetes-version&#34;&gt;Test with specific Kubernetes version&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;For each Kubernetes version (1.18, 1.19, 1.20, 1.21, 1.22), test basic functionalities of Longhorn v1.2.0&#xA;(create/attach/detach/delete volume/backup/snapshot using yaml/UI)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;test-kubernetes-and-longhorn-upgrade&#34;&gt;Test Kubernetes and Longhorn upgrade&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Deploy K3s v1.21&lt;/li&gt;&#xA;&lt;li&gt;Deploy Longhorn v1.1.2&lt;/li&gt;&#xA;&lt;li&gt;Create some workload pods using Longhorn volumes&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to v1.2.0&lt;/li&gt;&#xA;&lt;li&gt;Verify that everything is OK&lt;/li&gt;&#xA;&lt;li&gt;Upgrade K3s to v1.22&lt;/li&gt;&#xA;&lt;li&gt;Verify that everything is OK&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;retest-the-upgrade-lease-lock&#34;&gt;Retest the Upgrade Lease Lock&lt;/h1&gt;&#xA;&lt;p&gt;We remove the client-go patch &lt;a href=&#34;https://github.com/longhorn/longhorn-manager/pull/639#issuecomment-905030885&#34;&gt;https://github.com/longhorn/longhorn-manager/pull/639#issuecomment-905030885&lt;/a&gt;,&#xA;so we need to retest the test ../v1.0.2/upgrade-lease-lock.md&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test Volume Replica Zone Soft Anti-Affinity Setting</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.5.0/test-the-volume-replica-scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.5.0/test-the-volume-replica-scheduling/</guid>
      <description>&lt;h2 id=&#34;related-issue&#34;&gt;Related issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/5358&#34;&gt;https://github.com/longhorn/longhorn/issues/5358&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-step---enable-volume-replica-zone-soft-anti-affinity-setting&#34;&gt;Test step - Enable Volume Replica Zone Soft Anti-Affinity Setting&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt; EKS Cluster with 3 nodes across 2 AWS zones (zone#1, zone#2)&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Deploy Longhorn v1.5.0&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Disable global replica zone anti-affinity&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; Create a volume with 2 replicas, &lt;code&gt;replicaZoneSoftAntiAffinity=enabled&lt;/code&gt; and attach it to a node.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;When&lt;/strong&gt; Scale volume replicas to 3&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Then&lt;/strong&gt; New replica should be scheduled&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;And&lt;/em&gt; No error messages in the longhorn manager pod logs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Testing ext4 with custom fs params1 (no 64bit, no metadata_csum)</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-custom-fs-params-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-custom-fs-params-1/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;set the following filesystem parameters: &lt;code&gt;-O ^64bit,^metadata_csum&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;create a volume + pv + pvc with filesystem &lt;code&gt;ext4&lt;/code&gt; named &lt;code&gt;ext4-no-ck-no-64&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;create a deployment that uses &lt;code&gt;ext4-no-ck-no-64&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;verify that the pod enters running state and the volume is accessible&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Testing ext4 with custom fs params2 (no metadata_csum)</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-custom-fs-params-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-custom-fs-params-2/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;set the following filesystem parameters: &lt;code&gt;-O ^metadata_csum&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;create a volume + pv + pvc with filesystem &lt;code&gt;ext4&lt;/code&gt; named &lt;code&gt;ext4-no-ck&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;create a deployment that uses &lt;code&gt;ext4-no-ck&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;verify that the pod enters running state and the volume is accessible&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Testing ext4 without custom fs params</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-no-custom-fs-params/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.0/suse-sles12sp3/ext4-no-custom-fs-params/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;create a volume + pv + pvc with filesystem &lt;code&gt;ext4&lt;/code&gt; named &lt;code&gt;ext-ck-fail&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;create a deployment that uses &lt;code&gt;ext-ck-fail&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;verify &lt;code&gt;MountVolume.SetUp failed for volume &amp;quot;ext4-ck-fails&amp;quot;&lt;/code&gt; is part of the pod events&lt;/li&gt;&#xA;&lt;li&gt;verify that the pod does not enter running state&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Testing xfs after custom fs params (xfs should ignore the custom fs params)</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.0/suse-sles12sp3/xfs-after-custom-fs-params/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.0/suse-sles12sp3/xfs-after-custom-fs-params/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;create a volume + pv + pvc with filesystem &lt;code&gt;xfs&lt;/code&gt; named &lt;code&gt;xfs-ignores&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;create a deployment that uses &lt;code&gt;xfs-ignores&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;verify that the pod enters running state and the volume is accessible&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Uninstallation Checks</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/uninstallation/uninstallation-checks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/uninstallation/uninstallation-checks/</guid>
      <description>&lt;h3 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Have a setup of Longhorn installed on a kubernetes cluster.&lt;/li&gt;&#xA;&lt;li&gt;Have few volumes backups stored on S3/NFS backup store.&lt;/li&gt;&#xA;&lt;li&gt;Have one DR volume created (not activated) in another cluster with a volume in current cluster.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;test-steps&#34;&gt;Test steps&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Uninstall Longhorn.&lt;/li&gt;&#xA;&lt;li&gt;Check the logs of the job &lt;code&gt;longhorn-uninstall&lt;/code&gt;, make sure there is no error.&lt;/li&gt;&#xA;&lt;li&gt;Check all the components of Longhorn from the namespace longhorn-system are uninstalled. E.g. Longhorn manager, Longhorn driver, Longhorn UI, instance manager, engine image, CSI driver etc.&lt;/li&gt;&#xA;&lt;li&gt;Check all the CRDs are removed &lt;code&gt;kubectl get crds | grep longhorn&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Check the backup stores, the backups taken should NOT be removed.&lt;/li&gt;&#xA;&lt;li&gt;Activate the DR volume in the other cluster and check the data.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If uninstalling from Rancher cluster using cluster explorer, uninstall Longhorn first then the crds.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Upgrade Conflict Handling test</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/upgrade-conflict-handling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/pre-release/upgrade/upgrade-conflict-handling/</guid>
      <description>&lt;h3 id=&#34;new-installation&#34;&gt;New installation:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a large cluster of many nodes (about 30 nodes)&lt;/li&gt;&#xA;&lt;li&gt;Install Longhorn &lt;code&gt;master&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create 100 volumes using volume template claim in statefulSet.&lt;/li&gt;&#xA;&lt;li&gt;Have the backup store configured and create some backups.&lt;/li&gt;&#xA;&lt;li&gt;Set some recurring jobs in the cluster every 1 minute.&lt;/li&gt;&#xA;&lt;li&gt;Observe the setup for 1/2 an hr. Do some operation like attaching detaching the volumes.&lt;/li&gt;&#xA;&lt;li&gt;Verify there is no error in the Longhorn manager.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;upgrading-from-old-version&#34;&gt;Upgrading from old version:&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Repeat the steps from above test case with Longhorn Prior version.&lt;/li&gt;&#xA;&lt;li&gt;Upgrade Longhorn to &lt;code&gt;master&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Do some operation like attaching and detaching the volumes.&lt;/li&gt;&#xA;&lt;li&gt;Verify there is no error in the Longhorn manager.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;strong&gt;Success if:&lt;/strong&gt; install/upgrade successfully after maximum 15 minutes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Upgrade Kubernetes using Rancher UI</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/rancher-integration/upgrade-using-rancher-ui/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/rancher-integration/upgrade-using-rancher-ui/</guid>
      <description>&lt;p&gt;Note: Longhorn version v1.3.x doesn&amp;rsquo;t support Kubernetes v1.25 and onwards&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-with-longhorn-default-setting-of-node-drain-policy-block-if-contains-last-replica&#34;&gt;Test with Longhorn default setting of &amp;lsquo;Node Drain Policy&amp;rsquo;: &lt;code&gt;block-if-contains-last-replica&lt;/code&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-upgrade-single-node-cluster-using-rancher-ui---rke2-cluster&#34;&gt;1. Upgrade single node cluster using Rancher UI - RKE2 cluster&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt;  Single node RKE2 cluster provisioned in Rancher with K8s prior version with Longhorn installed&lt;/p&gt;&#xA;&lt;p&gt;AND few RWO and RWX volumes attached with node/pod exists&lt;/p&gt;&#xA;&lt;p&gt;AND 1 RWO and 1 RWX volumes unattached&lt;/p&gt;&#xA;&lt;p&gt;AND 1 RWO volume with 50 Gi data&lt;/p&gt;</description>
    </item>
    <item>
      <title>Upgrade Kubernetes using SUC</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/rancher-integration/upgrade-using-suc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/rancher-integration/upgrade-using-suc/</guid>
      <description>&lt;p&gt;Note: Longhorn version v1.3.x doesn&amp;rsquo;t support Kubernetes v1.25 and onwards&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-with-longhorn-default-setting-of-node-drain-policy-block-if-contains-last-replica&#34;&gt;Test with Longhorn default setting of &amp;lsquo;Node Drain Policy&amp;rsquo;: &lt;code&gt;block-if-contains-last-replica&lt;/code&gt;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-upgrade-multi-node-cluster-using-suc---k3s-cluster&#34;&gt;1. Upgrade multi node cluster using SUC - K3s cluster&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Given&lt;/strong&gt;  Multi node (1 master and 3 worker) K3s cluster (not provisioned by Rancher) with K3s prior version with Longhorn installed&lt;/p&gt;&#xA;&lt;p&gt;AND &lt;a href=&#34;https://github.com/rancher/system-upgrade-controller#deploying&#34;&gt;System Upgrade Controller&lt;/a&gt; deployed&lt;/p&gt;&#xA;&lt;p&gt;AND few RWO and RWX volumes attached with node/pod exists&lt;/p&gt;&#xA;&lt;p&gt;AND 1 RWO and 1 RWX volumes unattached&lt;/p&gt;</description>
    </item>
    <item>
      <title>Upgrade Lease Lock</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.2/upgrade-lease-lock/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.2/upgrade-lease-lock/</guid>
      <description>&lt;p&gt;The time it takes between the Longhorn Manager starting up and the upgrade completing for that Longhorn Manager can be used to determine if the upgrade lock was released correctly:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a fresh Longhorn installation or delete all of the Longhorn Manager Pods in the existing installation.&lt;/li&gt;&#xA;&lt;li&gt;Check the logs for the Longhorn Manager Pods and note the timestamps for the first line in the log and the timestamp for when the upgrade has completed.&#xA;For example, in this log, the relevant timestamps are &lt;code&gt;2020-08-03T22:55:39Z&lt;/code&gt; and &lt;code&gt;2020-08-03T22:56:24Z&lt;/code&gt;:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;time=&amp;#34;2020-08-03T22:55:39Z&amp;#34; level=info msg=&amp;#34;Start overwriting built-in settings with customized values&amp;#34;&#xA;E0803 22:55:40.141071       1 leaderelection.go:310] error initially creating leader election record: leases.coordination.k8s.io &amp;#34;longhorn-manager-upgrade-lock&amp;#34; already exists&#xA;time=&amp;#34;2020-08-03T22:55:43Z&amp;#34; level=info msg=&amp;#34;New upgrade leader elected: 74.207.240.60&amp;#34;&#xA;time=&amp;#34;2020-08-03T22:56:01Z&amp;#34; level=info msg=&amp;#34;New upgrade leader elected: 96.126.101.152&amp;#34;&#xA;time=&amp;#34;2020-08-03T22:56:24Z&amp;#34; level=info msg=&amp;#34;Start upgrading&amp;#34;&#xA;time=&amp;#34;2020-08-03T22:56:24Z&amp;#34; level=info msg=&amp;#34;No API version upgrade is needed&amp;#34;&#xA;time=&amp;#34;2020-08-03T22:56:24Z&amp;#34; level=info msg=&amp;#34;Finish upgrading&amp;#34;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;Calculate the amount of time between the two timestamps.&#xA;If the lock was released successfully, then it should take no longer than about 15 seconds for the upgrade process to complete on each Pod:&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;longhorn-manager-k95bm&lt;/code&gt;: 2020-08-03T23:04:14Z - 2020-08-03T23:04:20Z &lt;strong&gt;(6 seconds)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;longhorn-manager-rgpvt&lt;/code&gt;: 2020-08-03T23:04:14Z - 2020-08-03T23:04:15Z &lt;strong&gt;(1 second)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;longhorn-manager-z2jd9&lt;/code&gt;: 2020-08-03T23:04:21Z - 2020-08-03T23:04:21Z &lt;strong&gt;(0 seconds)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Here is an example of a failing case with the Longhorn Manager attempting to upgrade and the upgrade lock not being released successfully.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Upgrade Longhorn with modified Storage Class</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/upgrade_with_modified_storageclass/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.1.0/upgrade_with_modified_storageclass/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;&#xA;&lt;p&gt;Longhorn can be upgraded with modified Storage Class.&lt;/p&gt;&#xA;&lt;h2 id=&#34;related-issue&#34;&gt;Related Issue&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/longhorn/longhorn/issues/1527&#34;&gt;https://github.com/longhorn/longhorn/issues/1527&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;test-steps&#34;&gt;Test steps:&lt;/h2&gt;&#xA;&lt;h3 id=&#34;kubectl-apply--f&#34;&gt;Kubectl apply -f&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Install Longhorn v1.0.2&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.0.2/deploy/longhorn.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Create a statefulset using &lt;code&gt;longhorn&lt;/code&gt; storageclass for PVCs. Set the scale to 1.&lt;/li&gt;&#xA;&lt;li&gt;Observe that there is a workload pod (&lt;code&gt;pod-1&lt;/code&gt;) is using 1 volume (&lt;code&gt;vol-1&lt;/code&gt;) with 3 replicas.&lt;/li&gt;&#xA;&lt;li&gt;In Longhorn repo, on &lt;code&gt;master&lt;/code&gt; branch. Modify &lt;code&gt;numberOfReplicas: &amp;quot;1&amp;quot;&lt;/code&gt; in &lt;code&gt;https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml&lt;/code&gt;. Upgrade Longhorn to master by running&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;li&gt;Verify that &lt;code&gt;longhorn&lt;/code&gt; storage class now has the field &lt;code&gt;numberOfReplicas: 1&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Scale up the deployment to 2. Verify that:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;pod-1&lt;/code&gt; is using volume &lt;code&gt;vol-1&lt;/code&gt; with 3 replicas.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;pod-2&lt;/code&gt; is using volume &lt;code&gt;vol-2&lt;/code&gt; with 1 replica.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Scale up the deployment to 0 then back to 2. Verify that:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;pod-1&lt;/code&gt; is using volume &lt;code&gt;vol-1&lt;/code&gt; with 3 replicas.&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;pod-2&lt;/code&gt; is using volume &lt;code&gt;vol-2&lt;/code&gt; with 1 replica.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;helm&#34;&gt;Helm&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Follow this instruction to install Longhorn v1.0.2 using Helm &lt;a href=&#34;https://longhorn.io/docs/1.0.2/deploy/install/install-with-helm/#installing-longhorn&#34;&gt;https://longhorn.io/docs/1.0.2/deploy/install/install-with-helm/#installing-longhorn&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Volume Deletion UI Warnings</title>
      <link>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/ui-volume-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://longhorn.github.io/longhorn-tests/manual/release-specific/v1.0.1/ui-volume-deletion/</guid>
      <description>&lt;p&gt;A number of cases need to be manually tested in &lt;code&gt;longhorn-ui&lt;/code&gt;. To test these cases, create the &lt;code&gt;Volume&lt;/code&gt; with the specified conditions in each case, and then try to delete it. What is observed should match what is described in the test case:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;A regular &lt;code&gt;Volume&lt;/code&gt;. Only the default deletion prompt should show up asking to confirm deletion.&lt;/li&gt;&#xA;&lt;li&gt;A &lt;code&gt;Volume&lt;/code&gt; with a &lt;code&gt;Persistent Volume&lt;/code&gt;. The deletion prompt should tell the user that there is a &lt;code&gt;Persistent Volume&lt;/code&gt; that will be deleted along with the &lt;code&gt;Volume&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;A &lt;code&gt;Volume&lt;/code&gt; with a &lt;code&gt;Persistent Volume&lt;/code&gt; and &lt;code&gt;Persistent Volume Claim&lt;/code&gt;. The deletion prompt should tell the user that there is a &lt;code&gt;Persistent Volume&lt;/code&gt; and &lt;code&gt;Persistent Volume Claim&lt;/code&gt; that will be deleted along with the &lt;code&gt;Volume&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;A &lt;code&gt;Volume&lt;/code&gt; that is &lt;code&gt;Attached&lt;/code&gt;. The deletion prompt should indicate what &lt;code&gt;Node&lt;/code&gt; the &lt;code&gt;Volume&lt;/code&gt; is attached to and warn the user about errors that may occur as a result of deleting an attached &lt;code&gt;Volume&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;A &lt;code&gt;Volume&lt;/code&gt; that is &lt;code&gt;Attached&lt;/code&gt; and has a &lt;code&gt;Persistent Volume&lt;/code&gt;. The deletion prompt should contain the information from both test cases 2 and 4.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Additionally, here are bulk deletion test cases that need testing:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
